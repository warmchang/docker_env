<!DOCTYPE HTML>
<html lang="zh" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>OpenShift4 慢慢走</title>
        <meta name="robots" content="noindex" />
        <!-- Custom HTML head -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-E3FRMDB7L2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-E3FRMDB7L2');
</script>        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="index.html">介绍</a></li><li class="chapter-item expanded "><a href="install.html"><strong aria-hidden="true">1.</strong> openshift4 安装系列</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="ocp4/4.5/4.5.ocp.pull.secret.html"><strong aria-hidden="true">1.1.</strong> 如何获得 openshift4 免费下载密钥</a></li><li class="chapter-item expanded "><a href="ocp4/4.6/4.6.build.dist.html"><strong aria-hidden="true">1.2.</strong> openshift4 离线安装介质的制作</a></li><li class="chapter-item expanded "><a href="ocp4/4.6/4.6.disconnect.bm.upi.static.ip.on.rhel7.html"><strong aria-hidden="true">1.3.</strong> openshift4 rhel7物理机 baremetal UPI模式 离线安装</a></li><li class="chapter-item expanded "><a href="ocp4/4.6/4.6.disconnect.bm.upi.static.ip.on.rhel8.html"><strong aria-hidden="true">1.4.</strong> openshift4 rhel8物理机 baremetal UPI模式 离线安装</a></li><li class="chapter-item expanded "><a href="ocp4/4.6/4.6.disconnect.bm.ipi.on.rhel8.html"><strong aria-hidden="true">1.5.</strong> openshift4 物理机 baremetal IPI模式 离线安装 单网络模式</a></li><li class="chapter-item expanded "><a href="ocp4/4.6/4.6.disconnect.bm.ipi.on.rhel8.provisionning.network.html"><strong aria-hidden="true">1.6.</strong> openshift4 物理机 baremetal IPI模式 离线安装 双网络模式</a></li><li class="chapter-item expanded "><a href="ocp4/4.6/4.6.nvidia.gpu.disconnected.html"><strong aria-hidden="true">1.7.</strong> nvidia gpu for openshift 4.6 disconnected 英伟达GPU离线安装</a></li><li class="chapter-item expanded "><a href="ocp4/4.6/4.6.add.image.html"><strong aria-hidden="true">1.8.</strong> openshift4 初始安装后 补充镜像</a></li><li class="chapter-item expanded "><a href="ocp4/4.5/4.5.is.sample.html"><strong aria-hidden="true">1.9.</strong> openshift4 补充samples operator 需要的 image stream</a></li><li class="chapter-item expanded "><a href="ocp4/4.3/4.3.calico.html"><strong aria-hidden="true">1.10.</strong> openshift4 calico 离线部署</a></li><li class="chapter-item expanded "><a href="ocp4/4.2/4.2.upgrade.html"><strong aria-hidden="true">1.11.</strong> openshift4 集群升级</a></li><li class="chapter-item expanded "><a href="ocp4/4.8/4.8.shrink.sysroot.html"><strong aria-hidden="true">1.12.</strong> 缩小根分区 / sysroot 的大小</a></li><li class="chapter-item expanded "><a href="ocp4/4.8/4.8.update.service.html"><strong aria-hidden="true">1.13.</strong> 部署升级服务 完善离线升级功能</a></li><li class="chapter-item expanded "><a href="ocp4/4.8/4.8.windows.node.html"><strong aria-hidden="true">1.14.</strong> 添加 win10 worker 节点</a></li></ol></li><li class="chapter-item expanded "><a href="usage.html"><strong aria-hidden="true">2.</strong> openshift4 使用系列</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="ocp4/4.9/4.9.load.3rd.part.driver.html"><strong aria-hidden="true">2.1.</strong> 加载第三方设备驱动</a></li><li class="chapter-item expanded "><a href="ocp4/4.9/4.9.nep.containerized.helm.html"><strong aria-hidden="true">2.2.</strong> helm chart/helm operator</a></li><li class="chapter-item expanded "><a href="ocp4/4.8/4.8.metalb.l2.html"><strong aria-hidden="true">2.3.</strong> 使用 MetalLB 用 Layer2 发布 LoadBalancer</a></li><li class="chapter-item expanded "><a href="ocp4/4.8/4.8.metalb.html"><strong aria-hidden="true">2.4.</strong> 使用 MetalLB 用 BGP 发布 LoadBalancer</a></li><li class="chapter-item expanded "><a href="ocp4/4.8/4.8.kata.html"><strong aria-hidden="true">2.5.</strong> kata / 沙盒容器</a></li><li class="chapter-item expanded "><a href="ocp4/4.7/4.7.sriov.html"><strong aria-hidden="true">2.6.</strong> 在非官方支持的网卡上，测试SRIOV/DPDK</a></li><li class="chapter-item expanded "><a href="ocp4/4.7/4.7.keepalived.operator.html"><strong aria-hidden="true">2.7.</strong> 使用 keepalived 激活 LoadBalancer 服务类型</a></li><li class="chapter-item expanded "><a href="ocp4/4.7/4.7.real-time.kernel.html"><strong aria-hidden="true">2.8.</strong> 在节点上启用实时操作系统 real-time kernel</a></li><li class="chapter-item expanded "><a href="ocp4/4.7/4.7.install.kmod.driver.html"><strong aria-hidden="true">2.9.</strong> 从容器向宿主机注入内核模块 kmod / driver</a></li><li class="chapter-item expanded "><a href="ocp4/4.6/4.6.vgpu.sharing.deploy.html"><strong aria-hidden="true">2.10.</strong> GPU/vGPU 共享</a></li><li class="chapter-item expanded "><a href="ocp4/4.4/4.4.headless.service.html"><strong aria-hidden="true">2.11.</strong> openshift headless service讲解</a></li><li class="chapter-item expanded "><a href="ocp4/4.3/4.3.volumn.html"><strong aria-hidden="true">2.12.</strong> openshift volumn 存储的各种测试</a></li><li class="chapter-item expanded "><a href="ocp4/4.3/4.3.SupportPodPidsLimit.html"><strong aria-hidden="true">2.13.</strong> openshift 设置 SupportPodPidsLimit 解除 pids 限制</a></li><li class="chapter-item expanded "><a href="ocp4/4.3/4.3.sso.html"><strong aria-hidden="true">2.14.</strong> openshift4 配置 SSO 点单认证</a></li><li class="chapter-item expanded "><a href="ocp4/4.3/4.3.scc.html"><strong aria-hidden="true">2.15.</strong> openshift4 SCC 相关安全能力测试</a></li><li class="chapter-item expanded "><a href="ocp4/4.3/4.3.recover.node.not.ready.html"><strong aria-hidden="true">2.16.</strong> openshift4 从 node not ready 状态恢复</a></li><li class="chapter-item expanded "><a href="ocp4/4.3/4.3.QoS.nic.html"><strong aria-hidden="true">2.17.</strong> openshift4 QoS 能力</a></li><li class="chapter-item expanded "><a href="ocp4/4.3/4.3.QoS.nic.high.html"><strong aria-hidden="true">2.18.</strong> openshift4 QoS 在流量压力下的表现</a></li><li class="chapter-item expanded "><a href="ocp4/4.3/4.3.proxy.html"><strong aria-hidden="true">2.19.</strong> openshift4 使用 image proxy 来下载镜像</a></li><li class="chapter-item expanded "><a href="ocp4/4.3/4.3.numa.html"><strong aria-hidden="true">2.20.</strong> openshift4 NUMA 绑核测试</a></li><li class="chapter-item expanded "><a href="ocp4/4.3/4.3.network.policy.html"><strong aria-hidden="true">2.21.</strong> openshift4 Network Policy 测试</a></li><li class="chapter-item expanded "><a href="ocp4/4.3/4.3.multicast.html"><strong aria-hidden="true">2.22.</strong> openshift4 网络多播 测试</a></li><li class="chapter-item expanded "><a href="ocp4/4.3/4.3.firewall.html"><strong aria-hidden="true">2.23.</strong> openshift4 配置节点防火墙</a></li><li class="chapter-item expanded "><a href="ocp4/4.3/4.3.ldap.html"><strong aria-hidden="true">2.24.</strong> openshift4 集成 ldap</a></li><li class="chapter-item expanded "><a href="ocp4/4.3/4.3.image.pull.html"><strong aria-hidden="true">2.25.</strong> openshift4 维护 image pull secret</a></li><li class="chapter-item expanded "><a href="ocp4/4.3/4.3.huge.page.html"><strong aria-hidden="true">2.26.</strong> openshift4 使用大页内存 huge page</a></li><li class="chapter-item expanded "><a href="ocp4/4.3/4.3.helm.html"><strong aria-hidden="true">2.27.</strong> openshift4 使用 helm</a></li><li class="chapter-item expanded "><a href="ocp4/4.3/4.3.haproxy.html"><strong aria-hidden="true">2.28.</strong> openshift4 定制router 支持 TCP ingress</a></li><li class="chapter-item expanded "><a href="ocp4/4.3/4.3.grafana.html"><strong aria-hidden="true">2.29.</strong> openshift4 监控能力展示 grafana</a></li><li class="chapter-item expanded "><a href="ocp4/4.3/4.3.cpu.manager.html"><strong aria-hidden="true">2.30.</strong> openshift4 CPU 绑核 测试</a></li><li class="chapter-item expanded "><a href="ocp4/4.3/4.3.build.config.html"><strong aria-hidden="true">2.31.</strong> openshift4 build config &amp; hpa 自动化编译和自动扩缩容</a></li></ol></li><li class="chapter-item expanded "><a href="ccn.html"><strong aria-hidden="true">3.</strong> 应用上云系列教程 CCN</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="ocp4/4.4/4.4.ccn.devops.deploy.html"><strong aria-hidden="true">3.1.</strong> 应用上云系列教程 containerized cloud native (CCN) for openshift 4.4</a></li><li class="chapter-item expanded "><a href="ocp4/4.4/4.4.ccn.devops.build.html"><strong aria-hidden="true">3.2.</strong> CCN 安装介质制作 for openshift 4.4</a></li><li class="chapter-item expanded "><a href="ocp4/4.6/4.6.ccn.devops.deploy.html"><strong aria-hidden="true">3.3.</strong> 应用上云系列教程 containerized cloud native (CCN) for openshift 4.6</a></li><li class="chapter-item expanded "><a href="ocp4/4.6/4.6.ccn.devops.build.html"><strong aria-hidden="true">3.4.</strong> CCN 安装介质制作 for openshift 4.6</a></li></ol></li><li class="chapter-item expanded "><a href="rh.cloud.html"><strong aria-hidden="true">4.</strong> 红帽容器云产品系列</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="notes/2021/2021.08.virus.html"><strong aria-hidden="true">4.1.</strong> RHACS 应对log4j 原理和实践</a></li><li class="chapter-item expanded "><a href="ocp4/4.5/4.5.ocp.ocs.cnv.ceph.html"><strong aria-hidden="true">4.2.</strong> openshift承载虚拟化业务(CNV)</a></li><li class="chapter-item expanded "><a href="ocp4/4.7/4.7.rhacs.html"><strong aria-hidden="true">4.3.</strong> RHACS / stackrox</a></li><li class="chapter-item expanded "><a href="ocp4/4.7/4.7.rhacs.deep.html"><strong aria-hidden="true">4.4.</strong> 为 RHACS 找个应用场景： 安全合规测试云 </a></li></ol></li><li class="chapter-item expanded "><a href="os.html"><strong aria-hidden="true">5.</strong> 操作系统相关</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="notes/2021/2021.10.cx6dx.vdpa.offload.html"><strong aria-hidden="true">5.1.</strong> Mellanox CX6 vdpa 硬件卸载 ovs-kernel 方式</a></li><li class="chapter-item expanded "><a href="rhel/rhel.build.kernel.html"><strong aria-hidden="true">5.2.</strong> RHEL8编译定制化内核</a></li><li class="chapter-item expanded "><a href="ocp4/4.5/4.5.check.whether.vm.html"><strong aria-hidden="true">5.3.</strong> 检查OS是否是运行在虚拟机上</a></li><li class="chapter-item expanded "><a href="ocp4/4.4/4.4.ovs.html"><strong aria-hidden="true">5.4.</strong> 两个主机用ovs组网</a></li><li class="chapter-item expanded "><a href="notes/2021/2021.01.ssh.tunnel.html"><strong aria-hidden="true">5.5.</strong> 内网隔离情况下，使用SSH正向和反向代理，实现连通外网http proxy</a></li></ol></li><li class="chapter-item expanded "><a href="workshop.html"><strong aria-hidden="true">6.</strong> 优秀的workshop</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="ocp4/4.5/4.5.ocp.ocs.workshop.html"><strong aria-hidden="true">6.1.</strong> openshift4 &amp; openshift storage workshop</a></li></ol></li><li class="chapter-item expanded "><a href="poc.html"><strong aria-hidden="true">7.</strong> POC</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="ocp4/4.3/poc.sc/install.poc.sc.html"><strong aria-hidden="true">7.1.</strong> 2020.04 某次POC openshift LVM调优</a></li></ol></li><li class="chapter-item expanded "><a href="osx.html"><strong aria-hidden="true">8.</strong> OSX使用技巧</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="osx/osx.record.system.audio.html"><strong aria-hidden="true">8.1.</strong> 如何录制系统声音</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">OpenShift4 慢慢走</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/wangzheng422/docker_env" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="openshift4-慢慢走"><a class="header" href="#openshift4-慢慢走">Openshift4 慢慢走</a></h1>
<p>本仓库是作者在日常系统操作中的技术笔记。作者平日有些机会进行很多系统操作，包括很多PoC，新系统验证，方案探索工作，所以会有很多系统实际操作的机会，涉及到操作系统安装，iaas, paas平台搭建，中间件系统验证，应用系统的开发和验证。很多操作步骤比较复杂，所以需要一个地方进行集中的笔记记录，方便自己整理，并第一时间在线分享。</p>
<p>作者还做了一个<a href="https://chrome.google.com/webstore/detail/bing-image-new-tab/hahpccmdkmgmaoebhfnkpcnndnklfbpj/">chrome extension</a>，用来在new tab上展示bing.com的美图，简单美观，欢迎使用。</p>
<!-- [<kbd><img src="../imgs/2021-01-17-17-29-10.png" width="600"></kbd>](https://chrome.google.com/webstore/detail/bing-image-new-tab/hahpccmdkmgmaoebhfnkpcnndnklfbpj/) -->
<!-- [![](../imgs/2021-01-17-17-29-10.png)](https://chrome.google.com/webstore/detail/bing-image-new-tab/hahpccmdkmgmaoebhfnkpcnndnklfbpj/) -->
<p>作者还有很多视频演示，欢迎前往作者的频道订阅</p>
<ul>
<li><a href="https://space.bilibili.com/19536819">bilibili</a></li>
<li><a href="https://www.youtube.com/user/wangzheng422">youtube</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="免费获得openshift4下载密钥"><a class="header" href="#免费获得openshift4下载密钥">免费获得OpenShift4下载密钥</a></h1>
<p><a href="https://www.bilibili.com/video/BV1WK4y1a7y4/"><kbd><img src="ocp4/4.5/imgs/2020-09-11-13-57-25.png" width="600"><kbd></a></p>
<!-- <img src="https://user-images.githubusercontent.com/16319829/81180309-2b51f000-8fee-11ea-8a78-ddfe8c3412a7.png" width="150" height="280"> -->
<!-- https://gist.github.com/stevecondylios/dcadb4fc73e63f27a3bbcf17e52058bf#how-to-resize-an-image-in-github-flavored-markdown-in-2020-and-beyond -->
<ul>
<li><a href="https://www.bilibili.com/video/BV1WK4y1a7y4/">bilibili</a></li>
<li><a href="https://www.ixigua.com/6871106317063455245">xigua</a></li>
<li><a href="https://www.youtube.com/watch?v=sh4e8j-tonw">youtube</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="46-离线安装-介质准备"><a class="header" href="#46-离线安装-介质准备">4.6 离线安装， 介质准备</a></h1>
<p>本文的安装步骤，最好是在美国的VPS上完成，然后打包传输回来。</p>
<p>准备离线安装源的步骤如下</p>
<ul>
<li>准备好operator hub catalog，主要是需要里面的日期信息</li>
<li>运行脚本，准备离线安装源</li>
</ul>
<h2 id="环境准备"><a class="header" href="#环境准备">环境准备</a></h2>
<pre><code class="language-bash"># on vultr
yum -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm

yum -y install htop byobu ethtool dstat

rm -rf /data/ocp4
mkdir -p /data/ocp4
cd /data/ocp4

yum -y install podman docker-distribution pigz skopeo docker buildah jq python3-pip git python36

pip3 install yq

# https://blog.csdn.net/ffzhihua/article/details/85237411
# wget http://mirror.centos.org/centos/7/os/x86_64/Packages/python-rhsm-certificates-1.19.10-1.el7_4.x86_64.rpm
# rpm2cpio python-rhsm-certificates-1.19.10-1.el7_4.x86_64.rpm | cpio -iv --to-stdout ./etc/rhsm/ca/redhat-uep.pem | tee /etc/rhsm/ca/redhat-uep.pem

systemctl enable --now docker

# systemctl start docker

docker login -u ****** -p ******** registry.redhat.io
docker login -u ****** -p ******** registry.access.redhat.com
docker login -u ****** -p ******** registry.connect.redhat.com

podman login -u ****** -p ******** registry.redhat.io
podman login -u ****** -p ******** registry.access.redhat.com
podman login -u ****** -p ******** registry.connect.redhat.com

# to download the pull-secret.json, open following link
# https://cloud.redhat.com/openshift/install/metal/user-provisioned
cat &lt;&lt; 'EOF' &gt; /data/pull-secret.json
{&quot;auths&quot;:{&quot;cloud.openshift.com&quot;:*********************
EOF

cat &lt;&lt; EOF &gt;&gt;  /etc/hosts
127.0.0.1 registry.redhat.ren
EOF

# 配置registry
mkdir -p /etc/crts/ &amp;&amp; cd /etc/crts

# https://access.redhat.com/documentation/en-us/red_hat_codeready_workspaces/2.1/html/installation_guide/installing-codeready-workspaces-in-tls-mode-with-self-signed-certificates_crw
openssl genrsa -out /etc/crts/redhat.ren.ca.key 4096
openssl req -x509 \
  -new -nodes \
  -key /etc/crts/redhat.ren.ca.key \
  -sha256 \
  -days 36500 \
  -out /etc/crts/redhat.ren.ca.crt \
  -subj /CN=&quot;Local Red Hat Ren Signer&quot; \
  -reqexts SAN \
  -extensions SAN \
  -config &lt;(cat /etc/pki/tls/openssl.cnf \
      &lt;(printf '[SAN]\nbasicConstraints=critical, CA:TRUE\nkeyUsage=keyCertSign, cRLSign, digitalSignature'))

openssl genrsa -out /etc/crts/redhat.ren.key 2048

openssl req -new -sha256 \
    -key /etc/crts/redhat.ren.key \
    -subj &quot;/O=Local Red Hat Ren /CN=*.ocp4.redhat.ren&quot; \
    -reqexts SAN \
    -config &lt;(cat /etc/pki/tls/openssl.cnf \
        &lt;(printf &quot;\n[SAN]\nsubjectAltName=DNS:*.ocp4.redhat.ren,DNS:*.apps.ocp4.redhat.ren,DNS:*.redhat.ren\nbasicConstraints=critical, CA:FALSE\nkeyUsage=digitalSignature, keyEncipherment, keyAgreement, dataEncipherment\nextendedKeyUsage=serverAuth&quot;)) \
    -out /etc/crts/redhat.ren.csr

openssl x509 \
    -req \
    -sha256 \
    -extfile &lt;(printf &quot;subjectAltName=DNS:*.ocp4.redhat.ren,DNS:*.apps.ocp4.redhat.ren,DNS:*.redhat.ren\nbasicConstraints=critical, CA:FALSE\nkeyUsage=digitalSignature, keyEncipherment, keyAgreement, dataEncipherment\nextendedKeyUsage=serverAuth&quot;) \
    -days 36500 \
    -in /etc/crts/redhat.ren.csr \
    -CA /etc/crts/redhat.ren.ca.crt \
    -CAkey /etc/crts/redhat.ren.ca.key \
    -CAcreateserial -out /etc/crts/redhat.ren.crt

openssl x509 -in /etc/crts/redhat.ren.crt -text

/bin/cp -f /etc/crts/redhat.ren.ca.crt /etc/pki/ca-trust/source/anchors/
update-ca-trust extract

cd /data/ocp4
# systemctl stop docker-distribution

/bin/rm -rf /data/registry
mkdir -p /data/registry
cat &lt;&lt; EOF &gt; /etc/docker-distribution/registry/config.yml
version: 0.1
log:
  fields:
    service: registry
storage:
    cache:
        layerinfo: inmemory
    filesystem:
        rootdirectory: /data/registry
    delete:
        enabled: true
http:
    addr: :5443
    tls:
       certificate: /etc/crts/redhat.ren.crt
       key: /etc/crts/redhat.ren.key
compatibility:
  schema1:
    enabled: true
EOF
# systemctl restart docker
# systemctl enable docker-distribution

# systemctl restart docker-distribution

# podman login registry.redhat.ren:5443 -u a -p a

systemctl enable --now docker-distribution

</code></pre>
<h2 id="operator-hub-catalog"><a class="header" href="#operator-hub-catalog">operator hub catalog</a></h2>
<pre><code class="language-bash">mkdir -p /data/ocp4
cd /data/ocp4

export BUILDNUMBER=4.6.28

wget -O openshift-client-linux-${BUILDNUMBER}.tar.gz https://mirror.openshift.com/pub/openshift-v4/clients/ocp/${BUILDNUMBER}/openshift-client-linux-${BUILDNUMBER}.tar.gz
wget -O openshift-install-linux-${BUILDNUMBER}.tar.gz https://mirror.openshift.com/pub/openshift-v4/clients/ocp/${BUILDNUMBER}/openshift-install-linux-${BUILDNUMBER}.tar.gz

tar -xzf openshift-client-linux-${BUILDNUMBER}.tar.gz -C /usr/local/sbin/
tar -xzf openshift-install-linux-${BUILDNUMBER}.tar.gz -C /usr/local/sbin/

wget -O operator.sh https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.6/scripts/operator.sh

bash operator.sh

# 2021.05.07.0344

</code></pre>
<h2 id="离线安装源制作"><a class="header" href="#离线安装源制作">离线安装源制作</a></h2>
<pre><code class="language-bash">rm -rf /data/ocp4
mkdir -p /data/ocp4
cd /data/ocp4
# wget -O build.dist.sh https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.6/scripts/build.dist.sh

# bash build.dist.sh

wget -O prepare.offline.content.sh https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.6/scripts/prepare.offline.content.sh

# git clone https://github.com/wangzheng422/docker_env.git
# cd docker_env
# git checkout dev
# cp redhat/ocp4/4.6/scripts/prepare.offline.content.sh /data/ocp4/
# cd /data/ocp4
# rm -rf docker_env

bash prepare.offline.content.sh -v 4.6.28, -m 4.6 -h 2021.05.07.0344

</code></pre>
<p>output of mirror of images</p>
<pre><code>Success
Update image:  registry.redhat.ren:5443/ocp4/openshift4:4.6.5
Mirror prefix: registry.redhat.ren:5443/ocp4/openshift4

To use the new mirrored repository to install, add the following section to the install-config.yaml:

imageContentSources:
- mirrors:
  - registry.redhat.ren:5443/ocp4/openshift4
  source: quay.io/openshift-release-dev/ocp-release
- mirrors:
  - registry.redhat.ren:5443/ocp4/openshift4
  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev


To use the new mirrored repository for upgrades, use the following to create an ImageContentSourcePolicy:

apiVersion: operator.openshift.io/v1alpha1
kind: ImageContentSourcePolicy
metadata:
  name: example
spec:
  repositoryDigestMirrors:
  - mirrors:
    - registry.redhat.ren:5443/ocp4/openshift4
    source: quay.io/openshift-release-dev/ocp-release
  - mirrors:
    - registry.redhat.ren:5443/ocp4/openshift4
    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev

########################################
##
Success
Update image:  openshift/release:4.3.3

To upload local images to a registry, run:

    oc image mirror --from-dir=/data/mirror_dir file://openshift/release:4.3.3* REGISTRY/REPOSITORY


</code></pre>
<h2 id="download-image-for-components"><a class="header" href="#download-image-for-components">download image for components</a></h2>
<pre><code class="language-bash">########################################
# your images
cd /data/ocp4/
export MIRROR_DIR='/data/install.image'
/bin/rm -rf ${MIRROR_DIR}
bash add.image.sh install.image.list ${MIRROR_DIR}

export MIRROR_DIR='/data/poc.image'
/bin/rm -rf ${MIRROR_DIR}
bash add.image.sh poc.image.list ${MIRROR_DIR}

########################################
# common function
build_image_list() {
  VAR_INPUT_FILE=$1
  VAR_OUTPUT_FILE=$2
  VAR_OPERATOR=$3

  VAR_FINAL=`cat $VAR_INPUT_FILE | grep $VAR_OPERATOR | awk '{if ($2) print $2;}' | sort | uniq | tail -1`

  echo $VAR_FINAL

  cat $VAR_INPUT_FILE | grep $VAR_FINAL | awk '{if ($2) print $1;}' &gt;&gt; $VAR_OUTPUT_FILE
}

########################################
# redhat operator hub
export MIRROR_DIR='/data/redhat-operator'

/bin/rm -rf ${MIRROR_DIR}
/bin/rm -f /data/ocp4/mapping-redhat.list
wanted_operator_list=$(cat redhat-operator-image.list | awk '{if ($2) print $2;}' \
  | sed 's/\..*//g' | sort | uniq
)

while read -r line; do
    build_image_list '/data/ocp4/redhat-operator-image.list' '/data/ocp4/mapping-redhat.list' $line
done &lt;&lt;&lt; &quot;$wanted_operator_list&quot;

bash add.image.sh mapping-redhat.list ${MIRROR_DIR}

# /bin/cp -f pull.add.image.failed.list pull.add.image.failed.list.bak
# bash add.image.resume.sh pull.add.image.failed.list.bak ${MIRROR_DIR}

cd ${MIRROR_DIR%/*}
tar cf - echo ${MIRROR_DIR##*/}/ | pigz -c &gt; echo ${MIRROR_DIR##*/}.tgz 

# to load image back
bash add.image.load.sh '/data/redhat-operator' 'registry.redhat.ren:5443'

######################################
# certified operator hub
export MIRROR_DIR='/data/certified-operator'

/bin/rm -rf ${MIRROR_DIR}
/bin/rm -f /data/ocp4/mapping-certified.list
wanted_operator_list=$(cat certified-operator-image.list | awk '{if ($2) print $2;}' \
  | sed 's/\..*//g' | sort | uniq
)

while read -r line; do
    build_image_list '/data/ocp4/certified-operator-image.list' '/data/ocp4/mapping-certified.list' $line
done &lt;&lt;&lt; &quot;$wanted_operator_list&quot;

bash add.image.sh mapping-certified.list ${MIRROR_DIR}

# /bin/cp -f pull.add.image.failed.list pull.add.image.failed.list.bak
# bash add.image.resume.sh pull.add.image.failed.list.bak ${MIRROR_DIR}

cd ${MIRROR_DIR%/*}
tar cf - echo ${MIRROR_DIR##*/}/ | pigz -c &gt; echo ${MIRROR_DIR##*/}.tgz 

# bash add.image.sh mapping-certified.txt

#######################################
# community operator hub
export MIRROR_DIR='/data/community-operator'

/bin/rm -rf ${MIRROR_DIR}
/bin/rm -f /data/ocp4/mapping-community.list
wanted_operator_list=$(cat community-operator-image.list | awk '{if ($2) print $2;}' \
  | sed 's/\..*//g' | sort | uniq
)

while read -r line; do
    build_image_list '/data/ocp4/community-operator-image.list' '/data/ocp4/mapping-community.list' $line
done &lt;&lt;&lt; &quot;$wanted_operator_list&quot;

bash add.image.sh mapping-community.list ${MIRROR_DIR}

# /bin/cp -f pull.add.image.failed.list pull.add.image.failed.list.bak
# bash add.image.resume.sh pull.add.image.failed.list.bak ${MIRROR_DIR}

cd ${MIRROR_DIR%/*}
tar cf - echo ${MIRROR_DIR##*/}/ | pigz -c &gt; echo ${MIRROR_DIR##*/}.tgz 

# bash add.image.sh mapping-community.txt

# to load image back
bash add.image.load.sh '/data/community-operator' 'registry.redhat.ren:5443'

#####################################
# samples operator
export MIRROR_DIR='/data/is.samples'

/bin/rm -rf ${MIRROR_DIR}
bash add.image.sh is.openshift.list  ${MIRROR_DIR}


</code></pre>
<h2 id="镜像仓库代理--image-registry-proxy"><a class="header" href="#镜像仓库代理--image-registry-proxy">镜像仓库代理 / image registry proxy</a></h2>
<p>准备离线镜像仓库非常麻烦，好在我们找到了一台在线的主机，那么我们可以使用nexus构造image registry proxy，在在线环境上面，做一遍PoC，然后就能通过image registry proxy得到离线镜像了</p>
<ul>
<li>https://mtijhof.wordpress.com/2018/07/23/using-nexus-oss-as-a-proxy-cache-for-docker-images/</li>
</ul>
<pre><code class="language-bash">#####################################################
# init build the nexus fs
mkdir -p /data/ccn/nexus-image
chown -R 200 /data/ccn/nexus-image

# podman run -d -p 8082:8081 -p 8083:8083 -it --name nexus-image -v /data/ccn/nexus-image:/nexus-data:Z docker.io/sonatype/nexus3:3.29.0

podman run -d -p 8082:8081 -p 8083:8083 -it --name nexus-image -v /data/ccn/nexus-image:/nexus-data:Z docker.io/wangzheng422/imgs:nexus3-3.29.0-wzh

podman stop nexus-image
podman rm nexus-image

# get the admin password
cat /data/ccn/nexus-image/admin.password &amp;&amp; echo
# 84091bcd-c82f-44a3-8b7b-dfc90f5b7da1

# open http://nexus.ocp4.redhat.ren:8082

# 开启 https
# https://blog.csdn.net/s7799653/article/details/105378645
# https://help.sonatype.com/repomanager3/system-configuration/configuring-ssl#ConfiguringSSL-InboundSSL-ConfiguringtoServeContentviaHTTPS
mkdir -p /data/install/tmp
cd /data/install/tmp

# 将证书导出成pkcs格式
# 这里需要输入密码  用 password，
openssl pkcs12 -export -out keystore.pkcs12 -inkey /etc/crts/redhat.ren.key -in /etc/crts/redhat.ren.crt

cat &lt;&lt; EOF &gt;&gt; Dockerfile
FROM docker.io/sonatype/nexus3:3.29.0
USER root
COPY keystore.pkcs12 /keystore.pkcs12
RUN keytool -v -importkeystore -srckeystore keystore.pkcs12 -srcstoretype PKCS12 -destkeystore keystore.jks -deststoretype JKS -storepass password -srcstorepass password  &amp;&amp;\
    cp keystore.jks /opt/sonatype/nexus/etc/ssl/
USER nexus
EOF
buildah bud --format=docker -t docker.io/wangzheng422/imgs:nexus3-3.29.0-wzh -f Dockerfile .
buildah push docker.io/wangzheng422/imgs:nexus3-3.29.0-wzh

######################################################
# go to helper, update proxy setting for ocp cluster
cd /data/ocp4
bash image.registries.conf.sh nexus.ocp4.redhat.ren:8083

mkdir -p /etc/containers/registries.conf.d
/bin/cp -f image.registries.conf /etc/containers/registries.conf.d/

cd /data/ocp4
oc apply -f ./99-worker-container-registries.yaml -n openshift-config
oc apply -f ./99-master-container-registries.yaml -n openshift-config

######################################################
# dump the nexus image fs out
podman stop nexus-image

var_date=$(date '+%Y-%m-%d-%H%M')
echo $var_date
cd /data/ccn

tar cf - ./nexus-image | pigz -c &gt; nexus-image.tgz 
buildah from --name onbuild-container scratch
buildah copy onbuild-container nexus-image.tgz  /
buildah umount onbuild-container 
buildah commit --rm --format=docker onbuild-container docker.io/wangzheng422/nexus-fs:image-$var_date
# buildah rm onbuild-container
# rm -f nexus-image.tgz 
buildah push docker.io/wangzheng422/nexus-fs:image-$var_date
echo &quot;docker.io/wangzheng422/nexus-fs:image-$var_date&quot;

# 以下这个版本，可以作为初始化的image proxy，里面包含了nfs provision，以及sample operator的metadata。很高兴的发现，image stream并不会完全下载镜像，好想只是下载metadata，真正用的时候，才去下载。
# docker.io/wangzheng422/nexus-fs:image-2020-12-26-1118

##################################################
## call nexus api to get image list
# https://community.sonatype.com/t/how-can-i-get-a-list-of-tags-for-a-docker-image-akin-to-the-docker-hub-list/3210
# https://help.sonatype.com/repomanager3/rest-and-integration-api/search-api
curl -k -u admin:84091bcd-c82f-44a3-8b7b-dfc90f5b7da1 -X GET 'http://nexus.ocp4.redhat.ren:8082/service/rest/v1/search?repository=registry.redhat.io'

curl -u admin:84091bcd-c82f-44a3-8b7b-dfc90f5b7da1 -X GET 'http://nexus.ocp4.redhat.ren:8082/service/rest/v1/components?repository=registry.redhat.io'

podman pull docker.io/anoxis/registry-cli
podman run --rm anoxis/registry-cli -l admin:84091bcd-c82f-44a3-8b7b-dfc90f5b7da1 -r https://nexus.ocp4.redhat.ren:8083

# https://github.com/rpardini/docker-registry-proxy

REPO_URL=nexus.ocp4.redhat.ren:8083

curl -k -s -X GET https://$REPO_URL/v2/_catalog \
 | jq '.repositories[]' \
 | sort \
 | xargs -I _ curl -s -k -X GET https://$REPO_URL/v2/_/tags/list



##################################################
## prepare for baidu disk
mkdir -p /data/ccn/baidu
cd /data/ccn

tar cf - ./nexus-image | pigz -c &gt; /data/ccn/baidu/nexus-image.tgz 

cd /data/ccn/baidu
split -b 20000m nexus-image.tgz  nexus-image.tgz.
rm -f nexus-image.tgz

yum -y install python3-pip
pip3 install --user bypy 
/root/.local/bin/bypy list
/root/.local/bin/bypy upload

</code></pre>
<h2 id="upload-to-baidu-disk"><a class="header" href="#upload-to-baidu-disk">upload to baidu disk</a></h2>
<pre><code class="language-bash">export BUILDNUMBER=4.6.28

mkdir -p /data/bypy
cd /data
tar -cvf - ocp4/ | pigz -c &gt; /data/bypy/ocp.$BUILDNUMBER.tgz
tar -cvf - registry/ | pigz -c &gt; /data/bypy/registry.$BUILDNUMBER.tgz

cd /data/bypy
# https://github.com/houtianze/bypy
yum -y install python3-pip
pip3 install --user bypy 
/root/.local/bin/bypy list
/root/.local/bin/bypy upload


</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-46-静态ip离线-baremetal-安装包含operator-hub"><a class="header" href="#openshift-46-静态ip离线-baremetal-安装包含operator-hub">openshift 4.6 静态IP离线 baremetal 安装，包含operator hub</a></h1>
<h2 id="安装过程视频"><a class="header" href="#安装过程视频">安装过程视频</a></h2>
<p>本文描述ocp4.6在baremetal(kvm模拟)上面，静态ip安装的方法。包括operator hub步骤。</p>
<p><img src="ocp4/4.6/../4.5/dia/4.5.install.dia.drawio.svg" alt="架构图" /></p>
<p><img src="ocp4/4.6/../4.5/dia/4.5.disconnected.install.drawio.svg" alt="" /></p>
<h2 id="离线安装包下载"><a class="header" href="#离线安装包下载">离线安装包下载</a></h2>
<p>ocp4.3的离线安装包下载和3.11不太一样，按照如下方式准备。另外，由于默认的baremetal是需要dhcp, pxe环境的，那么需要准备一个工具机，上面有dhcp, tftp, haproxy等工具，另外为了方便项目现场工作，还准备了ignition文件的修改工具，所以离线安装包需要一些其他第三方的工具。</p>
<p>https://github.com/wangzheng422/ocp4-upi-helpernode 这个工具，是创建工具机用的。</p>
<p>https://github.com/wangzheng422/filetranspiler 这个工具，是修改ignition文件用的。</p>
<p>打包好的安装包，在这里下载，百度盘下载链接，版本是4.6.5:</p>
<p>链接: https://pan.baidu.com/s/1-5QWpayV2leinq4DOtiFEg  密码: gjoe</p>
<p>其中包括如下类型的文件：</p>
<ul>
<li>ocp4.tgz  这个文件包含了iso等安装介质，以及各种安装脚本，全部下载的镜像列表等。需要复制到宿主机，以及工具机上去。</li>
<li>registry.tgz  这个文件也是docker image registry的仓库打包文件。需要先补充镜像的话，按照这里操作: <a href="ocp4/4.6/./4.6.add.image.html">4.6.add.image.md</a></li>
<li>install.image.tgz  这个文件是安装集群的时候，需要的补充镜像.</li>
<li>rhel-data.7.9.tgz 这个文件是 rhel 7 主机的yum更新源，这么大是因为里面有gpu, epel等其他的东西。这个包主要用于安装宿主机，工具机，以及作为计算节点的rhel。</li>
</ul>
<p>合并这些切分文件，使用类似如下的命令</p>
<pre><code class="language-bash">cat registry.?? &gt; registry.tgz
</code></pre>
<h2 id="在外网云主机上面准备离线安装源"><a class="header" href="#在外网云主机上面准备离线安装源">在外网云主机上面准备离线安装源</a></h2>
<p>准备离线安装介质的文档，已经转移到了这里：<a href="ocp4/4.6/4.6.build.dist.html">4.6.build.dist.md</a></p>
<h2 id="宿主机准备"><a class="header" href="#宿主机准备">宿主机准备</a></h2>
<p>本次实验，是在一个32C， 256G 的主机上面，用很多个虚拟机安装测试。所以先准备这个宿主机。</p>
<p>如果是多台宿主机，记得一定要调整时间配置，让这些宿主机的时间基本一致，否则证书会出问题。</p>
<p>主要的准备工作有</p>
<ul>
<li>配置yum源</li>
<li>配置dns</li>
<li>安装镜像仓库</li>
<li>配置vnc环境</li>
<li>配置kvm需要的网络</li>
<li>创建helper kvm</li>
<li>配置一个haproxy，从外部导入流量给kvm</li>
</ul>
<p>以上准备工作，dns部分需要根据实际项目环境有所调整。</p>
<p>本次的宿主机是一台rhel7</p>
<pre><code class="language-bash">cat &lt;&lt; EOF &gt;&gt;  /etc/hosts
127.0.0.1 registry.ocp4.redhat.ren
EOF

# 准备yum更新源
mkdir /etc/yum.repos.d.bak
mv /etc/yum.repos.d/* /etc/yum.repos.d.bak
cat &lt;&lt; EOF &gt; /etc/yum.repos.d/remote.repo
[remote]
name=RHEL FTP
baseurl=ftp://127.0.0.1/data
enabled=1
gpgcheck=0

EOF

yum clean all
yum repolist

yum -y install byobu htop 

systemctl disable --now firewalld

# 配置registry
mkdir -p /etc/crts/ &amp;&amp; cd /etc/crts
openssl req \
   -newkey rsa:2048 -nodes -keyout redhat.ren.key \
   -x509 -days 3650 -out redhat.ren.crt -subj \
   &quot;/C=CN/ST=GD/L=SZ/O=Global Security/OU=IT Department/CN=*.ocp4.redhat.ren&quot; \
   -config &lt;(cat /etc/pki/tls/openssl.cnf \
        &lt;(printf &quot;[SAN]\nsubjectAltName=DNS:registry.ocp4.redhat.ren,DNS:*.ocp4.redhat.ren,DNS:*.redhat.ren&quot;))

/bin/cp -f /etc/crts/redhat.ren.crt /etc/pki/ca-trust/source/anchors/
update-ca-trust extract

cd /data
mkdir -p /data/registry
# tar zxf registry.tgz
yum -y install podman docker-distribution pigz skopeo
# pigz -dc registry.tgz | tar xf -
cat &lt;&lt; EOF &gt; /etc/docker-distribution/registry/config.yml
version: 0.1
log:
  fields:
    service: registry
storage:
    cache:
        layerinfo: inmemory
    filesystem:
        rootdirectory: /data/4.6.5/registry
    delete:
        enabled: true
http:
    addr: :5443
    tls:
       certificate: /etc/crts/redhat.ren.crt
       key: /etc/crts/redhat.ren.key
compatibility:
  schema1:
    enabled: true
EOF
# systemctl restart docker
# systemctl stop docker-distribution
systemctl enable --now docker-distribution
# systemctl restart docker-distribution
# podman login registry.redhat.ren:5443 -u a -p a

# firewall-cmd --permanent --add-port=5443/tcp
# firewall-cmd --reload

# 加载更多的镜像
# 解压缩 ocp4.tgz
bash add.image.load.sh /data/4.6.5/install.image 'registry.ocp4.redhat.ren:5443'

# https://github.com/christianh814/ocp4-upi-helpernode/blob/master/docs/quickstart.md

# 准备vnc环境

yum -y install tigervnc-server tigervnc gnome-terminal gnome-session \
  gnome-classic-session gnome-terminal nautilus-open-terminal \
  control-center liberation-mono-fonts google-noto-sans-cjk-fonts \
  google-noto-sans-fonts fonts-tweak-tool

yum install -y    qgnomeplatform   xdg-desktop-portal-gtk \
  NetworkManager-libreswan-gnome   PackageKit-command-not-found \
  PackageKit-gtk3-module   abrt-desktop   at-spi2-atk   at-spi2-core   \
  avahi   baobab   caribou   caribou-gtk2-module   caribou-gtk3-module   \
  cheese   compat-cheese314   control-center   dconf   empathy   eog   \
  evince   evince-nautilus   file-roller   file-roller-nautilus   \
  firewall-config   firstboot   fprintd-pam   gdm   gedit   glib-networking   \
  gnome-bluetooth   gnome-boxes   gnome-calculator   gnome-classic-session   \
  gnome-clocks   gnome-color-manager   gnome-contacts   gnome-dictionary   \
  gnome-disk-utility   gnome-font-viewer   gnome-getting-started-docs   \
  gnome-icon-theme   gnome-icon-theme-extras   gnome-icon-theme-symbolic   \
  gnome-initial-setup   gnome-packagekit   gnome-packagekit-updater   \
  gnome-screenshot   gnome-session   gnome-session-xsession   \
  gnome-settings-daemon   gnome-shell   gnome-software   gnome-system-log   \
  gnome-system-monitor   gnome-terminal   gnome-terminal-nautilus   \
  gnome-themes-standard   gnome-tweak-tool   nm-connection-editor   orca   \
  redhat-access-gui   sane-backends-drivers-scanners   seahorse   \
  setroubleshoot   sushi   totem   totem-nautilus   vinagre   vino   \
  xdg-user-dirs-gtk   yelp

yum install -y    cjkuni-uming-fonts   dejavu-sans-fonts   \
  dejavu-sans-mono-fonts   dejavu-serif-fonts   gnu-free-mono-fonts   \
  gnu-free-sans-fonts   gnu-free-serif-fonts   \
  google-crosextra-caladea-fonts   google-crosextra-carlito-fonts   \
  google-noto-emoji-fonts   jomolhari-fonts   khmeros-base-fonts   \
  liberation-mono-fonts   liberation-sans-fonts   liberation-serif-fonts   \
  lklug-fonts   lohit-assamese-fonts   lohit-bengali-fonts   \
  lohit-devanagari-fonts   lohit-gujarati-fonts   lohit-kannada-fonts   \
  lohit-malayalam-fonts   lohit-marathi-fonts   lohit-nepali-fonts   \
  lohit-oriya-fonts   lohit-punjabi-fonts   lohit-tamil-fonts   \
  lohit-telugu-fonts   madan-fonts   nhn-nanum-gothic-fonts   \
  open-sans-fonts   overpass-fonts   paktype-naskh-basic-fonts   \
  paratype-pt-sans-fonts   sil-abyssinica-fonts   sil-nuosu-fonts   \
  sil-padauk-fonts   smc-meera-fonts   stix-fonts   \
  thai-scalable-waree-fonts   ucs-miscfixed-fonts   vlgothic-fonts   \
  wqy-microhei-fonts   wqy-zenhei-fonts

vncpasswd

cat &lt;&lt; EOF &gt; ~/.vnc/xstartup
#!/bin/sh
unset SESSION_MANAGER
unset DBUS_SESSION_BUS_ADDRESS
vncconfig &amp;
gnome-session &amp;
EOF
chmod +x ~/.vnc/xstartup

vncserver :1 -geometry 1280x800
# 如果你想停掉vnc server，这么做
vncserver -kill :1

# firewall-cmd --permanent --add-port=6001/tcp
# firewall-cmd --permanent --add-port=5901/tcp
# firewall-cmd --reload

# connect vnc at port 5901
# export DISPLAY=:1

# https://www.cyberciti.biz/faq/how-to-install-kvm-on-centos-7-rhel-7-headless-server/

# 配置kvm环境
yum -y install qemu-kvm libvirt libvirt-python libguestfs-tools virt-install virt-viewer virt-manager

systemctl enable libvirtd
systemctl start libvirtd

lsmod | grep -i kvm
brctl show
virsh net-list
virsh net-dumpxml default

# 创建实验用虚拟网络

cat &lt;&lt; EOF &gt;  /data/virt-net.xml
&lt;network&gt;
  &lt;name&gt;openshift4&lt;/name&gt;
  &lt;forward mode='nat'&gt;
    &lt;nat&gt;
      &lt;port start='1024' end='65535'/&gt;
    &lt;/nat&gt;
  &lt;/forward&gt;
  &lt;bridge name='openshift4' stp='on' delay='0'/&gt;
  &lt;domain name='openshift4'/&gt;
  &lt;ip address='192.168.7.1' netmask='255.255.255.0'&gt;
  &lt;/ip&gt;
&lt;/network&gt;
EOF

virsh net-define --file virt-net.xml
virsh net-autostart openshift4
virsh net-start openshift4

# restore back
virsh net-destroy openshift4
virsh net-undefine openshift4

# 创建工具机

mkdir -p /data/kvm
cd /data/kvm

lvremove -f datavg/helperlv
lvcreate -y -L 430G -n helperlv datavg

virt-install --name=&quot;ocp4-aHelper&quot; --vcpus=2 --ram=4096 \
--disk path=/dev/datavg/helperlv,device=disk,bus=virtio,format=raw \
--os-variant centos7.0 --network network=openshift4,model=virtio \
--boot menu=on --location /data/kvm/rhel-server-7.8-x86_64-dvd.iso \
--initrd-inject helper-ks.cfg --extra-args &quot;inst.ks=file:/helper-ks.cfg&quot; 

# virt-viewer --domain-name ocp4-aHelper
# virsh start ocp4-aHelper
# virsh list --all

# start chrony/ntp server on host
cat &lt;&lt; EOF &gt; /etc/chrony.conf
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
allow 192.0.0.0/8
local stratum 10
logdir /var/log/chrony
EOF
systemctl enable --now chronyd
# systemctl restart chronyd
chronyc tracking
chronyc sources -v
chronyc sourcestats -v
chronyc makestep

</code></pre>
<h2 id="工具机准备"><a class="header" href="#工具机准备">工具机准备</a></h2>
<p>以下是在工具机里面，进行的安装操作。</p>
<p>主要的操作有</p>
<ul>
<li>配置yum源</li>
<li>运行ansible脚本，自动配置工具机</li>
<li>上传定制的安装配置文件</li>
<li>生成ignition文件</li>
</ul>
<pre><code class="language-bash">
sed -i 's/#UseDNS yes/UseDNS no/g' /etc/ssh/sshd_config
systemctl restart sshd

cat &lt;&lt; EOF &gt; /root/.ssh/config
StrictHostKeyChecking no
UserKnownHostsFile=/dev/null
EOF

# in helper node
mkdir /etc/yum.repos.d.bak
mv /etc/yum.repos.d/* /etc/yum.repos.d.bak/
cat &lt;&lt; EOF &gt; /etc/yum.repos.d/remote.repo
[remote]
name=RHEL FTP
baseurl=ftp://192.168.7.1/data
enabled=1
gpgcheck=0

EOF

yum clean all
yum repolist

yum -y install ansible git unzip podman python36

mkdir -p /data/ocp4/
# scp ocp4.tgz to /data
cd /data
tar zvxf ocp4.tgz
cd /data/ocp4

# 这里使用了一个ansible的项目，用来部署helper节点的服务。
# https://github.com/wangzheng422/ocp4-upi-helpernode
unzip ocp4-upi-helpernode.zip
# 这里使用了一个ignition文件合并的项目，用来帮助自定义ignition文件。
# https://github.com/wangzheng422/filetranspiler
podman load -i filetranspiler.tgz

# 接下来，我们使用ansible来配置helper节点，装上各种openshift集群需要的服务
# 根据现场环境，修改 ocp4-upi-helpernode-master/vars-static.yaml
# 主要是修改各个节点的网卡和硬盘参数，还有IP地址
cd /data/ocp4/ocp4-upi-helpernode-master
ansible-playbook -e @vars-static.yaml -e '{staticips: true}' tasks/main.yml

# try this:
/usr/local/bin/helpernodecheck

mkdir -p /data/install

# GOTO image registry host
# copy crt files to helper node
scp /etc/crts/redhat.ren.ca.crt root@192.168.7.11:/data/install/
scp /etc/crts/redhat.ren.crt root@192.168.7.11:/data/install/
scp /etc/crts/redhat.ren.key root@192.168.7.11:/data/install/

# GO back to help node
/bin/cp -f /data/install/redhat.ren.crt /etc/pki/ca-trust/source/anchors/
update-ca-trust extract

# 定制ignition
cd /data/install

# 根据现场环境，修改 install-config.yaml
# 至少要修改ssh key， 还有 additionalTrustBundle，这个是镜像仓库的csr 

# vi install-config.yaml 
cat &lt;&lt; EOF &gt; /data/install/install-config.yaml 
apiVersion: v1
baseDomain: redhat.ren
compute:
- hyperthreading: Enabled
  name: worker
  replicas: 3
controlPlane:
  hyperthreading: Enabled
  name: master
  replicas: 3
metadata:
  name: ocp4
networking:
  clusterNetworks:
  - cidr: 10.254.0.0/16
    hostPrefix: 24
  networkType: OpenShiftSDN
  serviceNetwork:
  - 172.30.0.0/16
platform:
  none: {}
pullSecret: '{&quot;auths&quot;:{&quot;registry.ocp4.redhat.ren:5443&quot;: {&quot;auth&quot;: &quot;ZHVtbXk6ZHVtbXk=&quot;,&quot;email&quot;: &quot;noemail@localhost&quot;},&quot;registry.ppa.redhat.ren:5443&quot;: {&quot;auth&quot;: &quot;ZHVtbXk6ZHVtbXk=&quot;,&quot;email&quot;: &quot;noemail@localhost&quot;}}}'
sshKey: |
$( cat /root/.ssh/helper_rsa.pub | sed 's/^/   /g' )
additionalTrustBundle: |
$( cat /data/install/redhat.ren.ca.crt | sed 's/^/   /g' )
imageContentSources:
- mirrors:
  - registry.ocp4.redhat.ren:5443/ocp4/openshift4
  source: quay.io/openshift-release-dev/ocp-release
- mirrors:
  - registry.ocp4.redhat.ren:5443/ocp4/openshift4
  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
EOF

cd /data/install/
/bin/rm -rf *.ign .openshift_install_state.json auth bootstrap manifests master*[0-9] worker*[0-9] 

openshift-install create ignition-configs --dir=/data/install

cd /data/ocp4/ocp4-upi-helpernode-master
# 我们来为每个主机，复制自己版本的ign，并复制到web server的目录下
ansible-playbook -e @vars-static.yaml -e '{staticips: true}' tasks/ign.yml
# 如果对每个主机有自己ign的独特需求，在这一步，去修改ign。

# 以下操作本来是想设置网卡地址，但是实践发现是不需要的。
# 保留在这里，是因为他可以在安装的时候注入文件，非常有用。
# mkdir -p bootstrap/etc/sysconfig/network-scripts/
# cat &lt;&lt;EOF &gt; bootstrap/etc/sysconfig/network-scripts/ifcfg-ens3
# DEVICE=ens3
# BOOTPROTO=none
# ONBOOT=yes
# IPADDR=192.168.7.12
# NETMASK=255.255.255.0
# GATEWAY=192.168.7.1
# DNS=192.168.7.11
# DNS1=192.168.7.11
# DNS2=192.168.7.1
# DOMAIN=redhat.ren
# PREFIX=24
# DEFROUTE=yes
# IPV6INIT=no
# EOF
# filetranspiler -i bootstrap.ign -f bootstrap -o bootstrap-static.ign
# /bin/cp -f bootstrap-static.ign /var/www/html/ignition/

# 我们为每个节点创建各自的iso文件
cd /data/ocp4/ocp4-upi-helpernode-master
ansible-playbook -e @vars-static.yaml -e '{staticips: true}' tasks/iso.yml

</code></pre>
<h2 id="回到宿主机"><a class="header" href="#回到宿主机">回到宿主机</a></h2>
<p>本来，到了这一步，就可以开始安装了，但是我们知道coreos装的时候，要手动输入很长的命令行，实际操作的时候，那是不可能输入对的，输入错一个字符，安装就失败，要重启，重新输入。。。</p>
<p>为了避免这种繁琐的操作，参考网上的做法，我们就需要为每个主机定制iso了。好在，之前的步骤，我们已经用ansible创建了需要的iso，我们把这些iso复制到宿主机上，就可以继续了。</p>
<p>这里面有一个坑，我们是不知道主机的网卡名称的，只能先用coreos iso安装启动一次，进入单用户模式以后，ip a 来查看以下，才能知道，一般来说，是ens3。</p>
<p>另外，如果是安装物理机，disk是哪个，也需要上述的方法，来看看具体的盘符。另外，推荐在物理机上安装rhel 8 来测试一下物理机是不是支持coreos。物理机安装的时候，遇到不写盘的问题，可以尝试添加启动参数： ignition.firstboot=1</p>
<pre><code class="language-bash"># on kvm host

export KVM_DIRECTORY=/data/kvm

cd ${KVM_DIRECTORY}
scp root@192.168.7.11:/data/install/*.iso ${KVM_DIRECTORY}/

create_lv() {
    var_name=$1
    lvremove -f datavg/$var_name
    lvcreate -y -L 120G -n $var_name datavg
    # wipefs --all --force /dev/datavg/$var_name
}

create_lv bootstraplv
create_lv master0lv
create_lv master1lv
create_lv master2lv
create_lv worker0lv
create_lv worker1lv
create_lv worker2lv

# finally, we can start install :)
# 你可以一口气把虚拟机都创建了，然后喝咖啡等着。
# 从这一步开始，到安装完毕，大概30分钟。
virt-install --name=ocp4-bootstrap --vcpus=4 --ram=8192 \
--disk path=/dev/datavg/bootstraplv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network network=openshift4,model=virtio \
--boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-bootstrap.iso   

# 想登录进coreos一探究竟？那么这么做
# ssh core@bootstrap 
# journalctl -b -f -u bootkube.service

virt-install --name=ocp4-master0 --vcpus=4 --ram=16384 \
--disk path=/dev/datavg/master0lv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network network=openshift4,model=virtio \
--boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-master-0.iso 

# ssh core@192.168.7.13

virt-install --name=ocp4-master1 --vcpus=4 --ram=16384 \
--disk path=/dev/datavg/master1lv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network network=openshift4,model=virtio \
--boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-master-1.iso 

virt-install --name=ocp4-master2 --vcpus=4 --ram=16384 \
--disk path=/dev/datavg/master2lv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network network=openshift4,model=virtio \
--boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-master-2.iso 

virt-install --name=ocp4-worker0 --vcpus=4 --ram=32768 \
--disk path=/dev/datavg/worker0lv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network network=openshift4,model=virtio \
--boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-worker-0.iso 

virt-install --name=ocp4-worker1 --vcpus=4 --ram=16384 \
--disk path=/dev/datavg/worker1lv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network network=openshift4,model=virtio \
--boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-worker-1.iso 

virt-install --name=ocp4-worker2 --vcpus=4 --ram=16384 \
--disk path=/dev/datavg/worker2lv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network network=openshift4,model=virtio \
--boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-worker-2.iso 

# on workstation
# open http://192.168.7.11:9000/
# to check

# if you want to stop or delete vm, try this
virsh list --all
virsh destroy ocp4-bootstrap
virsh destroy ocp4-master0 
virsh destroy ocp4-master1 
virsh destroy ocp4-master2 
virsh destroy ocp4-worker0 
virsh destroy ocp4-worker1 
virsh destroy ocp4-worker2
virsh undefine ocp4-bootstrap
virsh undefine ocp4-master0 
virsh undefine ocp4-master1 
virsh undefine ocp4-master2 
virsh undefine ocp4-worker0 
virsh undefine ocp4-worker1 
virsh undefine ocp4-worker2

</code></pre>
<h2 id="在工具机上面"><a class="header" href="#在工具机上面">在工具机上面</a></h2>
<p>这个时候，安装已经自动开始了，我们只需要回到工具机上静静的观察就可以了。</p>
<p>在bootstrap和装master阶段，用这个命令看进度。</p>
<pre><code class="language-bash">cd /data/install
export KUBECONFIG=/data/install/auth/kubeconfig
echo &quot;export KUBECONFIG=/data/install/auth/kubeconfig&quot; &gt;&gt; ~/.bashrc
oc completion bash | sudo tee /etc/bash_completion.d/openshift &gt; /dev/null

cd /data/install
openshift-install wait-for bootstrap-complete --log-level debug

</code></pre>
<p>一切正常的话，会看到这个。
<img src="ocp4/4.6/../imgs/2019-10-23-12-17-30.png" alt="" /></p>
<p>有时候证书会过期，验证方法是登录 bootstrap, 看看过期时间。如果确定过期，要清除所有的openshift-install生成配置文件的缓存，重新来过。</p>
<pre><code class="language-bash">echo | openssl s_client -connect localhost:6443 | openssl x509 -noout -text | grep Not
</code></pre>
<p>一般来说，如果在openshift-install这一步之前，按照文档，删除了缓存文件，就不会出现过期的现象。</p>
<pre><code class="language-bash">oc get nodes
</code></pre>
<p>这个时候，只能看到master，是因为worker的csr没有批准。如果虚拟机是一口气创建的，那么多半不会遇到下面的问题。</p>
<pre><code class="language-bash">oc get csr
</code></pre>
<p>会发现有很多没有被批准的
<img src="ocp4/4.6/../imgs/2019-10-22-15-19-58.png" alt="" /></p>
<p>批准之</p>
<pre><code class="language-bash">yum -y install jq
oc get csr | grep -v Approved
oc get csr -ojson | jq -r '.items[] | select(.status == {} ) | .metadata.name' | xargs oc adm certificate approve
# oc get csr -o name | xargs oc adm certificate approve
</code></pre>
<p>然后worker 节点cpu飙高，之后就能看到worker了。
<img src="ocp4/4.6/../imgs/2019-10-22-15-23-08.png" alt="" /></p>
<p>等一会，会看到这个，就对了。
<img src="ocp4/4.6/../imgs/2019-10-23-13-54-06.png" alt="" /></p>
<p>上面的操作完成以后，就可以完成最后的安装了</p>
<pre><code class="language-bash">openshift-install wait-for install-complete --log-level debug
# here is the output
# INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/data/install/auth/kubeconfig'
# INFO Access the OpenShift web-console here: https://console-openshift-console.apps.ocp4.redhat.ren
# INFO Login to the console with user: &quot;kubeadmin&quot;, and password: &quot;7MXaT-vqouq-UukdG-uzNEi&quot;
</code></pre>
<p>我们的工具机是带nfs的，那么就配置高档一些的nfs存储吧，不要用emptydir</p>
<pre><code class="language-bash">bash /data/ocp4/ocp4-upi-helpernode-master/files/nfs-provisioner-setup.sh

# oc edit configs.imageregistry.operator.openshift.io
# 修改 storage 部分
# storage:
#   pvc:
#     claim:
oc patch configs.imageregistry.operator.openshift.io cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Managed&quot;,&quot;storage&quot;:{&quot;pvc&quot;:{&quot;claim&quot;:&quot;&quot;}}}}' --type=merge

oc patch configs.imageregistry.operator.openshift.io cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Removed&quot;}}' --type=merge

oc get clusteroperator image-registry

oc get configs.imageregistry.operator.openshift.io cluster -o yaml

# 把imagepruner给停掉
# https://bugzilla.redhat.com/show_bug.cgi?id=1852501#c24
# oc patch imagepruner.imageregistry/cluster --patch '{&quot;spec&quot;:{&quot;suspend&quot;:true}}' --type=merge
# oc -n openshift-image-registry delete jobs --all

oc get configs.samples.operator.openshift.io/cluster -o yaml

oc patch configs.samples.operator.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Managed&quot;}}' --type=merge

oc patch configs.samples.operator.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Unmanaged&quot;}}' --type=merge

oc patch configs.samples.operator.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Removed&quot;}}' --type=merge

</code></pre>
<p>配置一下本地的dns ( 把 *.apps.ocp4.redhat.ren 配置成 192.168.7.11 ) ，指向工具机的haproxy，打开浏览器就能访问管理界面了
<img src="ocp4/4.6/../imgs/2019-10-22-16-01-03.png" alt="" /></p>
<h2 id="chronyntp-设置"><a class="header" href="#chronyntp-设置">chrony/NTP 设置</a></h2>
<p>在 ocp 4.6 里面，需要设定ntp同步，我们之前ansible脚本，已经创建好了ntp的mco配置，把他打到系统里面就好了。</p>
<pre><code class="language-bash">oc apply -f /data/ocp4/ocp4-upi-helpernode-master/machineconfig/

</code></pre>
<h2 id="operator-hub-离线安装"><a class="header" href="#operator-hub-离线安装">Operator Hub 离线安装</a></h2>
<p>https://docs.openshift.com/container-platform/4.2/operators/olm-restricted-networks.html</p>
<p>https://github.com/operator-framework/operator-registry</p>
<p>https://www.cnblogs.com/ericnie/p/11777384.html?from=timeline&amp;isappinstalled=0</p>
<p>https://access.redhat.com/documentation/en-us/openshift_container_platform/4.2/html-single/images/index</p>
<p>operator hub 准备分2个层次，一个是本文章描述的，制作operator hub的离线资源，并镜像operator 镜像。做到这一步，能够在离线部署的ocp4.2上，看到operator hub，并且能够部署operator。但是如果要用operator来部署要用的组件，那么operator会再去下载镜像，这个层次的镜像，也需要离线部署，但是由于每个operator需要的镜像都不一样，也没有统一的地方进行描述，所以需要各个项目现场，根据需要另外部署，本项目会尽量多的下载需要的镜像，但是目前无法避免遗漏。</p>
<pre><code class="language-bash"># on helper node， 在工具机上
cd /data/ocp4

# scp /etc/crts/redhat.ren.crt 192.168.7.11:/root/ocp4/
# https://docs.openshift.com/container-platform/4.4/builds/setting-up-trusted-ca.html
oc project openshift-config
oc create configmap ca.for.registry -n openshift-config \
    --from-file=registry.ocp4.redhat.ren..5443=/data/install/redhat.ren.crt
# 如果你想删除这个config map，这么做
# oc delete configmap ca.for.registry
oc patch image.config.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;additionalTrustedCA&quot;:{&quot;name&quot;:&quot;ca.for.registry&quot;}}}'  --type=merge
# oc patch image.config.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;registrySources&quot;:{&quot;insecureRegistries&quot;:[&quot;registry.redhat.ren&quot;]}}}'  --type=merge
oc get image.config.openshift.io/cluster -o yaml

# 以下这个步骤是官网文档要做的，实践中发现，disconnected环境不需要
# oc patch OperatorHub cluster --type json -p '[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/disableAllDefaultSources&quot;, &quot;value&quot;: true}]'
# 如果你不小心还是照着官网做了，用如下步骤删掉
# oc patch OperatorHub cluster --type json  -p '[{&quot;op&quot;: &quot;remove&quot;, &quot;path&quot;: &quot;/spec/disableAllDefaultSources&quot;}]'

oc patch OperatorHub cluster --type json \
    -p '[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/disableAllDefaultSources&quot;, &quot;value&quot;: true}]'

oc get OperatorHub cluster -o yaml

# yum -y install python36
# 根据项目现场情况，调整参数，运行以下命令，生成配置文件，指向内网镜像仓库
cd /data/ocp4/
bash image.registries.conf.sh registry.ocp4.redhat.ren:5443

# 由于某些ocp 4.2的更新机制，以下操作会触发集群更新，
# 集群节点会逐个重启，集群组件也会逐个重启，请等待集群重启完毕。
oc apply -f ./99-worker-container-registries.yaml -n openshift-config
oc apply -f ./99-master-container-registries.yaml -n openshift-config

# ！！！正常情况，以下操作不需要！！！
# 以下操作，删除mirror镜像信息，也会触发集群更新操作，请等待集群重启完毕
oc delete -f ./99-worker-container-registries.yaml -n openshift-config
oc delete -f ./99-master-container-registries.yaml -n openshift-config

watch oc get machineconfigpools

watch oc get node
</code></pre>
<p>从监控界面，能看到节点在升级，重启。
<img src="ocp4/4.6/../imgs/2019-11-04-12-07-09.png" alt="" /></p>
<pre><code class="language-bash">
# on helper node

# params for operator hub images
export var_date='2020.11.23.0135'
echo $var_date
export var_major_version='4.6'
echo ${var_major_version}

export LOCAL_REG='registry.ocp4.redhat.ren:5443'

# 如果想看到redhat的operator，这样做
# 镜像源在 docker.io/wangzheng422/operator-catalog:redhat-$var_major_version-$var_date
# 后面的参数，去build.dist.sh文件里面，查看
# var_date 和 var_major_version 参数得到
cat &lt;&lt;EOF &gt; redhat-operator-catalog.yaml
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: redhat-operators-catalog
  namespace: openshift-marketplace
spec:
  displayName: Red Hat Operators
  sourceType: grpc
  image: ${LOCAL_REG}/ocp4/operator-catalog:redhat-${var_major_version}-${var_date}
  publisher: Red Hat
EOF
oc create -f redhat-operator-catalog.yaml

# 如果想看到certified的operator，这样做
# 镜像源在 docker.io/wangzheng422/operator-catalog:certified-$var_major_version-$var_date
# 后面的参数，去build.dist.sh文件里面，查看
# var_date 和 var_major_version 参数得到
cat &lt;&lt;EOF &gt; certified-operator-catalog.yaml
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: certified-operator-catalog
  namespace: openshift-marketplace
spec:
  displayName: Certified Operators
  sourceType: grpc
  image: ${LOCAL_REG}/ocp4/operator-catalog:certified-${var_major_version}-${var_date}
  publisher: Red Hat
EOF
oc create -f certified-operator-catalog.yaml

# 如果想看到community的operator，这样做
# 镜像源在 docker.io/wangzheng422/operator-catalog:community-$var_major_version-$var_date
# 后面的参数，去build.dist.sh文件里面，查看
# var_date 和 var_major_version 参数得到
cat &lt;&lt;EOF &gt; community-operator-catalog.yaml
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: community-operator-catalog
  namespace: openshift-marketplace
spec:
  displayName: Community Operator
  sourceType: grpc
  image: ${LOCAL_REG}/ocp4/operator-catalog:community-${var_major_version}-${var_date}
  publisher: Red Hat
EOF
oc create -f community-operator-catalog.yaml

cat &lt;&lt;EOF &gt; marketplace-operator-catalog.yaml
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: redhat-marketplace-catalog
  namespace: openshift-marketplace
spec:
  displayName: Red Hat Marketplace
  sourceType: grpc
  image: ${LOCAL_REG}/ocp4/operator-catalog:redhat-marketplace-${var_major_version}-${var_date}
  publisher: Red Hat
EOF
oc create -f marketplace-operator-catalog.yaml

# 想删除这些离线operator hub，就这样做。
# find . -name &quot;*-operator-catalog.yaml&quot; -exec oc delete -f {} \;

oc get pods -n openshift-marketplace
oc get catalogsource -n openshift-marketplace
oc get packagemanifest -n openshift-marketplace

</code></pre>
<p>能看到operator 列表
<img src="ocp4/4.6/../imgs/2019-11-01-18-02-58.png" alt="" /></p>
<p>部署一个operator也能成功
<img src="ocp4/4.6/../imgs/2019-11-01-18-03-37.png" alt="" /></p>
<p><img src="ocp4/4.6/../imgs/2019-11-05-18-02-39.png" alt="" /></p>
<pre><code class="language-bash"># set master and worker combine
# https://github.com/openshift-telco/openshift4x-poc/blob/master/MASTER-WORKER-COMBINED.md
oc edit schedulers cluster
# apiVersion: config.openshift.io/v1
# kind: Scheduler
# metadata:
# name: cluster
# spec:
#     mastersSchedulable: true

</code></pre>
<h2 id="其他链接"><a class="header" href="#其他链接">其他链接</a></h2>
<p>https://www.cnblogs.com/ericnie/p/11764124.html</p>
<h2 id="以下是参考材料"><a class="header" href="#以下是参考材料">以下是参考材料</a></h2>
<p>https://blog.openshift.com/openshift-4-2-disconnected-install/</p>
<p>https://blog.openshift.com/openshift-4-bare-metal-install-quickstart/</p>
<p>https://github.com/christianh814/ocp4-upi-helpernode#ocp4-upi-helper-node-playbook</p>
<p>https://github.com/openshift/cluster-samples-operator/blob/master/manifests/image-references</p>
<p>https://github.com/e-minguez/ocp4-upi-bm-pxeless-staticips/blob/master/docs/12-post-installation.md</p>
<p>https://www.openshift.com/blog/deploying-a-upi-environment-for-openshift-4-1-on-vms-and-bare-metal</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-46-静态ip离线-baremetal-安装包含operator-hub-1"><a class="header" href="#openshift-46-静态ip离线-baremetal-安装包含operator-hub-1">openshift 4.6 静态IP离线 baremetal 安装，包含operator hub</a></h1>
<h2 id="安装过程视频-1"><a class="header" href="#安装过程视频-1">安装过程视频</a></h2>
<p>本文描述ocp4.6在baremetal(kvm模拟)上面，静态ip安装的方法。包括operator hub步骤。</p>
<!-- ![架构图](../4.5/dia/4.5.install.dia.drawio.svg) -->
<p><img src="ocp4/4.6/../4.5/dia/4.5.disconnected.install.drawio.svg" alt="" /></p>
<h2 id="离线安装包下载-1"><a class="header" href="#离线安装包下载-1">离线安装包下载</a></h2>
<p>ocp4.3的离线安装包下载和3.11不太一样，按照如下方式准备。另外，由于默认的baremetal是需要dhcp, pxe环境的，那么需要准备一个工具机，上面有dhcp, tftp, haproxy等工具，另外为了方便项目现场工作，还准备了ignition文件的修改工具，所以离线安装包需要一些其他第三方的工具。</p>
<p>https://github.com/wangzheng422/ocp4-upi-helpernode 这个工具，是创建工具机用的。</p>
<p>https://github.com/wangzheng422/filetranspiler 这个工具，是修改ignition文件用的。</p>
<p>打包好的安装包，在这里下载，百度盘下载链接，版本是4.6.28:</p>
<ul>
<li>链接: https://pan.baidu.com/s/1XFbiOAcz7nul-N9U0aDxHg  密码: 6qtt</li>
</ul>
<p>其中包括如下类型的文件：</p>
<ul>
<li>ocp4.tgz  这个文件包含了iso等安装介质，以及各种安装脚本，全部下载的镜像列表等。需要复制到宿主机，以及工具机上去。</li>
<li>registry.tgz  这个文件也是docker image registry的仓库打包文件。需要先补充镜像的话，按照这里操作: <a href="ocp4/4.6/./4.6.add.image.html">4.6.add.image.md</a></li>
<li>install.image.tgz  这个文件是安装集群的时候，需要的补充镜像.</li>
<li>rhel-data.7.9.tgz 这个文件是 rhel 7 主机的yum更新源，这么大是因为里面有gpu, epel等其他的东西。这个包主要用于安装宿主机，工具机，以及作为计算节点的rhel。</li>
</ul>
<p>合并这些切分文件，使用类似如下的命令</p>
<pre><code class="language-bash">cat registry.?? &gt; registry.tgz
</code></pre>
<h2 id="在外网云主机上面准备离线安装源-1"><a class="header" href="#在外网云主机上面准备离线安装源-1">在外网云主机上面准备离线安装源</a></h2>
<p>准备离线安装介质的文档，已经转移到了这里：<a href="ocp4/4.6/4.6.build.dist.html">4.6.build.dist.md</a></p>
<h2 id="宿主机准备-1"><a class="header" href="#宿主机准备-1">宿主机准备</a></h2>
<p>本次实验，是在一个32C， 256G 的主机上面，用很多个虚拟机安装测试。所以先准备这个宿主机。</p>
<p>如果是多台宿主机，记得一定要调整时间配置，让这些宿主机的时间基本一致，否则证书会出问题。</p>
<p>主要的准备工作有</p>
<ul>
<li>配置yum源</li>
<li>配置dns</li>
<li>安装镜像仓库</li>
<li>配置vnc环境</li>
<li>配置kvm需要的网络</li>
<li>创建helper kvm</li>
<li>配置一个haproxy，从外部导入流量给kvm</li>
</ul>
<p>以上准备工作，dns部分需要根据实际项目环境有所调整。</p>
<p>本次的宿主机是一台rhel8, 参考这里进行离线repo等基本的配置<a href="ocp4/4.6/../../rhel/rhel8.build.kernel.repo.cache.html">rhel8.build.kernel.repo.cache.md</a></p>
<pre><code class="language-bash">cat &lt;&lt; EOF &gt; /root/.ssh/config
StrictHostKeyChecking no
UserKnownHostsFile=/dev/null
EOF

cat &lt;&lt; EOF &gt;&gt;  /etc/hosts
127.0.0.1 registry.ocp4.redhat.ren
EOF

dnf clean all
dnf repolist

dnf -y install byobu htop 

systemctl disable --now firewalld

# 配置registry
mkdir -p /etc/crts/ &amp;&amp; cd /etc/crts
openssl req \
   -newkey rsa:2048 -nodes -keyout redhat.ren.key \
   -x509 -days 3650 -out redhat.ren.crt -subj \
   &quot;/C=CN/ST=GD/L=SZ/O=Global Security/OU=IT Department/CN=*.ocp4.redhat.ren&quot; \
   -config &lt;(cat /etc/pki/tls/openssl.cnf \
        &lt;(printf &quot;[SAN]\nsubjectAltName=DNS:registry.ocp4.redhat.ren,DNS:*.ocp4.redhat.ren,DNS:*.redhat.ren&quot;))

/bin/cp -f /etc/crts/redhat.ren.crt /etc/pki/ca-trust/source/anchors/
update-ca-trust extract

cd /data
mkdir -p /data/registry
# tar zxf registry.tgz
dnf -y install podman pigz skopeo jq 
# pigz -dc registry.tgz | tar xf -
cd /data/ocp4
podman load -i /data/ocp4/registry.tgz

podman run --name local-registry -p 5443:5000 \
  -d --restart=always \
  -v /data/registry/:/var/lib/registry:z \
  -v /etc/crts:/certs:z \
  -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/redhat.ren.crt \
  -e REGISTRY_HTTP_TLS_KEY=/certs/redhat.ren.key \
  docker.io/library/registry:2

# firewall-cmd --permanent --add-port=5443/tcp
# firewall-cmd --reload

# 加载更多的镜像
# 解压缩 ocp4.tgz
bash add.image.load.sh /data/install.image 'registry.ocp4.redhat.ren:5443'

# https://github.com/christianh814/ocp4-upi-helpernode/blob/master/docs/quickstart.md

# 准备vnc环境
vncpasswd

cat &lt;&lt; EOF &gt; ~/.vnc/config
session=gnome
securitytypes=vncauth,tlsvnc
desktop=sandbox
geometry=1280x800
alwaysshared
EOF

cat &lt;&lt; EOF &gt;&gt; /etc/tigervnc/vncserver.users
:1=root
EOF

systemctl start vncserver@:1
# 如果你想停掉vnc server，这么做
systemctl stop vncserver@:1

# firewall-cmd --permanent --add-port=6001/tcp
# firewall-cmd --permanent --add-port=5901/tcp
# firewall-cmd --reload

# connect vnc at port 5901
# export DISPLAY=:1

# 创建实验用虚拟网络

cat &lt;&lt; EOF &gt;  /data/kvm/virt-net.xml
&lt;network&gt;
  &lt;name&gt;openshift4&lt;/name&gt;
  &lt;forward mode='nat'&gt;
    &lt;nat&gt;
      &lt;port start='1024' end='65535'/&gt;
    &lt;/nat&gt;
  &lt;/forward&gt;
  &lt;bridge name='openshift4' stp='on' delay='0'/&gt;
  &lt;domain name='openshift4'/&gt;
  &lt;ip address='192.168.7.1' netmask='255.255.255.0'&gt;
  &lt;/ip&gt;
&lt;/network&gt;
EOF

virsh net-define --file /data/kvm/virt-net.xml
virsh net-autostart openshift4
virsh net-start openshift4

# restore back
virsh net-destroy openshift4
virsh net-undefine openshift4

# 创建工具机

mkdir -p /data/kvm
cd /data/kvm

lvremove -f rhel/helperlv
lvcreate -y -L 200G -n helperlv rhel

virt-install --name=&quot;ocp4-aHelper&quot; --vcpus=2 --ram=4096 \
--disk path=/dev/rhel/helperlv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network network=openshift4,model=virtio \
--boot menu=on --location /data/kvm/rhel-8.3-x86_64-dvd.iso \
--initrd-inject helper-ks-rhel8.cfg --extra-args &quot;inst.ks=file:/helper-ks-rhel8.cfg&quot; 

# restore kvm
virsh destroy ocp4-aHelper
virsh undefine ocp4-aHelper

# virt-viewer --domain-name ocp4-aHelper
# virsh start ocp4-aHelper
# virsh list --all

# start chrony/ntp server on host
/bin/cp -f /etc/chrony.conf /etc/chrony.conf.default
cat &lt;&lt; EOF &gt; /etc/chrony.conf
# pool 2.rhel.pool.ntp.org iburst
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
allow 192.0.0.0/8
local stratum 10
logdir /var/log/chrony
EOF
systemctl enable --now chronyd
# systemctl restart chronyd
chronyc tracking
chronyc sources -v
chronyc sourcestats -v
chronyc makestep

# setup ftp data root
mount --bind /data/dnf /var/ftp/dnf
chcon -R -t public_content_t  /var/ftp/dnf


</code></pre>
<h2 id="工具机准备-1"><a class="header" href="#工具机准备-1">工具机准备</a></h2>
<p>以下是在工具机里面，进行的安装操作。</p>
<p>主要的操作有</p>
<ul>
<li>配置yum源</li>
<li>运行ansible脚本，自动配置工具机</li>
<li>上传定制的安装配置文件</li>
<li>生成ignition文件</li>
</ul>
<pre><code class="language-bash">
sed -i 's/#UseDNS yes/UseDNS no/g' /etc/ssh/sshd_config
systemctl restart sshd

cat &lt;&lt; EOF &gt; /root/.ssh/config
StrictHostKeyChecking no
UserKnownHostsFile=/dev/null
EOF

# in helper node
mkdir /etc/yum.repos.d.bak
mv /etc/yum.repos.d/* /etc/yum.repos.d.bak

export YUMIP=&quot;192.168.7.1&quot;
cat &lt;&lt; EOF &gt; /etc/yum.repos.d/remote.repo
[remote-epel]
name=epel
baseurl=ftp://${YUMIP}/dnf/epel
enabled=1
gpgcheck=0

[remote-epel-modular]
name=epel-modular
baseurl=ftp://${YUMIP}/dnf/epel-modular
enabled=1
gpgcheck=0

[remote-appstream]
name=appstream
baseurl=ftp://${YUMIP}/dnf/rhel-8-for-x86_64-appstream-rpms
enabled=1
gpgcheck=0

[remote-baseos]
name=baseos
baseurl=ftp://${YUMIP}/dnf/rhel-8-for-x86_64-baseos-rpms
enabled=1
gpgcheck=0

[remote-baseos-source]
name=baseos-source
baseurl=ftp://${YUMIP}/dnf/rhel-8-for-x86_64-baseos-source-rpms
enabled=1
gpgcheck=0

[remote-supplementary]
name=supplementary
baseurl=ftp://${YUMIP}/dnf/rhel-8-for-x86_64-supplementary-rpms
enabled=1
gpgcheck=0

[remote-codeready-builder]
name=supplementary
baseurl=ftp://${YUMIP}/dnf/codeready-builder-for-rhel-8-x86_64-rpms
enabled=1
gpgcheck=0

EOF

yum clean all
yum makecache
yum repolist

yum -y install ansible git unzip podman python3

yum -y update

reboot

# yum -y install ansible git unzip podman python36

mkdir -p /data/ocp4/
# scp ocp4.tgz to /data
# scp /data/down/ocp4.tgz root@192.168.7.11:/data/
cd /data
tar zvxf ocp4.tgz
cd /data/ocp4

# 这里使用了一个ansible的项目，用来部署helper节点的服务。
# https://github.com/wangzheng422/ocp4-upi-helpernode
unzip ocp4-upi-helpernode.zip
# 这里使用了一个ignition文件合并的项目，用来帮助自定义ignition文件。
# https://github.com/wangzheng422/filetranspiler
podman load -i filetranspiler.tgz

# 接下来，我们使用ansible来配置helper节点，装上各种openshift集群需要的服务
# 根据现场环境，修改 ocp4-upi-helpernode-master/vars-static.yaml
# 主要是修改各个节点的网卡和硬盘参数，还有IP地址
cd /data/ocp4/ocp4-upi-helpernode-master
ansible-playbook -e @vars-static.rhel8.yaml -e '{staticips: true}' tasks/main.yml

# try this:
/usr/local/bin/helpernodecheck

mkdir -p /data/install

# GOTO image registry host
# copy crt files to helper node
scp /etc/crts/redhat.ren.ca.crt root@192.168.7.11:/data/install/
scp /etc/crts/redhat.ren.crt root@192.168.7.11:/data/install/
scp /etc/crts/redhat.ren.key root@192.168.7.11:/data/install/

# GO back to help node
/bin/cp -f /data/install/redhat.ren.crt /etc/pki/ca-trust/source/anchors/
update-ca-trust extract

# 定制ignition
cd /data/install

# 根据现场环境，修改 install-config.yaml
# 至少要修改ssh key， 还有 additionalTrustBundle，这个是镜像仓库的csr 

# vi install-config.yaml 
cat &lt;&lt; EOF &gt; /data/install/install-config.yaml 
apiVersion: v1
baseDomain: redhat.ren
compute:
- hyperthreading: Enabled
  name: worker
  replicas: 3
controlPlane:
  hyperthreading: Enabled
  name: master
  replicas: 3
metadata:
  name: ocp4
networking:
  clusterNetworks:
  - cidr: 10.254.0.0/16
    hostPrefix: 24
  networkType: OpenShiftSDN
  serviceNetwork:
  - 172.30.0.0/16
platform:
  none: {}
pullSecret: '{&quot;auths&quot;:{&quot;registry.ocp4.redhat.ren:5443&quot;: {&quot;auth&quot;: &quot;ZHVtbXk6ZHVtbXk=&quot;,&quot;email&quot;: &quot;noemail@localhost&quot;},&quot;registry.ppa.redhat.ren:5443&quot;: {&quot;auth&quot;: &quot;ZHVtbXk6ZHVtbXk=&quot;,&quot;email&quot;: &quot;noemail@localhost&quot;}}}'
sshKey: |
$( cat /root/.ssh/helper_rsa.pub | sed 's/^/   /g' )
additionalTrustBundle: |
$( cat /data/install/redhat.ren.ca.crt | sed 's/^/   /g' )
imageContentSources:
- mirrors:
  - registry.ocp4.redhat.ren:5443/ocp4/openshift4
  source: quay.io/openshift-release-dev/ocp-release
- mirrors:
  - registry.ocp4.redhat.ren:5443/ocp4/openshift4
  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
EOF

cd /data/install/
/bin/rm -rf *.ign .openshift_install_state.json auth bootstrap manifests master*[0-9] worker*[0-9] 

openshift-install create ignition-configs --dir=/data/install

cd /data/ocp4/ocp4-upi-helpernode-master
# 我们来为每个主机，复制自己版本的ign，并复制到web server的目录下
ansible-playbook -e @vars-static.rhel8.yaml -e '{staticips: true}' tasks/ign.yml
# 如果对每个主机有自己ign的独特需求，在这一步，去修改ign。

# 以下操作本来是想设置网卡地址，但是实践发现是不需要的。
# 保留在这里，是因为他可以在安装的时候注入文件，非常有用。
# mkdir -p bootstrap/etc/sysconfig/network-scripts/
# cat &lt;&lt;EOF &gt; bootstrap/etc/sysconfig/network-scripts/ifcfg-ens3
# DEVICE=ens3
# BOOTPROTO=none
# ONBOOT=yes
# IPADDR=192.168.7.12
# NETMASK=255.255.255.0
# GATEWAY=192.168.7.1
# DNS=192.168.7.11
# DNS1=192.168.7.11
# DNS2=192.168.7.1
# DOMAIN=redhat.ren
# PREFIX=24
# DEFROUTE=yes
# IPV6INIT=no
# EOF
# filetranspiler -i bootstrap.ign -f bootstrap -o bootstrap-static.ign
# /bin/cp -f bootstrap-static.ign /var/www/html/ignition/

# 我们为每个节点创建各自的iso文件
cd /data/ocp4/ocp4-upi-helpernode-master
ansible-playbook -e @vars-static.rhel8.yaml -e '{staticips: true}' tasks/iso.yml

</code></pre>
<h2 id="回到宿主机-1"><a class="header" href="#回到宿主机-1">回到宿主机</a></h2>
<p>本来，到了这一步，就可以开始安装了，但是我们知道coreos装的时候，要手动输入很长的命令行，实际操作的时候，那是不可能输入对的，输入错一个字符，安装就失败，要重启，重新输入。。。</p>
<p>为了避免这种繁琐的操作，参考网上的做法，我们就需要为每个主机定制iso了。好在，之前的步骤，我们已经用ansible创建了需要的iso，我们把这些iso复制到宿主机上，就可以继续了。</p>
<p>这里面有一个坑，我们是不知道主机的网卡名称的，只能先用coreos iso安装启动一次，进入单用户模式以后，ip a 来查看以下，才能知道，一般来说，是ens3。</p>
<p>另外，如果是安装物理机，disk是哪个，也需要上述的方法，来看看具体的盘符。另外，推荐在物理机上安装rhel 8 来测试一下物理机是不是支持coreos。物理机安装的时候，遇到不写盘的问题，可以尝试添加启动参数： ignition.firstboot=1</p>
<pre><code class="language-bash"># on kvm host

export KVM_DIRECTORY=/data/kvm

cd ${KVM_DIRECTORY}
scp root@192.168.7.11:/data/install/*.iso ${KVM_DIRECTORY}/

create_lv() {
    var_vg=$1
    var_lv=$2
    lvremove -f $var_vg/$var_lv
    lvcreate -y -L 120G -n $var_lv $var_vg
    # wipefs --all --force /dev/datavg/$var_name
}

create_lv rhel bootstraplv
create_lv nvme master0lv
create_lv nvme master1lv
create_lv nvme master2lv
create_lv rhel worker0lv
create_lv rhel worker1lv
create_lv rhel worker2lv

# finally, we can start install :)
# 你可以一口气把虚拟机都创建了，然后喝咖啡等着。
# 从这一步开始，到安装完毕，大概30分钟。
virt-install --name=ocp4-bootstrap --vcpus=4 --ram=8192 \
--disk path=/dev/rhel/bootstraplv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network network=openshift4,model=virtio \
--boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-bootstrap.iso   

# 想登录进coreos一探究竟？那么这么做
# ssh core@bootstrap
# journalctl -b -f -u bootkube.service

virt-install --name=ocp4-master0 --vcpus=4 --ram=16384 \
--disk path=/dev/nvme/master0lv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network network=openshift4,model=virtio \
--boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-master-0.iso 

# ssh core@192.168.7.13

virt-install --name=ocp4-master1 --vcpus=4 --ram=16384 \
--disk path=/dev/nvme/master1lv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network network=openshift4,model=virtio \
--boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-master-1.iso 

virt-install --name=ocp4-master2 --vcpus=4 --ram=16384 \
--disk path=/dev/nvme/master2lv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network network=openshift4,model=virtio \
--boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-master-2.iso 

virt-install --name=ocp4-worker0 --vcpus=4 --ram=32768 \
--disk path=/dev/rhel/worker0lv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network network=openshift4,model=virtio \
--boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-worker-0.iso 

virt-install --name=ocp4-worker1 --vcpus=4 --ram=16384 \
--disk path=/dev/rhel/worker1lv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network network=openshift4,model=virtio \
--boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-worker-1.iso 

virt-install --name=ocp4-worker2 --vcpus=4 --ram=16384 \
--disk path=/dev/rhel/worker2lv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network network=openshift4,model=virtio \
--boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-worker-2.iso 

# on workstation
# open http://192.168.7.11:9000/
# to check

# if you want to stop or delete vm, try this
virsh list --all
virsh destroy ocp4-bootstrap
virsh destroy ocp4-master0 
virsh destroy ocp4-master1 
virsh destroy ocp4-master2 
virsh destroy ocp4-worker0 
virsh destroy ocp4-worker1 
virsh destroy ocp4-worker2
virsh undefine ocp4-bootstrap
virsh undefine ocp4-master0 
virsh undefine ocp4-master1 
virsh undefine ocp4-master2 
virsh undefine ocp4-worker0 
virsh undefine ocp4-worker1 
virsh undefine ocp4-worker2

</code></pre>
<h2 id="在工具机上面-1"><a class="header" href="#在工具机上面-1">在工具机上面</a></h2>
<p>这个时候，安装已经自动开始了，我们只需要回到工具机上静静的观察就可以了。</p>
<p>在bootstrap和装master阶段，用这个命令看进度。</p>
<pre><code class="language-bash">cd /data/ocp4
export KUBECONFIG=/data/install/auth/kubeconfig
echo &quot;export KUBECONFIG=/data/install/auth/kubeconfig&quot; &gt;&gt; ~/.bashrc
oc completion bash | sudo tee /etc/bash_completion.d/openshift &gt; /dev/null

cd /data/install
openshift-install wait-for bootstrap-complete --log-level debug

</code></pre>
<p>一切正常的话，会看到这个。
<img src="ocp4/4.6/../imgs/2019-10-23-12-17-30.png" alt="" /></p>
<p>有时候证书会过期，验证方法是登录 bootstrap, 看看过期时间。如果确定过期，要清除所有的openshift-install生成配置文件的缓存，重新来过。</p>
<pre><code class="language-bash">echo | openssl s_client -connect localhost:6443 | openssl x509 -noout -text | grep Not
</code></pre>
<p>一般来说，如果在openshift-install这一步之前，按照文档，删除了缓存文件，就不会出现过期的现象。</p>
<pre><code class="language-bash">oc get nodes
</code></pre>
<p>这个时候，只能看到master，是因为worker的csr没有批准。如果虚拟机是一口气创建的，那么多半不会遇到下面的问题。</p>
<pre><code class="language-bash">oc get csr
</code></pre>
<p>会发现有很多没有被批准的
<img src="ocp4/4.6/../imgs/2019-10-22-15-19-58.png" alt="" /></p>
<p>批准之</p>
<pre><code class="language-bash">yum -y install jq
oc get csr | grep -v Approved
oc get csr -ojson | jq -r '.items[] | select(.status == {} ) | .metadata.name' | xargs oc adm certificate approve
# oc get csr -o name | xargs oc adm certificate approve
</code></pre>
<p>然后worker 节点cpu飙高，之后就能看到worker了。
<img src="ocp4/4.6/../imgs/2019-10-22-15-23-08.png" alt="" /></p>
<p>等一会，会看到这个，就对了。
<img src="ocp4/4.6/../imgs/2019-10-23-13-54-06.png" alt="" /></p>
<p>上面的操作完成以后，就可以完成最后的安装了</p>
<pre><code class="language-bash">openshift-install wait-for install-complete --log-level debug
# here is the output
# INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/data/install/auth/kubeconfig'
# INFO Access the OpenShift web-console here: https://console-openshift-console.apps.ocp4.redhat.ren
# INFO Login to the console with user: &quot;kubeadmin&quot;, and password: &quot;6yL7t-uDCaN-6grKP-VtYkx&quot;
</code></pre>
<p>我们的工具机是带nfs的，那么就配置高档一些的nfs存储吧，不要用emptydir</p>
<pre><code class="language-bash">bash /data/ocp4/ocp4-upi-helpernode-master/files/nfs-provisioner-setup.sh

# oc edit configs.imageregistry.operator.openshift.io
# 修改 storage 部分
# storage:
#   pvc:
#     claim:
oc patch configs.imageregistry.operator.openshift.io cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Managed&quot;,&quot;storage&quot;:{&quot;pvc&quot;:{&quot;claim&quot;:&quot;&quot;}}}}' --type=merge

oc patch configs.imageregistry.operator.openshift.io cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Removed&quot;}}' --type=merge

oc get clusteroperator image-registry

oc get configs.imageregistry.operator.openshift.io cluster -o yaml

# 把imagepruner给停掉
# https://bugzilla.redhat.com/show_bug.cgi?id=1852501#c24
# oc patch imagepruner.imageregistry/cluster --patch '{&quot;spec&quot;:{&quot;suspend&quot;:true}}' --type=merge
# oc -n openshift-image-registry delete jobs --all

oc get configs.samples.operator.openshift.io/cluster -o yaml

oc patch configs.samples.operator.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Managed&quot;}}' --type=merge

oc patch configs.samples.operator.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Unmanaged&quot;}}' --type=merge

oc patch configs.samples.operator.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Removed&quot;}}' --type=merge

</code></pre>
<p>配置一下本地的dns ( 把 *.apps.ocp4.redhat.ren 配置成 192.168.7.11 ) ，指向工具机的haproxy，打开浏览器就能访问管理界面了
<img src="ocp4/4.6/../imgs/2019-10-22-16-01-03.png" alt="" /></p>
<h2 id="chronyntp-设置-1"><a class="header" href="#chronyntp-设置-1">chrony/NTP 设置</a></h2>
<p>在 ocp 4.6 里面，需要设定ntp同步，我们之前ansible脚本，已经创建好了ntp的mco配置，把他打到系统里面就好了。</p>
<pre><code class="language-bash">oc apply -f /data/ocp4/ocp4-upi-helpernode-master/machineconfig/

</code></pre>
<h2 id="operator-hub-离线安装-1"><a class="header" href="#operator-hub-离线安装-1">Operator Hub 离线安装</a></h2>
<p>https://docs.openshift.com/container-platform/4.2/operators/olm-restricted-networks.html</p>
<p>https://github.com/operator-framework/operator-registry</p>
<p>https://www.cnblogs.com/ericnie/p/11777384.html?from=timeline&amp;isappinstalled=0</p>
<p>https://access.redhat.com/documentation/en-us/openshift_container_platform/4.2/html-single/images/index</p>
<p>operator hub 准备分2个层次，一个是本文章描述的，制作operator hub的离线资源，并镜像operator 镜像。做到这一步，能够在离线部署的ocp4.2上，看到operator hub，并且能够部署operator。但是如果要用operator来部署要用的组件，那么operator会再去下载镜像，这个层次的镜像，也需要离线部署，但是由于每个operator需要的镜像都不一样，也没有统一的地方进行描述，所以需要各个项目现场，根据需要另外部署，本项目会尽量多的下载需要的镜像，但是目前无法避免遗漏。</p>
<pre><code class="language-bash"># on helper node， 在工具机上
cd /data/ocp4

# scp /etc/crts/redhat.ren.crt 192.168.7.11:/root/ocp4/
# https://docs.openshift.com/container-platform/4.4/builds/setting-up-trusted-ca.html
oc project openshift-config
oc create configmap ca.for.registry -n openshift-config \
    --from-file=registry.ocp4.redhat.ren..5443=/data/install/redhat.ren.crt
# 如果你想删除这个config map，这么做
# oc delete configmap ca.for.registry
oc patch image.config.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;additionalTrustedCA&quot;:{&quot;name&quot;:&quot;ca.for.registry&quot;}}}'  --type=merge
# oc patch image.config.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;registrySources&quot;:{&quot;insecureRegistries&quot;:[&quot;registry.redhat.ren&quot;]}}}'  --type=merge
oc get image.config.openshift.io/cluster -o yaml

# 以下这个步骤是官网文档要做的，实践中发现，disconnected环境不需要
# oc patch OperatorHub cluster --type json -p '[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/disableAllDefaultSources&quot;, &quot;value&quot;: true}]'
# 如果你不小心还是照着官网做了，用如下步骤删掉
# oc patch OperatorHub cluster --type json  -p '[{&quot;op&quot;: &quot;remove&quot;, &quot;path&quot;: &quot;/spec/disableAllDefaultSources&quot;}]'

oc patch OperatorHub cluster --type json \
    -p '[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/disableAllDefaultSources&quot;, &quot;value&quot;: true}]'

oc get OperatorHub cluster -o yaml

# yum -y install python36
# 根据项目现场情况，调整参数，运行以下命令，生成配置文件，指向内网镜像仓库
cd /data/ocp4/
bash image.registries.conf.sh registry.ocp4.redhat.ren:5443

# 由于某些ocp 4.2的更新机制，以下操作会触发集群更新，
# 集群节点会逐个重启，集群组件也会逐个重启，请等待集群重启完毕。
oc apply -f ./99-worker-container-registries.yaml -n openshift-config
oc apply -f ./99-master-container-registries.yaml -n openshift-config

# ！！！正常情况，以下操作不需要！！！
# 以下操作，删除mirror镜像信息，也会触发集群更新操作，请等待集群重启完毕
oc delete -f ./99-worker-container-registries.yaml -n openshift-config
oc delete -f ./99-master-container-registries.yaml -n openshift-config

watch oc get machineconfigpools

watch oc get node
</code></pre>
<p>从监控界面，能看到节点在升级，重启。
<img src="ocp4/4.6/../imgs/2019-11-04-12-07-09.png" alt="" /></p>
<pre><code class="language-bash">
# on helper node

# params for operator hub images
export var_date='2020.11.23.0135'
echo $var_date
export var_major_version='4.6'
echo ${var_major_version}

export LOCAL_REG='registry.ocp4.redhat.ren:5443'

# 如果想看到redhat的operator，这样做
# 镜像源在 docker.io/wangzheng422/operator-catalog:redhat-$var_major_version-$var_date
# 后面的参数，去build.dist.sh文件里面，查看
# var_date 和 var_major_version 参数得到
cat &lt;&lt;EOF &gt; redhat-operator-catalog.yaml
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: redhat-operators-catalog
  namespace: openshift-marketplace
spec:
  displayName: Red Hat Operators
  sourceType: grpc
  image: ${LOCAL_REG}/ocp4/operator-catalog:redhat-${var_major_version}-${var_date}
  publisher: Red Hat
EOF
oc create -f redhat-operator-catalog.yaml

# 如果想看到certified的operator，这样做
# 镜像源在 docker.io/wangzheng422/operator-catalog:certified-$var_major_version-$var_date
# 后面的参数，去build.dist.sh文件里面，查看
# var_date 和 var_major_version 参数得到
cat &lt;&lt;EOF &gt; certified-operator-catalog.yaml
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: certified-operator-catalog
  namespace: openshift-marketplace
spec:
  displayName: Certified Operators
  sourceType: grpc
  image: ${LOCAL_REG}/ocp4/operator-catalog:certified-${var_major_version}-${var_date}
  publisher: Red Hat
EOF
oc create -f certified-operator-catalog.yaml

# 如果想看到community的operator，这样做
# 镜像源在 docker.io/wangzheng422/operator-catalog:community-$var_major_version-$var_date
# 后面的参数，去build.dist.sh文件里面，查看
# var_date 和 var_major_version 参数得到
cat &lt;&lt;EOF &gt; community-operator-catalog.yaml
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: community-operator-catalog
  namespace: openshift-marketplace
spec:
  displayName: Community Operator
  sourceType: grpc
  image: ${LOCAL_REG}/ocp4/operator-catalog:community-${var_major_version}-${var_date}
  publisher: Red Hat
EOF
oc create -f community-operator-catalog.yaml

cat &lt;&lt;EOF &gt; marketplace-operator-catalog.yaml
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: redhat-marketplace-catalog
  namespace: openshift-marketplace
spec:
  displayName: Red Hat Marketplace
  sourceType: grpc
  image: ${LOCAL_REG}/ocp4/operator-catalog:redhat-marketplace-${var_major_version}-${var_date}
  publisher: Red Hat
EOF
oc create -f marketplace-operator-catalog.yaml

# 想删除这些离线operator hub，就这样做。
# find . -name &quot;*-operator-catalog.yaml&quot; -exec oc delete -f {} \;

oc get pods -n openshift-marketplace
oc get catalogsource -n openshift-marketplace
oc get packagemanifest -n openshift-marketplace

</code></pre>
<p>能看到operator 列表
<img src="ocp4/4.6/../imgs/2019-11-01-18-02-58.png" alt="" /></p>
<p>部署一个operator也能成功
<img src="ocp4/4.6/../imgs/2019-11-01-18-03-37.png" alt="" /></p>
<p><img src="ocp4/4.6/../imgs/2019-11-05-18-02-39.png" alt="" /></p>
<pre><code class="language-bash"># set master and worker combine
# https://github.com/openshift-telco/openshift4x-poc/blob/master/MASTER-WORKER-COMBINED.md
oc edit schedulers cluster
# apiVersion: config.openshift.io/v1
# kind: Scheduler
# metadata:
# name: cluster
# spec:
#     mastersSchedulable: true

</code></pre>
<h2 id="其他链接-1"><a class="header" href="#其他链接-1">其他链接</a></h2>
<p>https://www.cnblogs.com/ericnie/p/11764124.html</p>
<h2 id="以下是参考材料-1"><a class="header" href="#以下是参考材料-1">以下是参考材料</a></h2>
<p>https://blog.openshift.com/openshift-4-2-disconnected-install/</p>
<p>https://blog.openshift.com/openshift-4-bare-metal-install-quickstart/</p>
<p>https://github.com/christianh814/ocp4-upi-helpernode#ocp4-upi-helper-node-playbook</p>
<p>https://github.com/openshift/cluster-samples-operator/blob/master/manifests/image-references</p>
<p>https://github.com/e-minguez/ocp4-upi-bm-pxeless-staticips/blob/master/docs/12-post-installation.md</p>
<p>https://www.openshift.com/blog/deploying-a-upi-environment-for-openshift-4-1-on-vms-and-bare-metal</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-46-离线-baremetal-ipi-全自动安装-单网络模式"><a class="header" href="#openshift-46-离线-baremetal-ipi-全自动安装-单网络模式">openshift 4.6 离线 baremetal IPI (全自动)安装 单网络模式</a></h1>
<h2 id="简介"><a class="header" href="#简介">简介</a></h2>
<p>视频讲解</p>
<p><a href="https://www.bilibili.com/video/BV1P5411V73R/"><kbd><img src="ocp4/4.6/imgs/2020-12-22-22-04-11.png" width="600"></kbd></a></p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1P5411V73R/">bilibili</a></li>
<li><a href="https://www.ixigua.com/6909075922935087623">xigua</a></li>
<li><a href="https://www.youtube.com/watch?v=7KdMbGudaTw">youtube</a></li>
</ul>
<p>本文描述ocp4.6在baremetal(kvm模拟)上面，IPI (全自动)安装。</p>
<p>根据openshift文档，baremetal IPI安装有两种模式，一种是provisioning网络独立，另外一种是provisioning网络和baremetal(服务)网络合并的模式。考虑到POC现场的环境，本次实验，使用简单的网络部署，也就是合并的网络模式。</p>
<p>以下是本次实验的架构图:</p>
<p><img src="ocp4/4.6/./dia/4.6.bm.ipi.drawio.svg" alt="" /></p>
<h2 id="离线安装包下载-2"><a class="header" href="#离线安装包下载-2">离线安装包下载</a></h2>
<p>打包好的安装包，在这里下载，百度盘下载链接，版本是4.6.9-ccn:</p>
<p>链接: https://pan.baidu.com/s/1jJU0HLnZMnvCNMNq1OEDxA  密码: uaaw</p>
<p>其中包括如下类型的文件：</p>
<ul>
<li>ocp4.tgz  这个文件包含了iso等安装介质，以及各种安装脚本，全部下载的镜像列表等。需要复制到宿主机，以及工具机上去。</li>
<li>registry.tgz  这个文件也是docker image registry的仓库打包文件。需要先补充镜像的话，按照这里操作: <a href="ocp4/4.6/./4.6.add.image.html">4.6.add.image.md</a></li>
<li>nexus-image.tgz 这个是nexus的镜像仓库打包，集群的镜像proxy指向nexus，由nexus提供镜像的cache</li>
<li>poc.image.tgz 这个是给registry.tgz补充的一些镜像，主要是ccn使用，补充的镜像列表在这里 <a href="ocp4/4.6/./ccn/poc.image.list">poc.image.list</a> ，按照这里操作: <a href="ocp4/4.6/./4.6.add.image.html">4.6.add.image.md</a></li>
</ul>
<p>合并这些切分文件，使用类似如下的命令</p>
<pre><code class="language-bash">cat registry.?? &gt; registry.tgz
</code></pre>
<p>注意，可能需要更新离线镜像包中的helper用的ansible脚本。</p>
<h2 id="在外网云主机上面准备离线安装源-2"><a class="header" href="#在外网云主机上面准备离线安装源-2">在外网云主机上面准备离线安装源</a></h2>
<p>准备离线安装介质的文档，已经转移到了这里：<a href="ocp4/4.6/4.6.build.dist.html">4.6.build.dist.md</a></p>
<h2 id="宿主机准备-2"><a class="header" href="#宿主机准备-2">宿主机准备</a></h2>
<p>本次实验，是在一个32C， 256G 的主机上面，用很多个虚拟机安装测试。所以先准备这个宿主机。</p>
<p>如果是多台宿主机，记得一定要调整时间配置，让这些宿主机的时间基本一致，否则证书会出问题。</p>
<p>主要的准备工作有</p>
<ul>
<li>配置yum源</li>
<li>配置dns</li>
<li>安装镜像仓库</li>
<li>配置vnc环境</li>
<li>配置kvm需要的网络</li>
<li>创建helper kvm</li>
</ul>
<p>以上准备工作，dns部分需要根据实际项目环境有所调整。</p>
<p>本次的宿主机是一台rhel8, 参考这里进行离线repo等基本的配置<a href="ocp4/4.6/../../rhel/rhel8.build.kernel.repo.cache.html">rhel8.build.kernel.repo.cache.md</a></p>
<pre><code class="language-bash">cat &lt;&lt; EOF &gt; /root/.ssh/config
StrictHostKeyChecking no
UserKnownHostsFile=/dev/null
EOF

cat &lt;&lt; EOF &gt;&gt;  /etc/hosts
127.0.0.1 registry.ocp4.redhat.ren nexus.ocp4.redhat.ren git.ocp4.redhat.ren
EOF

dnf clean all
dnf repolist

dnf -y install byobu htop jq ipmitool

systemctl disable --now firewalld

# 配置registry
mkdir -p /etc/crts/ &amp;&amp; cd /etc/crts

# https://access.redhat.com/documentation/en-us/red_hat_codeready_workspaces/2.1/html/installation_guide/installing-codeready-workspaces-in-tls-mode-with-self-signed-certificates_crw
openssl genrsa -out /etc/crts/redhat.ren.ca.key 4096
openssl req -x509 \
  -new -nodes \
  -key /etc/crts/redhat.ren.ca.key \
  -sha256 \
  -days 36500 \
  -out /etc/crts/redhat.ren.ca.crt \
  -subj /CN=&quot;Local Red Hat Ren Signer&quot; \
  -reqexts SAN \
  -extensions SAN \
  -config &lt;(cat /etc/pki/tls/openssl.cnf \
      &lt;(printf '[SAN]\nbasicConstraints=critical, CA:TRUE\nkeyUsage=keyCertSign, cRLSign, digitalSignature'))

openssl genrsa -out /etc/crts/redhat.ren.key 2048

openssl req -new -sha256 \
    -key /etc/crts/redhat.ren.key \
    -subj &quot;/O=Local Red Hat Ren /CN=*.ocp4.redhat.ren&quot; \
    -reqexts SAN \
    -config &lt;(cat /etc/pki/tls/openssl.cnf \
        &lt;(printf &quot;\n[SAN]\nsubjectAltName=DNS:*.ocp4.redhat.ren,DNS:*.apps.ocp4.redhat.ren,DNS:*.redhat.ren\nbasicConstraints=critical, CA:FALSE\nkeyUsage=digitalSignature, keyEncipherment, keyAgreement, dataEncipherment\nextendedKeyUsage=serverAuth&quot;)) \
    -out /etc/crts/redhat.ren.csr

openssl x509 \
    -req \
    -sha256 \
    -extfile &lt;(printf &quot;subjectAltName=DNS:*.ocp4.redhat.ren,DNS:*.apps.ocp4.redhat.ren,DNS:*.redhat.ren\nbasicConstraints=critical, CA:FALSE\nkeyUsage=digitalSignature, keyEncipherment, keyAgreement, dataEncipherment\nextendedKeyUsage=serverAuth&quot;) \
    -days 36500 \
    -in /etc/crts/redhat.ren.csr \
    -CA /etc/crts/redhat.ren.ca.crt \
    -CAkey /etc/crts/redhat.ren.ca.key \
    -CAcreateserial -out /etc/crts/redhat.ren.crt

openssl x509 -in /etc/crts/redhat.ren.crt -text

/bin/cp -f /etc/crts/redhat.ren.ca.crt /etc/pki/ca-trust/source/anchors/
update-ca-trust extract

cd /data
mkdir -p /data/registry
# tar zxf registry.tgz
dnf -y install podman pigz skopeo jq 
# pigz -dc registry.tgz | tar xf -
cd /data/ocp4
podman load -i /data/ocp4/registry.tgz

podman run --name local-registry -p 5443:5000 \
  -d --restart=always \
  -v /data/registry/:/var/lib/registry:z \
  -v /etc/crts:/certs:z \
  -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/redhat.ren.crt \
  -e REGISTRY_HTTP_TLS_KEY=/certs/redhat.ren.key \
  docker.io/library/registry:2

podman start local-registry

# firewall-cmd --permanent --add-port=5443/tcp
# firewall-cmd --reload

# 加载更多的镜像
# 解压缩 ocp4.tgz
bash add.image.load.sh /data/install.image 'registry.ocp4.redhat.ren:5443'

# https://github.com/christianh814/ocp4-upi-helpernode/blob/master/docs/quickstart.md

# 准备vnc环境
vncpasswd

cat &lt;&lt; EOF &gt; ~/.vnc/config
session=gnome
securitytypes=vncauth,tlsvnc
desktop=sandbox
geometry=1440x855
alwaysshared
EOF

cat &lt;&lt; EOF &gt;&gt; /etc/tigervnc/vncserver.users
:1=root
EOF

systemctl start vncserver@:1
# 如果你想停掉vnc server，这么做
systemctl stop vncserver@:1

# firewall-cmd --permanent --add-port=6001/tcp
# firewall-cmd --permanent --add-port=5901/tcp
# firewall-cmd --reload

# connect vnc at port 5901
# export DISPLAY=:1

# 创建实验用虚拟网络

cat &lt;&lt; 'EOF' &gt; /data/kvm/bridge.sh
#!/usr/bin/env bash

PUB_CONN='eno1'
PUB_IP='172.21.6.105/24'
PUB_GW='172.21.6.254'
PUB_DNS='172.21.1.1'

nmcli con down &quot;$PUB_CONN&quot;
nmcli con delete &quot;$PUB_CONN&quot;
nmcli con down baremetal
nmcli con delete baremetal
# RHEL 8.1 appends the word &quot;System&quot; in front of the connection,delete in case it exists
nmcli con down &quot;System $PUB_CONN&quot;
nmcli con delete &quot;System $PUB_CONN&quot;
nmcli connection add ifname baremetal type bridge con-name baremetal ipv4.method 'manual' \
    ipv4.address &quot;$PUB_IP&quot; \
    ipv4.gateway &quot;$PUB_GW&quot; \
    ipv4.dns &quot;$PUB_DNS&quot;
    
nmcli con add type bridge-slave ifname &quot;$PUB_CONN&quot; master baremetal
nmcli con down &quot;$PUB_CONN&quot;;pkill dhclient;dhclient baremetal
nmcli con up baremetal
EOF

nmcli con mod baremetal +ipv4.address '192.168.7.1/24'
nmcli networking off; nmcli networking on

# 创建工具机

mkdir -p /data/kvm
cd /data/kvm

lvremove -f rhel/helperlv
lvcreate -y -L 200G -n helperlv rhel

virt-install --name=&quot;ocp4-aHelper&quot; --vcpus=2 --ram=4096 \
--disk path=/dev/rhel/helperlv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network bridge=baremetal,model=virtio \
--boot menu=on --location /data/kvm/rhel-8.3-x86_64-dvd.iso \
--initrd-inject helper-ks-rhel8-ipi.cfg --extra-args &quot;inst.ks=file:/helper-ks-rhel8-ipi.cfg&quot; 

virsh start ocp4-aHelper

# DO NOT USE, restore kvm
virsh destroy ocp4-aHelper
virsh undefine ocp4-aHelper

# virt-viewer --domain-name ocp4-aHelper
# virsh start ocp4-aHelper
# virsh list --all

# start chrony/ntp server on host
/bin/cp -f /etc/chrony.conf /etc/chrony.conf.default
cat &lt;&lt; EOF &gt; /etc/chrony.conf
# pool 2.rhel.pool.ntp.org iburst
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
allow 192.0.0.0/8
local stratum 10
logdir /var/log/chrony
EOF
systemctl enable --now chronyd
# systemctl restart chronyd
chronyc tracking
chronyc sources -v
chronyc sourcestats -v
chronyc makestep

# setup ftp data root
mount --bind /data/dnf /var/ftp/dnf
chcon -R -t public_content_t  /var/ftp/dnf

# create the master and worker vm, but not start them
export KVM_DIRECTORY=/data/kvm
mkdir -p ${KVM_DIRECTORY}
cd ${KVM_DIRECTORY}
# scp root@192.168.7.11:/data/install/*.iso ${KVM_DIRECTORY}/

remove_lv() {
    var_vg=$1
    var_lv=$2
    lvremove -f $var_vg/$var_lv
}

create_lv() {
    var_vg=$1
    var_lv=$2
    lvcreate -y -L 120G -n $var_lv $var_vg
    wipefs --all --force /dev/$var_vg/$var_lv
}

remove_lv nvme master0lv
remove_lv nvme master1lv
remove_lv nvme master2lv
remove_lv rhel worker0lv
remove_lv rhel worker1lv
remove_lv rhel worker2lv

# create_lv rhel bootstraplv
create_lv nvme master0lv
create_lv nvme master1lv
create_lv nvme master2lv
create_lv rhel worker0lv
create_lv rhel worker1lv
create_lv rhel worker2lv

virt-install --name=ocp4-master0 --vcpus=4 --ram=16384 \
--disk path=/dev/nvme/master0lv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network bridge=baremetal,model=virtio \
--boot uefi,nvram_template=/usr/share/OVMF/OVMF_VARS.fd,menu=on  \
--print-xml &gt; ${KVM_DIRECTORY}/ocp4-master0.xml
virsh define --file ${KVM_DIRECTORY}/ocp4-master0.xml

virt-install --name=ocp4-master1 --vcpus=4 --ram=16384 \
--disk path=/dev/nvme/master1lv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network bridge=baremetal,model=virtio \
--boot uefi,nvram_template=/usr/share/OVMF/OVMF_VARS.fd,menu=on  \
--print-xml &gt; ${KVM_DIRECTORY}/ocp4-master1.xml
virsh define --file ${KVM_DIRECTORY}/ocp4-master1.xml

virt-install --name=ocp4-master2 --vcpus=4 --ram=16384 \
--disk path=/dev/nvme/master2lv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network bridge=baremetal,model=virtio \
--boot uefi,nvram_template=/usr/share/OVMF/OVMF_VARS.fd,menu=on  \
--print-xml &gt; ${KVM_DIRECTORY}/ocp4-master2.xml
virsh define --file ${KVM_DIRECTORY}/ocp4-master2.xml

virt-install --name=ocp4-worker0 --vcpus=8 --ram=65536 \
--disk path=/dev/rhel/worker0lv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network bridge=baremetal,model=virtio \
--boot uefi,nvram_template=/usr/share/OVMF/OVMF_VARS.fd,menu=on  \
--print-xml &gt; ${KVM_DIRECTORY}/ocp4-worker0.xml
virsh define --file ${KVM_DIRECTORY}/ocp4-worker0.xml

virt-install --name=ocp4-worker1 --vcpus=4 --ram=32768 \
--disk path=/dev/rhel/worker1lv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network bridge=baremetal,model=virtio \
--boot uefi,nvram_template=/usr/share/OVMF/OVMF_VARS.fd,menu=on  \
--print-xml &gt; ${KVM_DIRECTORY}/ocp4-worker1.xml
virsh define --file ${KVM_DIRECTORY}/ocp4-worker1.xml

virt-install --name=ocp4-worker2 --vcpus=2 --ram=8192 \
--disk path=/dev/rhel/worker2lv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network bridge=baremetal,model=virtio \
--boot uefi,nvram_template=/usr/share/OVMF/OVMF_VARS.fd,menu=on  \
--print-xml &gt; ${KVM_DIRECTORY}/ocp4-worker2.xml
virsh define --file ${KVM_DIRECTORY}/ocp4-worker2.xml

cd /data/kvm/
for i in master{0..2} worker{0..2}
do
  echo -ne &quot;${i}\t&quot; ; 
  virsh dumpxml ocp4-${i} | grep &quot;mac address&quot; | cut -d\' -f2 | tr '\n' '\t'
  echo 
done &gt; mac.list
cat /data/kvm/mac.list
# master0 52:54:00:7b:5b:83
# master1 52:54:00:9b:f4:bc
# master2 52:54:00:72:16:ac
# worker0 52:54:00:19:f4:65
# worker1 52:54:00:88:4f:2c
# worker2 52:54:00:ed:25:30

# GOTO image registry &amp; kvm host
# copy crt files to helper node
ssh-copy-id root@192.168.7.11

ssh root@192.168.7.11 mkdir -p /data/install
ssh root@192.168.7.11 mkdir -p /data/ocp4
scp /data/down/ocp4.tgz root@192.168.7.11:/data/
rsync -e ssh --info=progress2 -P --delete -arz /data/ocp4/ 192.168.7.11:/data/ocp4/

scp /etc/crts/redhat.ren.ca.crt root@192.168.7.11:/data/install/
scp /data/kvm/mac.list root@192.168.7.11:/data/install/

# install redfish for kvm
# https://access.redhat.com/solutions/4315581
# https://access.redhat.com/solutions/3057171
# https://docs.openstack.org/virtualbmc/latest/user/index.html
# https://docs.openstack.org/sushy-tools/latest/user/dynamic-emulator.html
dnf -y install python3-pip
# pip3 install --user sushy-tools

mkdir -p /data/install
cd /data/install

# podman create --name swap docker.io/wangzheng422/imgs:openshift-baremetal-install-4.6.5 ls
# podman cp swap:/openshift-baremetal-install ./
# podman rm -fv swap

podman create --name swap docker.io/wangzheng422/imgs:ocp.bm.ipi.python.dep.rhel8-4.6.7 ls
podman cp swap:/wheelhouse.tar.gz - &gt; wheelhouse.tar.gz
tar zvxf wheelhouse.tar.gz
podman rm -fv swap

pip3 install --user -r wheelhouse/requirements.txt --no-index --find-links wheelhouse

/root/.local/bin/sushy-emulator -i 0.0.0.0 --ssl-certificate /etc/crts/redhat.ren.crt --ssl-key /etc/crts/redhat.ren.key

# curl https://registry.ocp4.redhat.ren:8000/redfish/v1/Systems/

# DO NOT USE, restore 
# if you want to stop or delete vm, try this
virsh list --all
# virsh destroy ocp4-bootstrap
virsh destroy ocp4-master0 
virsh destroy ocp4-master1 
virsh destroy ocp4-master2 
virsh destroy ocp4-worker0 
virsh destroy ocp4-worker1 
virsh destroy ocp4-worker2
# virsh undefine ocp4-bootstrap
virsh undefine ocp4-master0 --nvram
virsh undefine ocp4-master1 --nvram
virsh undefine ocp4-master2 --nvram
virsh undefine ocp4-worker0 --nvram
virsh undefine ocp4-worker1 --nvram
virsh undefine ocp4-worker2 --nvram

</code></pre>
<h2 id="工具机准备-2"><a class="header" href="#工具机准备-2">工具机准备</a></h2>
<p>以下是在工具机里面，进行的安装操作。</p>
<p>主要的操作有</p>
<ul>
<li>配置yum源</li>
<li>运行ansible脚本，自动配置工具机</li>
<li>上传定制的安装配置文件</li>
<li>生成ignition文件</li>
</ul>
<pre><code class="language-bash">
sed -i 's/#UseDNS yes/UseDNS no/g' /etc/ssh/sshd_config
systemctl restart sshd

cat &lt;&lt; EOF &gt; /root/.ssh/config
StrictHostKeyChecking no
UserKnownHostsFile=/dev/null
EOF

systemctl disable --now firewalld

# in helper node
mkdir /etc/yum.repos.d.bak
mv /etc/yum.repos.d/* /etc/yum.repos.d.bak

export YUMIP=&quot;192.168.7.1&quot;
cat &lt;&lt; EOF &gt; /etc/yum.repos.d/remote.repo
[remote-epel]
name=epel
baseurl=ftp://${YUMIP}/dnf/epel
enabled=1
gpgcheck=0

[remote-epel-modular]
name=epel-modular
baseurl=ftp://${YUMIP}/dnf/epel-modular
enabled=1
gpgcheck=0

[remote-appstream]
name=appstream
baseurl=ftp://${YUMIP}/dnf/rhel-8-for-x86_64-appstream-rpms
enabled=1
gpgcheck=0

[remote-baseos]
name=baseos
baseurl=ftp://${YUMIP}/dnf/rhel-8-for-x86_64-baseos-rpms
enabled=1
gpgcheck=0

[remote-baseos-source]
name=baseos-source
baseurl=ftp://${YUMIP}/dnf/rhel-8-for-x86_64-baseos-source-rpms
enabled=1
gpgcheck=0

[remote-supplementary]
name=supplementary
baseurl=ftp://${YUMIP}/dnf/rhel-8-for-x86_64-supplementary-rpms
enabled=1
gpgcheck=0

[remote-codeready-builder]
name=supplementary
baseurl=ftp://${YUMIP}/dnf/codeready-builder-for-rhel-8-x86_64-rpms
enabled=1
gpgcheck=0

EOF

yum clean all
yum makecache
yum repolist

yum -y install ansible git unzip podman python3

yum -y update

reboot

# yum -y install ansible git unzip podman python36

mkdir -p /data/ocp4/
# scp ocp4.tgz to /data
# scp /data/down/ocp4.tgz root@192.168.7.11:/data/
cd /data
tar zvxf ocp4.tgz
cd /data/ocp4

# 这里使用了一个ansible的项目，用来部署helper节点的服务。
# https://github.com/wangzheng422/ocp4-upi-helpernode
unzip ocp4-upi-helpernode.zip
# 这里使用了一个ignition文件合并的项目，用来帮助自定义ignition文件。
# https://github.com/wangzheng422/filetranspiler
podman load -i filetranspiler.tgz

mkdir -p /data/install

mkdir -p /data/ocp4/
cd /data/ocp4/
cat &lt;&lt; 'EOF' &gt; redfish.sh
#!/usr/bin/env bash

curl -k -s https://192.168.7.1:8000/redfish/v1/Systems/ | jq -r '.Members[].&quot;@odata.id&quot;' &gt;  list

while read -r line; do
    curl -k -s https://192.168.7.1:8000/$line | jq -j '.Id, &quot; &quot;, .Name, &quot;\n&quot; '
done &lt; list

EOF
bash redfish.sh &gt; /data/install/vm.list
cat /data/install/vm.list
# 9cc02fbc-cbfe-4006-b5a9-f04712321157 ocp4-worker0
# b1a13dd1-7864-4b61-bd0c-851c11f87199 ocp4-master0
# 0a121472-6d24-47ae-9715-8e8e175ab397 ocp4-master2
# b30891d1-b14b-4645-9b05-504a58e1e059 ocp4-worker1
# fb261d6c-31c5-4e7e-8020-2789d5cc63e3 ocp4-aHelper
# 4497d313-390c-4c6b-a5d6-3f533e397aaf ocp4-master1
# f9b0a86d-1587-47ea-9a92-a2762b0684fd ocp4-worker2

cat &lt;&lt; EOF &gt; /data/ocp4/ocp4-upi-helpernode-master/vars-dhcp.rhel8.yaml
---
ssh_gen_key: true
staticips: false
bm_ipi: true
firewalld: false
dns_forward: false
iso:
  iso_dl_url: &quot;file:///data/ocp4/rhcos-live.x86_64.iso&quot;
  my_iso: &quot;rhcos-live.iso&quot;
helper:
  name: &quot;helper&quot;
  ipaddr: &quot;192.168.7.11&quot;
  networkifacename: &quot;enp1s0&quot;
  gateway: &quot;192.168.7.1&quot;
  netmask: &quot;255.255.255.0&quot;
dns:
  domain: &quot;redhat.ren&quot;
  clusterid: &quot;ocp4&quot;
  forwarder1: &quot;192.168.7.1&quot;
  forwarder2: &quot;192.168.7.1&quot;
  api_vip: &quot;192.168.7.100&quot;
  ingress_vip: &quot;192.168.7.101&quot;
dhcp:
  router: &quot;192.168.7.1&quot;
  bcast: &quot;192.168.7.255&quot;
  netmask: &quot;255.255.255.0&quot;
  poolstart: &quot;192.168.7.70&quot;
  poolend: &quot;192.168.7.90&quot;
  ipid: &quot;192.168.7.0&quot;
  netmaskid: &quot;255.255.255.0&quot;
bootstrap:
  name: &quot;bootstrap&quot;
  ipaddr: &quot;192.168.7.12&quot;
  interface: &quot;enp1s0&quot;
  install_drive: &quot;vda&quot;
  macaddr: &quot;52:54:00:7e:f8:f7&quot;
masters:
  - name: &quot;master-0&quot;
    ipaddr: &quot;192.168.7.13&quot;
    interface: &quot;enp1s0&quot;
    install_drive: &quot;vda&quot;
    macaddr: &quot;$(cat /data/install/mac.list | grep master0 | awk '{print $2}')&quot;
  - name: &quot;master-1&quot;
    ipaddr: &quot;192.168.7.14&quot;
    interface: &quot;enp1s0&quot;
    install_drive: &quot;vda&quot;    
    macaddr: &quot;$(cat /data/install/mac.list | grep master1 | awk '{print $2}')&quot;
  - name: &quot;master-2&quot;
    ipaddr: &quot;192.168.7.15&quot;
    interface: &quot;enp1s0&quot;
    install_drive: &quot;vda&quot;   
    macaddr: &quot;$(cat /data/install/mac.list | grep master2 | awk '{print $2}')&quot;
workers:
  - name: &quot;worker-0&quot;
    ipaddr: &quot;192.168.7.16&quot;
    interface: &quot;enp1s0&quot;
    install_drive: &quot;vda&quot;
    macaddr: &quot;$(cat /data/install/mac.list | grep worker0 | awk '{print $2}')&quot;
  - name: &quot;worker-1&quot;
    ipaddr: &quot;192.168.7.17&quot;
    interface: &quot;enp1s0&quot;
    install_drive: &quot;vda&quot;
    macaddr: &quot;$(cat /data/install/mac.list | grep worker1 | awk '{print $2}')&quot;
  - name: &quot;worker-2&quot;
    ipaddr: &quot;192.168.7.18&quot;
    interface: &quot;enp1s0&quot;
    install_drive: &quot;vda&quot;
    macaddr: &quot;$(cat /data/install/mac.list | grep worker2 | awk '{print $2}')&quot;
others:
  - name: &quot;registry&quot;
    ipaddr: &quot;192.168.7.1&quot;
    macaddr: &quot;52:54:00:7e:f8:f7&quot;
  - name: &quot;yum&quot;
    ipaddr: &quot;192.168.7.1&quot;
    macaddr: &quot;52:54:00:7e:f8:f7&quot;
  - name: &quot;quay&quot;
    ipaddr: &quot;192.168.7.1&quot;
    macaddr: &quot;52:54:00:7e:f8:f7&quot;
  - name: &quot;nexus&quot;
    ipaddr: &quot;192.168.7.1&quot;
    macaddr: &quot;52:54:00:7e:f8:f7&quot;
  - name: &quot;git&quot;
    ipaddr: &quot;192.168.7.1&quot;
    macaddr: &quot;52:54:00:7e:f8:f7&quot;
otherdomains:
  - domain: &quot;rhv.redhat.ren&quot;
    hosts:
    - name: &quot;manager&quot;
      ipaddr: &quot;192.168.7.71&quot;
    - name: &quot;rhv01&quot;
      ipaddr: &quot;192.168.7.72&quot;
  - domain: &quot;cmri-edge.redhat.ren&quot;
    hosts:
    - name: &quot;*&quot;
      ipaddr: &quot;192.168.7.71&quot;
    - name: &quot;*.apps&quot;
      ipaddr: &quot;192.168.7.72&quot;
force_ocp_download: false
remove_old_config_files: false
ocp_client: &quot;file:///data/ocp4/4.6.9/openshift-client-linux-4.6.9.tar.gz&quot;
ocp_installer: &quot;file:///data/ocp4/4.6.9/openshift-install-linux-4.6.9.tar.gz&quot;
ppc64le: false
arch: 'x86_64'
chronyconfig:
  enabled: true
  content:
    - server: &quot;192.168.7.1&quot;
      options: iburst
setup_registry:
  deploy: false
  registry_image: docker.io/library/registry:2
  local_repo: &quot;ocp4/openshift4&quot;
  product_repo: &quot;openshift-release-dev&quot;
  release_name: &quot;ocp-release&quot;
  release_tag: &quot;4.6.1-x86_64&quot;
registry_server: &quot;registry.ocp4.redhat.ren:5443&quot;
EOF

# 接下来，我们使用ansible来配置helper节点，装上各种openshift集群需要的服务
# 根据现场环境，修改 ocp4-upi-helpernode-master/vars-static.yaml
cd /data/ocp4/ocp4-upi-helpernode-master
ansible-playbook -e @vars-dhcp.rhel8.yaml -e '{ staticips: false, bm_ipi: true }'  tasks/main.yml

# try this:
/usr/local/bin/helpernodecheck

mkdir -p /data/install

# GO back to help node
/bin/cp -f /data/install/redhat.ren.ca.crt /etc/pki/ca-trust/source/anchors/
update-ca-trust extract

# 根据现场环境，修改 install-config.yaml
# 至少要修改ssh key， 还有 additionalTrustBundle，这个是镜像仓库的csr 

# copy your pull secret file into helper
# SEC_FILE='/data/pull-secret.json'
# cat &lt;&lt; 'EOF' &gt; $SEC_FILE

# 定制ignition
cd /data/install

# vi install-config.yaml 
cat &lt;&lt; EOF &gt; /data/install/install-config.yaml 
apiVersion: v1
baseDomain: redhat.ren
platform:
  baremetal:
    apiVIP: 192.168.7.100
    ingressVIP: 192.168.7.101
    bootstrapProvisioningIP: 192.168.7.102
    provisioningHostIP: 192.168.7.103
    provisioningNetwork: &quot;Disabled&quot;
    bootstrapOSImage: http://192.168.7.11:8080/install/rhcos-qemu.x86_64.qcow2.gz?sha256=$(zcat /var/www/html/install/rhcos-qemu.x86_64.qcow2.gz | sha256sum | awk '{print $1}')
    clusterOSImage: http://192.168.7.11:8080/install/rhcos-openstack.x86_64.qcow2.gz?sha256=$(sha256sum /var/www/html/install/rhcos-openstack.x86_64.qcow2.gz | awk '{print $1}')
    hosts:
      - name: master-0
        role: master
        bmc:
          address: redfish-virtualmedia://192.168.7.1:8000/redfish/v1/Systems/$(cat vm.list | grep master0 | awk '{print $1}')
          username: admin
          password: password
          disableCertificateVerification: True
        bootMACAddress: $(cat mac.list | grep master0 | awk '{print $2}')
        rootDeviceHints:
          deviceName: &quot;/dev/vda&quot;
      - name: master-1
        role: master
        bmc:
          address: redfish-virtualmedia://192.168.7.1:8000/redfish/v1/Systems/$(cat vm.list | grep master1 | awk '{print $1}')
          username: admin
          password: password
          disableCertificateVerification: True
        bootMACAddress: $(cat mac.list | grep master1 | awk '{print $2}')
        rootDeviceHints:
          deviceName: &quot;/dev/vda&quot;
      - name: master-2
        role: master
        bmc:
          address: redfish-virtualmedia://192.168.7.1:8000/redfish/v1/Systems/$(cat vm.list | grep master2 | awk '{print $1}')
          username: admin
          password: password
          disableCertificateVerification: True
        bootMACAddress: $(cat mac.list | grep master2 | awk '{print $2}')
        rootDeviceHints:
          deviceName: &quot;/dev/vda&quot;
      - name: worker-0
        role: worker
        bmc:
          address: redfish-virtualmedia://192.168.7.1:8000/redfish/v1/Systems/$(cat vm.list | grep worker0 | awk '{print $1}')
          username: admin
          password: password
          disableCertificateVerification: True
        bootMACAddress: $(cat mac.list | grep worker0 | awk '{print $2}')
        rootDeviceHints:
          deviceName: &quot;/dev/vda&quot;
      - name: worker-1
        role: worker
        bmc:
          address: redfish-virtualmedia://192.168.7.1:8000/redfish/v1/Systems/$(cat vm.list | grep worker1 | awk '{print $1}')
          username: admin
          password: password
          disableCertificateVerification: True
        bootMACAddress: $(cat mac.list | grep worker1 | awk '{print $2}')
        rootDeviceHints:
          deviceName: &quot;/dev/vda&quot;
metadata:
  name: ocp4
networking:
  clusterNetworks:
  - cidr: 10.254.0.0/16
    hostPrefix: 24
  networkType: OpenShiftSDN
  serviceNetwork:
  - 172.30.0.0/16
  machineCIDR: 192.168.7.0/24
compute:
- name: worker
  replicas: 2
controlPlane:
  name: master
  replicas: 3
  platform:
    baremetal: {}
pullSecret: '$( cat /data/pull-secret.json )'
sshKey: |
$( cat /root/.ssh/helper_rsa.pub | sed 's/^/   /g' )
additionalTrustBundle: |
$( cat /data/install/redhat.ren.ca.crt | sed 's/^/   /g' )
imageContentSources:
- mirrors:
  - registry.ocp4.redhat.ren:5443/ocp4/openshift4
  source: quay.io/openshift-release-dev/ocp-release
- mirrors:
  - registry.ocp4.redhat.ren:5443/ocp4/openshift4
  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
EOF

# GO back to host
mkdir -p /data/install
cd /data/install
scp root@192.168.7.11:/data/install/install-config.yaml /data/install/

cd /data/install
for i in $(sudo virsh list --all | tail -n +3 | grep bootstrap | awk {'print $2'});
do
  sudo virsh destroy $i;
  sudo virsh undefine $i;
  sudo virsh vol-delete $i --pool default;
  sudo virsh vol-delete $i.ign --pool default;
  virsh pool-destroy $i
  virsh pool-delete $i
  virsh pool-undefine $i
done
/bin/rm -rf .openshift_install.log .openshift_install_state.json terraform* auth tls 
/data/ocp4/4.6.9/openshift-baremetal-install --dir /data/install/ --log-level debug create cluster

# INFO Install complete!
# INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/data/install/auth/kubeconfig'
# INFO Access the OpenShift web-console here: https://console-openshift-console.apps.ocp4.redhat.ren
# INFO Login to the console with user: &quot;kubeadmin&quot;, and password: &quot;tjRNB-xHf2f-fFh8n-ppNXi&quot;

# on kvm host, copy back auth folder
rsync -arz /data/install/auth root@192.168.7.11:/data/install/

# Go back to helper
ansible localhost -m lineinfile -a 'path=$HOME/.bashrc regexp=&quot;^export KUBECONFIG&quot; line=&quot;export KUBECONFIG=/data/install/auth/kubeconfig&quot;'
source $HOME/.bashrc

oc get node
oc get pod -n openshift-machine-api
oc get BareMetalHost -n openshift-machine-api
oc get bmh -n openshift-machine-api
# NAME       STATUS   PROVISIONING STATUS      CONSUMER                    BMC                                                                                               HARDWARE PROFILE   ONLINE   ERROR
# master-0   OK       externally provisioned   ocp4-zn8lq-master-0         redfish-virtualmedia://192.168.7.1:8000/redfish/v1/Systems/965c420a-f127-4639-9184-fe3546d2bde4                      true
# master-1   OK       externally provisioned   ocp4-zn8lq-master-1         redfish-virtualmedia://192.168.7.1:8000/redfish/v1/Systems/46f9dff4-1b44-4286-8a7c-691673340030                      true
# master-2   OK       externally provisioned   ocp4-zn8lq-master-2         redfish-virtualmedia://192.168.7.1:8000/redfish/v1/Systems/9e544eb6-1b98-4b0a-ad32-7df232ae582a                      true
# worker-0   OK       provisioned              ocp4-zn8lq-worker-0-mv4d7   redfish-virtualmedia://192.168.7.1:8000/redfish/v1/Systems/c399c6b7-525a-4f4e-8280-0472b6494fc5   unknown            true
# worker-1   OK       provisioned              ocp4-zn8lq-worker-0-9frt6   redfish-virtualmedia://192.168.7.1:8000/redfish/v1/Systems/a4052132-7598-4879-b3e1-c48c47cf67ed   unknown            true
</code></pre>
<p>我们就能看到bm的输出了
<img src="ocp4/4.6/imgs/2020-12-10-13-19-30.png" alt="" />
可以看到web console上node的配置指向了bm
<img src="ocp4/4.6/imgs/2020-12-10-13-22-52.png" alt="" />
我们也可以看到久违的machine配置
<img src="ocp4/4.6/imgs/2020-12-10-13-22-17.png" alt="" /></p>
<h2 id="添加一个新节点"><a class="header" href="#添加一个新节点">添加一个新节点</a></h2>
<p>IPI 模式下，添加一个新节点非常方便，只要定义一个BareMetalHost就好了。</p>
<pre><code class="language-bash">cd /data/install/
cat &lt;&lt; EOF &gt; /data/install/bmh.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: worker-2-bmc-secret
type: Opaque
data:
  username: $(echo -ne &quot;admin&quot; | base64)
  password: $(echo -ne &quot;password&quot; | base64)
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: worker-2
spec:
  online: true
  bootMACAddress: $(cat mac.list | grep worker2 | awk '{print $2}')
  bmc:
    address: redfish-virtualmedia://192.168.7.1:8000/redfish/v1/Systems/$(cat vm.list | grep worker2 | awk '{print $1}')
    credentialsName: worker-2-bmc-secret
    disableCertificateVerification: true
  rootDeviceHints:
    deviceName: /dev/vda
EOF
oc -n openshift-machine-api create -f bmh.yaml

# DO NOT USE, restore, delete the vm
oc -n openshift-machine-api delete -f bmh.yaml

oc get bmh -n openshift-machine-api
# NAME       STATUS   PROVISIONING STATUS      CONSUMER                    BMC                                                                                               HARDWARE PROFILE   ONLINE   ERROR
# master-0   OK       externally provisioned   ocp4-zn8lq-master-0         redfish-virtualmedia://192.168.7.1:8000/redfish/v1/Systems/965c420a-f127-4639-9184-fe3546d2bde4                      true
# master-1   OK       externally provisioned   ocp4-zn8lq-master-1         redfish-virtualmedia://192.168.7.1:8000/redfish/v1/Systems/46f9dff4-1b44-4286-8a7c-691673340030                      true
# master-2   OK       externally provisioned   ocp4-zn8lq-master-2         redfish-virtualmedia://192.168.7.1:8000/redfish/v1/Systems/9e544eb6-1b98-4b0a-ad32-7df232ae582a                      true
# worker-0   OK       provisioned              ocp4-zn8lq-worker-0-mv4d7   redfish-virtualmedia://192.168.7.1:8000/redfish/v1/Systems/c399c6b7-525a-4f4e-8280-0472b6494fc5   unknown            true
# worker-1   OK       provisioned              ocp4-zn8lq-worker-0-9frt6   redfish-virtualmedia://192.168.7.1:8000/redfish/v1/Systems/a4052132-7598-4879-b3e1-c48c47cf67ed   unknown            true
# worker-2   OK       inspecting                                           redfish-virtualmedia://192.168.7.1:8000/redfish/v1/Systems/2eee2e57-e18b-460b-bb3f-7f048f84c69b                      true

oc get machinesets -n openshift-machine-api
# NAME                  DESIRED   CURRENT   READY   AVAILABLE   AGE
# ocp4-zn8lq-worker-0   2         2         2       2           155m

oc get machinesets -n openshift-machine-api -o json | jq -r .items[0].metadata.name

# 扩容worker到3副本，会触发worker-2的部署
oc scale --replicas=3 machineset $(oc get machinesets -n openshift-machine-api -o json | jq -r .items[0].metadata.name) -n openshift-machine-api

</code></pre>
<h2 id="镜像仓库代理--image-registry-proxy-1"><a class="header" href="#镜像仓库代理--image-registry-proxy-1">镜像仓库代理 / image registry proxy</a></h2>
<p>准备离线镜像仓库非常麻烦，好在我们找到了一台在线的主机，那么我们可以使用nexus构造image registry proxy，在在线环境上面，做一遍PoC，然后就能通过image registry proxy得到离线镜像了</p>
<ul>
<li>https://mtijhof.wordpress.com/2018/07/23/using-nexus-oss-as-a-proxy-cache-for-docker-images/</li>
</ul>
<pre><code class="language-bash">#####################################################
# init build the nexus fs
/bin/cp -f nexus-image.tgz /data/ccn/
tar zxf nexus-image.tgz
chown -R 200 /data/ccn/nexus-image

# podman run -d -p 8082:8081 -p 8083:8083 -it --name nexus-image -v /data/ccn/nexus-image:/nexus-data:Z docker.io/sonatype/nexus3:3.29.0

podman run -d -p 8082:8081 -p 8083:8083 -it --name nexus-image -v /data/ccn/nexus-image:/nexus-data:Z docker.io/wangzheng422/imgs:nexus3-3.29.0-wzh

podman stop nexus-image
podman rm nexus-image

# get the admin password
cat /data/ccn/nexus-image/admin.password &amp;&amp; echo
# 84091bcd-c82f-44a3-8b7b-dfc90f5b7da1

# open http://nexus.ocp4.redhat.ren:8082

# 开启 https
# https://blog.csdn.net/s7799653/article/details/105378645
# https://help.sonatype.com/repomanager3/system-configuration/configuring-ssl#ConfiguringSSL-InboundSSL-ConfiguringtoServeContentviaHTTPS
mkdir -p /data/install/tmp
cd /data/install/tmp

# 将证书导出成pkcs格式
# 这里需要输入密码  用 password，
openssl pkcs12 -export -out keystore.pkcs12 -inkey /etc/crts/redhat.ren.key -in /etc/crts/redhat.ren.crt

cat &lt;&lt; EOF &gt;&gt; Dockerfile
FROM docker.io/sonatype/nexus3:3.29.0
USER root
COPY keystore.pkcs12 /keystore.pkcs12
RUN keytool -v -importkeystore -srckeystore keystore.pkcs12 -srcstoretype PKCS12 -destkeystore keystore.jks -deststoretype JKS -storepass password -srcstorepass password  &amp;&amp;\
    cp keystore.jks /opt/sonatype/nexus/etc/ssl/
USER nexus
EOF
buildah bud --format=docker -t docker.io/wangzheng422/imgs:nexus3-3.29.0-wzh -f Dockerfile .
buildah push docker.io/wangzheng422/imgs:nexus3-3.29.0-wzh

######################################################
# go to helper, update proxy setting for ocp cluster
cd /data/ocp4
bash image.registries.conf.sh nexus.ocp4.redhat.ren:8083

mkdir -p /etc/containers/registries.conf.d
/bin/cp -f image.registries.conf /etc/containers/registries.conf.d/

cd /data/ocp4
oc apply -f ./99-worker-container-registries.yaml -n openshift-config
oc apply -f ./99-master-container-registries.yaml -n openshift-config

######################################################
# dump the nexus image fs out
podman stop nexus-image

var_date=$(date '+%Y-%m-%d-%H%M')
echo $var_date
cd /data/ccn

tar cf - ./nexus-image | pigz -c &gt; nexus-image.tgz 
buildah from --name onbuild-container scratch
buildah copy onbuild-container nexus-image.tgz  /
buildah umount onbuild-container 
buildah commit --rm --format=docker onbuild-container docker.io/wangzheng422/nexus-fs:image-$var_date
# buildah rm onbuild-container
# rm -f nexus-image.tgz 
buildah push docker.io/wangzheng422/nexus-fs:image-$var_date
echo &quot;docker.io/wangzheng422/nexus-fs:image-$var_date&quot;

# 以下这个版本，可以作为初始化的image proxy，里面包含了nfs provision，以及sample operator的metadata。很高兴的发现，image stream并不会完全下载镜像，好想只是下载metadata，真正用的时候，才去下载。
# docker.io/wangzheng422/nexus-fs:image-2020-12-26-1118

</code></pre>
<h2 id="配置镜像仓库的ca"><a class="header" href="#配置镜像仓库的ca">配置镜像仓库的ca</a></h2>
<p>安装过程里面，已经把镜像仓库的ca放进去了，但是好想image stream不认，让我们再试试</p>
<pre><code class="language-bash">oc project openshift-config
oc create configmap ca.for.registry -n openshift-config \
    --from-file=registry.ocp4.redhat.ren..5443=/data/install/redhat.ren.ca.crt \
    --from-file=nexus.ocp4.redhat.ren..8083=/data/install/redhat.ren.ca.crt 
oc patch image.config.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;additionalTrustedCA&quot;:{&quot;name&quot;:&quot;ca.for.registry&quot;}}}'  --type=merge

# oc patch image.config.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;registrySources&quot;:{&quot;insecureRegistries&quot;:[&quot;nexus.ocp4.redhat.ren:8083&quot;]}}}'  --type=merge

oc get image.config.openshift.io/cluster -o yaml

# openshift project下面的image stream重新加载一下把
oc get is -o json | jq -r '.items[].metadata.name' | xargs -L1 oc import-image --all 

</code></pre>
<h2 id="配置internal-registry"><a class="header" href="#配置internal-registry">配置internal registry</a></h2>
<p>我们的工具机是带nfs的，那么就给interneal registry配置高档一些的nfs存储吧，不要用emptydir</p>
<pre><code class="language-bash">bash /data/ocp4/ocp4-upi-helpernode-master/files/nfs-provisioner-setup.sh

# oc edit configs.imageregistry.operator.openshift.io
# 修改 storage 部分
# storage:
#   pvc:
#     claim:
oc patch configs.imageregistry.operator.openshift.io cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Managed&quot;,&quot;storage&quot;:{&quot;pvc&quot;:{&quot;claim&quot;:&quot;&quot;}}}}' --type=merge

oc patch configs.imageregistry.operator.openshift.io cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Removed&quot;}}' --type=merge

oc get clusteroperator image-registry

oc get configs.imageregistry.operator.openshift.io cluster -o yaml

# 把imagepruner给停掉
# https://bugzilla.redhat.com/show_bug.cgi?id=1852501#c24
# oc patch imagepruner.imageregistry/cluster --patch '{&quot;spec&quot;:{&quot;suspend&quot;:true}}' --type=merge
# oc -n openshift-image-registry delete jobs --all
</code></pre>
<h2 id="配置sample-operator"><a class="header" href="#配置sample-operator">配置sample operator</a></h2>
<p>openshift内置了一个sample operator，里面有一大堆红帽的产品。</p>
<pre><code class="language-bash">oc get configs.samples.operator.openshift.io/cluster -o yaml

oc patch configs.samples.operator.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Managed&quot;, &quot;samplesRegistry&quot;: &quot;nexus.ocp4.redhat.ren:8083&quot;}}' --type=merge

oc patch configs.samples.operator.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Unmanaged&quot;}}' --type=merge

oc patch configs.samples.operator.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Removed&quot;}}' --type=merge

</code></pre>
<h2 id="chronyntp-设置-2"><a class="header" href="#chronyntp-设置-2">chrony/NTP 设置</a></h2>
<p>在 ocp 4.6 里面，需要设定ntp同步，我们之前ansible脚本，已经创建好了ntp的mco配置，把他打到系统里面就好了。</p>
<pre><code class="language-bash">oc apply -f /data/ocp4/ocp4-upi-helpernode-master/machineconfig/

</code></pre>
<h2 id="operator-hub-离线安装-2"><a class="header" href="#operator-hub-离线安装-2">Operator Hub 离线安装</a></h2>
<p>使用nexus作为image proxy以后，就不需要做这个离线操作了，但是如果我们想搞CCN这种项目，因为他自带了一个catalog，为了避免冲突，我们可能还是需要屏蔽到默认的operator hub</p>
<pre><code class="language-bash">
oc patch OperatorHub cluster --type json \
    -p '[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/disableAllDefaultSources&quot;, &quot;value&quot;: true}]'

oc get OperatorHub cluster -o yaml

</code></pre>
<h2 id="给-openshift-project-image-stream-打补丁"><a class="header" href="#给-openshift-project-image-stream-打补丁">给 openshift project image stream 打补丁</a></h2>
<p>在有代理的网络环境中，我们需要给openshift project下的image stream打一些补丁。</p>
<pre><code class="language-bash">cd /data/ocp4
bash is.patch.sh registry.ocp4.redhat.ren:5443/ocp4/openshift4

</code></pre>
<h2 id="给-router--ingress-更换证书"><a class="header" href="#给-router--ingress-更换证书">给 router / ingress 更换证书</a></h2>
<p>有时候，我们需要公网CA认证的证书，给router来用，那么我们就搞一下</p>
<p>https://docs.openshift.com/container-platform/4.6/security/certificates/replacing-default-ingress-certificate.html</p>
<pre><code class="language-bash">
mkdir -p /data/ccn/ingress-keys/etc
mkdir -p /data/ccn/ingress-keys/lib
cd /data/ccn/ingress-keys
podman run -it --rm --name certbot \
            -v &quot;/data/ccn/ingress-keys/etc:/etc/letsencrypt&quot;:Z \
            -v &quot;/data/ccn/ingress-keys/lib:/var/lib/letsencrypt&quot;:Z \
            docker.io/certbot/certbot certonly  -d &quot;*.apps.ocp4.redhat.ren&quot; --manual --preferred-challenges dns-01  --server https://acme-v02.api.letsencrypt.org/directory

cp ./etc/archive/apps.ocp4.redhat.ren/fullchain1.pem apps.ocp4.redhat.ren.crt
cp ./etc/archive/apps.ocp4.redhat.ren/privkey1.pem apps.ocp4.redhat.ren.key

ssh root@192.168.7.11 mkdir -p /data/install/ingress-key

scp apps.* root@192.168.7.11:/data/install/ingress-key

# on helper
cd /data/install/ingress-key

oc create secret tls wzh-ingress-key \
     --cert=apps.ocp4.redhat.ren.crt \
     --key=apps.ocp4.redhat.ren.key \
     -n openshift-ingress

oc patch ingresscontroller.operator default \
     --type=merge -p \
     '{&quot;spec&quot;:{&quot;defaultCertificate&quot;: {&quot;name&quot;: &quot;wzh-ingress-key&quot;}}}' \
     -n openshift-ingress-operator

</code></pre>
<h2 id="排错技巧"><a class="header" href="#排错技巧">排错技巧</a></h2>
<pre><code class="language-bash">
# login to bootstrap to debug
# find the ip from kvm console
ssh -i ~/.ssh/helper_rsa core@192.168.7.75
journalctl -b -f -u release-image.service -u bootkube.service
journalctl -b -u release-image.service -u bootkube.service | grep -i baremetal
sudo -i
export KUBECONFIG=/etc/kubernetes/kubeconfig
oc get pod -n openshift-machine-api
oc get BareMetalHost -n openshift-machine-api

# debug why bootstrap can't be ping...
cat .openshift_install_state.json | jq  '.&quot;*bootstrap.Bootstrap&quot;'.Config.storage.files[].path

cat .openshift_install_state.json | jq -r '.&quot;*bootstrap.Bootstrap&quot;'.File.Data | base64 -d | jq -r . &gt; ign.json

cat .openshift_install_state.json | jq  -r '.&quot;*bootstrap.Bootstrap&quot;.Config.storage.files[].contents.source ' | sed 's/.*base64,//g' | base64 -d &gt; decode

cat .openshift_install_state.json | jq  -r '.&quot;*bootstrap.Bootstrap&quot;.Config.storage.files[] | .path, .contents.source ' | while read -r line ; do if [[ $line =~ .*base64,.* ]]; then echo $(echo $line | sed 's/.*base64,//g' | base64 -d) ; else echo $line; fi; done &gt; files


</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-46-离线-baremetal-ipi-全自动安装-使用-provisionning-network-双网络模式"><a class="header" href="#openshift-46-离线-baremetal-ipi-全自动安装-使用-provisionning-network-双网络模式">openshift 4.6 离线 baremetal IPI (全自动)安装 使用 provisionning network 双网络模式</a></h1>
<h2 id="简介-1"><a class="header" href="#简介-1">简介</a></h2>
<p>视频讲解</p>
<p><a href="https://www.bilibili.com/video/BV1hv41147or/"><kbd><img src="ocp4/4.6/imgs/2020-12-15-22-56-54.png" width="600"></kbd></a></p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1hv41147or/">bilibili</a></li>
<li><a href="https://www.ixigua.com/6906507322315670030">xigua</a></li>
<li><a href="https://youtu.be/AFpvyb2pTiI">youtube</a></li>
</ul>
<p>本文描述ocp4.6在baremetal(kvm模拟)上面，IPI (全自动)安装。</p>
<p>根据openshift文档，baremetal IPI安装有两种模式，一种是provisioning网络独立，另外一种是provisioning网络和baremetal(服务)网络合并的模式。考虑到POC现场的环境，本次实验，使用复杂的网络部署，也就是baremetal, provisioning network分离的模式。</p>
<p>以下是本次实验的架构图:</p>
<p><img src="ocp4/4.6/./dia/4.6.bm.ipi.provision.network.drawio.svg" alt="" /></p>
<h2 id="离线安装包下载-3"><a class="header" href="#离线安装包下载-3">离线安装包下载</a></h2>
<p>打包好的安装包，在这里下载，百度盘下载链接，版本是4.6.28:</p>
<ul>
<li>链接: https://pan.baidu.com/s/1XFbiOAcz7nul-N9U0aDxHg  密码: 6qtt</li>
</ul>
<p>其中包括如下类型的文件：</p>
<ul>
<li>ocp4.tgz  这个文件包含了iso等安装介质，以及各种安装脚本，全部下载的镜像列表等。需要复制到宿主机，以及工具机上去。</li>
<li>registry.tgz  这个文件也是docker image registry的仓库打包文件。需要先补充镜像的话，按照这里操作: <a href="ocp4/4.6/./4.6.add.image.html">4.6.add.image.md</a></li>
</ul>
<p>合并这些切分文件，使用类似如下的命令</p>
<pre><code class="language-bash">cat registry.?? &gt; registry.tgz
</code></pre>
<p>注意，可能需要更新离线镜像包中的helper用的ansible脚本。</p>
<h2 id="在外网云主机上面准备离线安装源-3"><a class="header" href="#在外网云主机上面准备离线安装源-3">在外网云主机上面准备离线安装源</a></h2>
<p>准备离线安装介质的文档，已经转移到了这里：<a href="ocp4/4.6/4.6.build.dist.html">4.6.build.dist.md</a></p>
<h2 id="宿主机准备-3"><a class="header" href="#宿主机准备-3">宿主机准备</a></h2>
<p>本次实验，是在一个32C， 256G 的主机上面，用很多个虚拟机安装测试。所以先准备这个宿主机。</p>
<p>如果是多台宿主机，记得一定要调整时间配置，让这些宿主机的时间基本一致，否则证书会出问题。</p>
<p>主要的准备工作有</p>
<ul>
<li>配置yum源</li>
<li>配置dns</li>
<li>安装镜像仓库</li>
<li>配置vnc环境</li>
<li>配置kvm需要的网络</li>
<li>创建helper kvm</li>
</ul>
<p>以上准备工作，dns部分需要根据实际项目环境有所调整。</p>
<p>本次的宿主机是一台rhel8, 参考这里进行离线repo等基本的配置<a href="ocp4/4.6/../../rhel/rhel8.build.kernel.repo.cache.html">rhel8.build.kernel.repo.cache.md</a></p>
<pre><code class="language-bash">cat &lt;&lt; EOF &gt; /root/.ssh/config
StrictHostKeyChecking no
UserKnownHostsFile=/dev/null
EOF

cat &lt;&lt; EOF &gt;&gt;  /etc/hosts
127.0.0.1 registry.ocp4.redhat.ren
EOF

dnf clean all
dnf repolist

dnf -y install byobu htop jq ipmitool

systemctl disable --now firewalld

# 配置registry
mkdir -p /etc/crts/ &amp;&amp; cd /etc/crts

# https://access.redhat.com/documentation/en-us/red_hat_codeready_workspaces/2.1/html/installation_guide/installing-codeready-workspaces-in-tls-mode-with-self-signed-certificates_crw
openssl genrsa -out /etc/crts/redhat.ren.ca.key 4096
openssl req -x509 \
  -new -nodes \
  -key /etc/crts/redhat.ren.ca.key \
  -sha256 \
  -days 36500 \
  -out /etc/crts/redhat.ren.ca.crt \
  -subj /CN=&quot;Local Red Hat Ren Signer&quot; \
  -reqexts SAN \
  -extensions SAN \
  -config &lt;(cat /etc/pki/tls/openssl.cnf \
      &lt;(printf '[SAN]\nbasicConstraints=critical, CA:TRUE\nkeyUsage=keyCertSign, cRLSign, digitalSignature'))

openssl genrsa -out /etc/crts/redhat.ren.key 2048

openssl req -new -sha256 \
    -key /etc/crts/redhat.ren.key \
    -subj &quot;/O=Local Red Hat Ren /CN=*.ocp4.redhat.ren&quot; \
    -reqexts SAN \
    -config &lt;(cat /etc/pki/tls/openssl.cnf \
        &lt;(printf &quot;\n[SAN]\nsubjectAltName=DNS:*.ocp4.redhat.ren,DNS:*.apps.ocp4.redhat.ren,DNS:*.redhat.ren\nbasicConstraints=critical, CA:FALSE\nkeyUsage=digitalSignature, keyEncipherment, keyAgreement, dataEncipherment\nextendedKeyUsage=serverAuth&quot;)) \
    -out /etc/crts/redhat.ren.csr

openssl x509 \
    -req \
    -sha256 \
    -extfile &lt;(printf &quot;subjectAltName=DNS:*.ocp4.redhat.ren,DNS:*.apps.ocp4.redhat.ren,DNS:*.redhat.ren\nbasicConstraints=critical, CA:FALSE\nkeyUsage=digitalSignature, keyEncipherment, keyAgreement, dataEncipherment\nextendedKeyUsage=serverAuth&quot;) \
    -days 365 \
    -in /etc/crts/redhat.ren.csr \
    -CA /etc/crts/redhat.ren.ca.crt \
    -CAkey /etc/crts/redhat.ren.ca.key \
    -CAcreateserial -out /etc/crts/redhat.ren.crt

openssl x509 -in /etc/crts/redhat.ren.crt -text

/bin/cp -f /etc/crts/redhat.ren.ca.crt /etc/pki/ca-trust/source/anchors/
update-ca-trust extract

cd /data
mkdir -p /data/registry
# tar zxf registry.tgz
dnf -y install podman pigz skopeo jq 
# pigz -dc registry.tgz | tar xf -
cd /data/ocp4
podman load -i /data/ocp4/registry.tgz

podman run --name local-registry -p 5443:5000 \
  -d --restart=always \
  -v /data/registry/:/var/lib/registry:z \
  -v /etc/crts:/certs:z \
  -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/redhat.ren.crt \
  -e REGISTRY_HTTP_TLS_KEY=/certs/redhat.ren.key \
  docker.io/library/registry:2

podman start local-registry

# firewall-cmd --permanent --add-port=5443/tcp
# firewall-cmd --reload

# 加载更多的镜像
# 解压缩 ocp4.tgz
bash add.image.load.sh /data/install.image 'registry.ocp4.redhat.ren:5443'

# https://github.com/christianh814/ocp4-upi-helpernode/blob/master/docs/quickstart.md

# 准备vnc环境
vncpasswd

cat &lt;&lt; EOF &gt; ~/.vnc/config
session=gnome
securitytypes=vncauth,tlsvnc
desktop=sandbox
geometry=1280x800
alwaysshared
EOF

cat &lt;&lt; EOF &gt;&gt; /etc/tigervnc/vncserver.users
:1=root
EOF

systemctl start vncserver@:1
# 如果你想停掉vnc server，这么做
systemctl stop vncserver@:1

# firewall-cmd --permanent --add-port=6001/tcp
# firewall-cmd --permanent --add-port=5901/tcp
# firewall-cmd --reload

# connect vnc at port 5901
# export DISPLAY=:1

# 创建实验用虚拟网络

cat &lt;&lt; 'EOF' &gt; /data/kvm/bridge.sh
#!/usr/bin/env bash

PUB_CONN='eno1'
PUB_IP='172.21.6.105/24'
PUB_GW='172.21.6.254'
PUB_DNS='172.21.1.1'

nmcli con down &quot;$PUB_CONN&quot;
nmcli con delete &quot;$PUB_CONN&quot;
nmcli con down baremetal
nmcli con delete baremetal
# RHEL 8.1 appends the word &quot;System&quot; in front of the connection,delete in case it exists
nmcli con down &quot;System $PUB_CONN&quot;
nmcli con delete &quot;System $PUB_CONN&quot;
nmcli connection add ifname baremetal type bridge con-name baremetal ipv4.method 'manual' \
    ipv4.address &quot;$PUB_IP&quot; \
    ipv4.gateway &quot;$PUB_GW&quot; \
    ipv4.dns &quot;$PUB_DNS&quot;
    
nmcli con add type bridge-slave ifname &quot;$PUB_CONN&quot; master baremetal
pkill dhclient;dhclient baremetal
nmcli con down baremetal
nmcli con up baremetal
EOF
bash /data/kvm/bridge.sh
nmcli con mod baremetal +ipv4.address '192.168.7.1/24'

cat &lt;&lt; 'EOF' &gt; /data/kvm/bridge.provisioning.sh
#!/usr/bin/env bash

PUB_CONN='eno2'
PUB_IP='172.22.0.1/24'

nmcli con down &quot;$PUB_CONN&quot;
nmcli con delete &quot;$PUB_CONN&quot;
nmcli con down provisioning
nmcli con delete provisioning
# RHEL 8.1 appends the word &quot;System&quot; in front of the connection,delete in case it exists
nmcli con down &quot;System $PUB_CONN&quot;
nmcli con delete &quot;System $PUB_CONN&quot;
nmcli connection add ifname provisioning type bridge con-name provisioning ipv4.addresses $PUB_IP ipv4.method manual
    
nmcli con add type bridge-slave ifname &quot;$PUB_CONN&quot; master provisioning
nmcli con down provisioning
nmcli con up provisioning
EOF
bash /data/kvm/bridge.provisioning.sh

nmcli networking off; nmcli networking on

# 创建工具机

mkdir -p /data/kvm
cd /data/kvm

lvremove -f rhel/helperlv
lvcreate -y -L 100G -n helperlv rhel

virt-install --name=&quot;ocp4-aHelper&quot; --vcpus=4 --ram=6144 \
--disk path=/dev/rhel/helperlv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network bridge=baremetal,model=virtio \
--boot menu=on --location /data/kvm/rhel-8.3-x86_64-dvd.iso \
--initrd-inject helper-ks-rhel8-ipi.cfg --extra-args &quot;inst.ks=file:/helper-ks-rhel8-ipi.cfg&quot; 

virsh start ocp4-aHelper

# DO NOT USE, restore kvm
virsh destroy ocp4-aHelper
virsh undefine ocp4-aHelper

# virt-viewer --domain-name ocp4-aHelper
# virsh start ocp4-aHelper
# virsh list --all

# start chrony/ntp server on host
/bin/cp -f /etc/chrony.conf /etc/chrony.conf.default
cat &lt;&lt; EOF &gt; /etc/chrony.conf
# pool 2.rhel.pool.ntp.org iburst
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
allow 192.0.0.0/8
local stratum 10
logdir /var/log/chrony
EOF
systemctl enable --now chronyd
# systemctl restart chronyd
chronyc tracking
chronyc sources -v
chronyc sourcestats -v
chronyc makestep

# setup ftp data root
mount --bind /data/dnf /var/ftp/dnf
chcon -R -t public_content_t  /var/ftp/dnf

# create the master and worker vm, but not start them
export KVM_DIRECTORY=/data/kvm

# cd ${KVM_DIRECTORY}
# scp root@192.168.7.11:/data/install/*.iso ${KVM_DIRECTORY}/

create_lv() {
    var_vg=$1
    var_lv=$2
    lvremove -f $var_vg/$var_lv
    lvcreate -y -L 120G -n $var_lv $var_vg
    wipefs --all --force /dev/$var_vg/$var_lv
}

# create_lv rhel bootstraplv
create_lv nvme master0lv
create_lv nvme master1lv
create_lv nvme master2lv
create_lv rhel worker0lv
create_lv rhel worker1lv
create_lv rhel worker2lv

virt-install --name=ocp4-master0 --vcpus=4 --ram=16384 \
--disk path=/dev/nvme/master0lv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network bridge=provisioning,model=virtio \
--network bridge=baremetal,model=virtio \
--boot uefi,nvram_template=/usr/share/OVMF/OVMF_VARS.fd,menu=on  \
--print-xml &gt; ${KVM_DIRECTORY}/ocp4-master0.xml
virsh define --file ${KVM_DIRECTORY}/ocp4-master0.xml

virt-install --name=ocp4-master1 --vcpus=4 --ram=16384 \
--disk path=/dev/nvme/master1lv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network bridge=provisioning,model=virtio \
--network bridge=baremetal,model=virtio \
--boot uefi,nvram_template=/usr/share/OVMF/OVMF_VARS.fd,menu=on  \
--print-xml &gt; ${KVM_DIRECTORY}/ocp4-master1.xml
virsh define --file ${KVM_DIRECTORY}/ocp4-master1.xml

virt-install --name=ocp4-master2 --vcpus=4 --ram=16384 \
--disk path=/dev/nvme/master2lv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network bridge=provisioning,model=virtio \
--network bridge=baremetal,model=virtio \
--boot uefi,nvram_template=/usr/share/OVMF/OVMF_VARS.fd,menu=on  \
--print-xml &gt; ${KVM_DIRECTORY}/ocp4-master2.xml
virsh define --file ${KVM_DIRECTORY}/ocp4-master2.xml

virt-install --name=ocp4-worker0 --vcpus=4 --ram=32768 \
--disk path=/dev/rhel/worker0lv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network bridge=provisioning,model=virtio \
--network bridge=baremetal,model=virtio \
--boot uefi,nvram_template=/usr/share/OVMF/OVMF_VARS.fd,menu=on  \
--print-xml &gt; ${KVM_DIRECTORY}/ocp4-worker0.xml
virsh define --file ${KVM_DIRECTORY}/ocp4-worker0.xml

virt-install --name=ocp4-worker1 --vcpus=4 --ram=16384 \
--disk path=/dev/rhel/worker1lv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network bridge=provisioning,model=virtio \
--network bridge=baremetal,model=virtio \
--boot uefi,nvram_template=/usr/share/OVMF/OVMF_VARS.fd,menu=on  \
--print-xml &gt; ${KVM_DIRECTORY}/ocp4-worker1.xml
virsh define --file ${KVM_DIRECTORY}/ocp4-worker1.xml

virt-install --name=ocp4-worker2 --vcpus=4 --ram=16384 \
--disk path=/dev/rhel/worker2lv,device=disk,bus=virtio,format=raw \
--os-variant rhel8.0 --network bridge=provisioning,model=virtio \
--network bridge=baremetal,model=virtio \
--boot uefi,nvram_template=/usr/share/OVMF/OVMF_VARS.fd,menu=on  \
--print-xml &gt; ${KVM_DIRECTORY}/ocp4-worker2.xml
virsh define --file ${KVM_DIRECTORY}/ocp4-worker2.xml

cd /data/kvm/
for i in master{0..2} worker{0..2}
do
  echo -ne &quot;${i}\t&quot; ; 
  virsh dumpxml ocp4-${i} | grep &quot;mac address&quot; | cut -d\' -f2 | tr '\n' '\t'
  echo 
done &gt; mac.list
cat /data/kvm/mac.list
# master0 52:54:00:a8:77:90       52:54:00:1f:1c:1f
# master1 52:54:00:8a:97:b3       52:54:00:a1:d6:df
# master2 52:54:00:54:8f:4a       52:54:00:0b:7c:61
# worker0 52:54:00:4c:8a:80       52:54:00:f0:f4:2b
# worker1 52:54:00:89:eb:62       52:54:00:ee:e4:2b
# worker2 52:54:00:e1:ec:6e       52:54:00:1b:d6:b5

# GOTO image registry &amp; kvm host
# copy crt files to helper node
ssh-copy-id root@192.168.7.11

ssh root@192.168.7.11 mkdir -p /data/install
ssh root@192.168.7.11 mkdir -p /data/ocp4
scp /data/down/ocp4.tgz root@192.168.7.11:/data/

scp /etc/crts/redhat.ren.ca.crt root@192.168.7.11:/data/install/
scp /data/kvm/mac.list root@192.168.7.11:/data/install/

# install redfish for kvm
# https://access.redhat.com/solutions/4315581
# https://access.redhat.com/solutions/3057171
# https://docs.openstack.org/virtualbmc/latest/user/index.html
# https://docs.openstack.org/sushy-tools/latest/user/dynamic-emulator.html
dnf -y install python3-pip
# pip3 install --user sushy-tools

mkdir -p /data/install
cd /data/install

podman create --name swap docker.io/wangzheng422/imgs:openshift-baremetal-install-4.6.5 ls
podman cp swap:/openshift-baremetal-install ./
podman rm -fv swap

podman create --name swap docker.io/wangzheng422/imgs:ocp.bm.ipi.python.dep.rhel8-4.6.7 ls
podman cp swap:/wheelhouse.tar.gz - &gt; wheelhouse.tar.gz
tar zvxf wheelhouse.tar.gz
podman rm -fv swap

pip3 install --user -r wheelhouse/requirements.txt --no-index --find-links wheelhouse

ps -ef | grep vbmcd | awk '{print $2}' | xargs kill
/bin/rm -f /root/.vbmc/master.pid
/root/.local/bin/vbmcd

# curl https://registry.ocp4.redhat.ren:8000/redfish/v1/Systems/

virsh list --all
# /root/.local/bin/vbmc add ocp4-bootstrap --port 6230 --username admin --password password
# /root/.local/bin/vbmc start ocp4-bootstrap

var_i=1
for i in master{0..2} worker{0..2}
do
  /root/.local/bin/vbmc add ocp4-$i --port $(( 6230 + $var_i )) --username admin --password password
  /root/.local/bin/vbmc start ocp4-$i
  (( var_i += 1))
done

/root/.local/bin/vbmc list
# +--------------+---------+---------+------+
# | Domain name  | Status  | Address | Port |
# +--------------+---------+---------+------+
# | ocp4-master0 | running | ::      | 6231 |
# | ocp4-master1 | running | ::      | 6232 |
# | ocp4-master2 | running | ::      | 6233 |
# | ocp4-worker0 | running | ::      | 6234 |
# | ocp4-worker1 | running | ::      | 6235 |
# | ocp4-worker2 | running | ::      | 6236 |
# +--------------+---------+---------+------+

/root/.local/bin/vbmc show ocp4-master0

# DO NOT USE, restore

var_i=1
for i in master{0..2} worker{0..2}
do
  /root/.local/bin/vbmc stop ocp4-$i
  /root/.local/bin/vbmc delete ocp4-$i
  (( var_i += 1))
done

# if you want to stop or delete vm, try this
virsh list --all
# virsh destroy ocp4-bootstrap
virsh destroy ocp4-master0 
virsh destroy ocp4-master1 
virsh destroy ocp4-master2 
virsh destroy ocp4-worker0 
virsh destroy ocp4-worker1 
virsh destroy ocp4-worker2
# virsh undefine ocp4-bootstrap
virsh undefine ocp4-master0 --nvram
virsh undefine ocp4-master1 --nvram
virsh undefine ocp4-master2 --nvram
virsh undefine ocp4-worker0 --nvram
virsh undefine ocp4-worker1 --nvram
virsh undefine ocp4-worker2 --nvram

</code></pre>
<h2 id="工具机准备-3"><a class="header" href="#工具机准备-3">工具机准备</a></h2>
<p>以下是在工具机里面，进行的安装操作。</p>
<p>主要的操作有</p>
<ul>
<li>配置yum源</li>
<li>运行ansible脚本，自动配置工具机</li>
<li>上传定制的安装配置文件</li>
<li>生成ignition文件</li>
</ul>
<pre><code class="language-bash">
sed -i 's/#UseDNS yes/UseDNS no/g' /etc/ssh/sshd_config
systemctl restart sshd

cat &lt;&lt; EOF &gt; /root/.ssh/config
StrictHostKeyChecking no
UserKnownHostsFile=/dev/null
EOF

systemctl disable --now firewalld

# in helper node
mkdir /etc/yum.repos.d.bak
mv /etc/yum.repos.d/* /etc/yum.repos.d.bak

export YUMIP=&quot;192.168.7.1&quot;
cat &lt;&lt; EOF &gt; /etc/yum.repos.d/remote.repo
[remote-epel]
name=epel
baseurl=ftp://${YUMIP}/dnf/epel
enabled=1
gpgcheck=0

[remote-epel-modular]
name=epel-modular
baseurl=ftp://${YUMIP}/dnf/epel-modular
enabled=1
gpgcheck=0

[remote-appstream]
name=appstream
baseurl=ftp://${YUMIP}/dnf/rhel-8-for-x86_64-appstream-rpms
enabled=1
gpgcheck=0

[remote-baseos]
name=baseos
baseurl=ftp://${YUMIP}/dnf/rhel-8-for-x86_64-baseos-rpms
enabled=1
gpgcheck=0

[remote-baseos-source]
name=baseos-source
baseurl=ftp://${YUMIP}/dnf/rhel-8-for-x86_64-baseos-source-rpms
enabled=1
gpgcheck=0

[remote-supplementary]
name=supplementary
baseurl=ftp://${YUMIP}/dnf/rhel-8-for-x86_64-supplementary-rpms
enabled=1
gpgcheck=0

[remote-codeready-builder]
name=supplementary
baseurl=ftp://${YUMIP}/dnf/codeready-builder-for-rhel-8-x86_64-rpms
enabled=1
gpgcheck=0

EOF

yum clean all
yum makecache
yum repolist

yum -y install ansible git unzip podman python3

yum -y update

reboot

# yum -y install ansible git unzip podman python36

mkdir -p /data/ocp4/
# scp ocp4.tgz to /data
# scp /data/down/ocp4.tgz root@192.168.7.11:/data/
cd /data
tar zvxf ocp4.tgz
cd /data/ocp4

# 这里使用了一个ansible的项目，用来部署helper节点的服务。
# https://github.com/wangzheng422/ocp4-upi-helpernode
unzip ocp4-upi-helpernode.zip
# 这里使用了一个ignition文件合并的项目，用来帮助自定义ignition文件。
# https://github.com/wangzheng422/filetranspiler
podman load -i filetranspiler.tgz

mkdir -p /data/install

mkdir -p /data/ocp4/

cat &lt;&lt; EOF &gt; /data/ocp4/ocp4-upi-helpernode-master/vars-dhcp.rhel8.yaml
---
ssh_gen_key: true
staticips: false
bm_ipi: true
firewalld: false
dns_forward: false
iso:
  iso_dl_url: &quot;file:///data/ocp4/rhcos-live.x86_64.iso&quot;
  my_iso: &quot;rhcos-live.iso&quot;
helper:
  name: &quot;helper&quot;
  ipaddr: &quot;192.168.7.11&quot;
  networkifacename: &quot;enp1s0&quot;
  gateway: &quot;192.168.7.1&quot;
  netmask: &quot;255.255.255.0&quot;
dns:
  domain: &quot;redhat.ren&quot;
  clusterid: &quot;ocp4&quot;
  forwarder1: &quot;192.168.7.1&quot;
  forwarder2: &quot;192.168.7.1&quot;
  api_vip: &quot;192.168.7.100&quot;
  ingress_vip: &quot;192.168.7.101&quot;
dhcp:
  router: &quot;192.168.7.1&quot;
  bcast: &quot;192.168.7.255&quot;
  netmask: &quot;255.255.255.0&quot;
  poolstart: &quot;192.168.7.70&quot;
  poolend: &quot;192.168.7.90&quot;
  ipid: &quot;192.168.7.0&quot;
  netmaskid: &quot;255.255.255.0&quot;
bootstrap:
  name: &quot;bootstrap&quot;
  ipaddr: &quot;192.168.7.12&quot;
  interface: &quot;enp1s0&quot;
  install_drive: &quot;vda&quot;
  macaddr: &quot;52:54:00:7e:f8:f7&quot;
masters:
  - name: &quot;master-0&quot;
    ipaddr: &quot;192.168.7.13&quot;
    interface: &quot;enp1s0&quot;
    install_drive: &quot;vda&quot;
    macaddr: &quot;$(cat /data/install/mac.list | grep master0 | awk '{print $3}')&quot;
  - name: &quot;master-1&quot;
    ipaddr: &quot;192.168.7.14&quot;
    interface: &quot;enp1s0&quot;
    install_drive: &quot;vda&quot;    
    macaddr: &quot;$(cat /data/install/mac.list | grep master1 | awk '{print $3}')&quot;
  - name: &quot;master-2&quot;
    ipaddr: &quot;192.168.7.15&quot;
    interface: &quot;enp1s0&quot;
    install_drive: &quot;vda&quot;   
    macaddr: &quot;$(cat /data/install/mac.list | grep master2 | awk '{print $3}')&quot;
workers:
  - name: &quot;worker-0&quot;
    ipaddr: &quot;192.168.7.16&quot;
    interface: &quot;enp1s0&quot;
    install_drive: &quot;vda&quot;
    macaddr: &quot;$(cat /data/install/mac.list | grep worker0 | awk '{print $3}')&quot;
  - name: &quot;worker-1&quot;
    ipaddr: &quot;192.168.7.17&quot;
    interface: &quot;enp1s0&quot;
    install_drive: &quot;vda&quot;
    macaddr: &quot;$(cat /data/install/mac.list | grep worker1 | awk '{print $3}')&quot;
  - name: &quot;worker-2&quot;
    ipaddr: &quot;192.168.7.18&quot;
    interface: &quot;enp1s0&quot;
    install_drive: &quot;vda&quot;
    macaddr: &quot;$(cat /data/install/mac.list | grep worker2 | awk '{print $3}')&quot;
others:
  - name: &quot;registry&quot;
    ipaddr: &quot;192.168.7.1&quot;
    macaddr: &quot;52:54:00:7e:f8:f7&quot;
  - name: &quot;yum&quot;
    ipaddr: &quot;192.168.7.1&quot;
    macaddr: &quot;52:54:00:7e:f8:f7&quot;
  - name: &quot;quay&quot;
    ipaddr: &quot;192.168.7.1&quot;
    macaddr: &quot;52:54:00:7e:f8:f7&quot;
otherdomains:
  - domain: &quot;rhv.redhat.ren&quot;
    hosts:
    - name: &quot;manager&quot;
      ipaddr: &quot;192.168.7.71&quot;
    - name: &quot;rhv01&quot;
      ipaddr: &quot;192.168.7.72&quot;
  - domain: &quot;cmri-edge.redhat.ren&quot;
    hosts:
    - name: &quot;*&quot;
      ipaddr: &quot;192.168.7.71&quot;
    - name: &quot;*.apps&quot;
      ipaddr: &quot;192.168.7.72&quot;
force_ocp_download: false
remove_old_config_files: false
ocp_client: &quot;file:///data/ocp4/4.6.5/openshift-client-linux-4.6.5.tar.gz&quot;
ocp_installer: &quot;file:///data/ocp4/4.6.5/openshift-install-linux-4.6.5.tar.gz&quot;
ppc64le: false
arch: 'x86_64'
chronyconfig:
  enabled: true
  content:
    - server: &quot;192.168.7.1&quot;
      options: iburst
setup_registry:
  deploy: false
  registry_image: docker.io/library/registry:2
  local_repo: &quot;ocp4/openshift4&quot;
  product_repo: &quot;openshift-release-dev&quot;
  release_name: &quot;ocp-release&quot;
  release_tag: &quot;4.6.1-x86_64&quot;
registry_server: &quot;registry.ocp4.redhat.ren:5443&quot;
EOF

# 接下来，我们使用ansible来配置helper节点，装上各种openshift集群需要的服务
# 根据现场环境，修改 ocp4-upi-helpernode-master/vars-static.yaml
cd /data/ocp4/ocp4-upi-helpernode-master
ansible-playbook -e @vars-dhcp.rhel8.yaml -e '{ staticips: false, bm_ipi: true }'  tasks/main.yml

# try this:
/usr/local/bin/helpernodecheck

# GO back to help node
/bin/cp -f /data/install/redhat.ren.ca.crt /etc/pki/ca-trust/source/anchors/
update-ca-trust extract

# 定制ignition
cd /data/install

# 根据现场环境，修改 install-config.yaml
# 至少要修改ssh key， 还有 additionalTrustBundle，这个是镜像仓库的csr 

# vi install-config.yaml 
cat &lt;&lt; EOF &gt; /data/install/install-config.yaml 
apiVersion: v1
baseDomain: redhat.ren
platform:
  baremetal:
    apiVIP: 192.168.7.100
    ingressVIP: 192.168.7.101
    # provisioningBridge: provisioning
    provisioningNetworkCIDR: 172.22.0.0/24
    # provisioningDHCPRange: 172.22.0.10,172.22.0.100
    # clusterProvisioningIP: 172.22.0.3
    # bootstrapProvisioningIP: 172.22.0.2
    # provisioningNetwork: Managed
    provisioningNetworkInterface: enp1s0
    # externalBridge: baremetal
    bootstrapOSImage: http://192.168.7.11:8080/install/rhcos-qemu.x86_64.qcow2.gz?sha256=$(zcat /var/www/html/install/rhcos-qemu.x86_64.qcow2.gz | sha256sum | awk '{print $1}')
    clusterOSImage: http://192.168.7.11:8080/install/rhcos-openstack.x86_64.qcow2.gz?sha256=$(sha256sum /var/www/html/install/rhcos-openstack.x86_64.qcow2.gz | awk '{print $1}')
    hosts:
      - name: master-0
        role: master
        bmc:
          address: ipmi://192.168.7.1:6231
          username: admin
          password: password
          disableCertificateVerification: True
        bootMACAddress: $(cat mac.list | grep master0 | awk '{print $2}')
        hardwareProfile: default 
        rootDeviceHints:
          deviceName: &quot;/dev/vda&quot;
      - name: master-1
        role: master
        bmc:
          address: ipmi://192.168.7.1:6232
          username: admin
          password: password
          disableCertificateVerification: True
        bootMACAddress: $(cat mac.list | grep master1 | awk '{print $2}')
        hardwareProfile: default 
        rootDeviceHints:
          deviceName: &quot;/dev/vda&quot;
      - name: master-2
        role: master
        bmc:
          address: ipmi://192.168.7.1:6233
          username: admin
          password: password
          disableCertificateVerification: True
        bootMACAddress: $(cat mac.list | grep master2 | awk '{print $2}')
        hardwareProfile: default 
        rootDeviceHints:
          deviceName: &quot;/dev/vda&quot;
      - name: worker-0
        role: worker
        bmc:
          address: ipmi://192.168.7.1:6234
          username: admin
          password: password
          disableCertificateVerification: True
        bootMACAddress: $(cat mac.list | grep worker0 | awk '{print $2}')
        hardwareProfile: unknown         
        rootDeviceHints:
          deviceName: &quot;/dev/vda&quot;
      - name: worker-1
        role: worker
        bmc:
          address: ipmi://192.168.7.1:6235
          username: admin
          password: password
          disableCertificateVerification: True
        bootMACAddress: $(cat mac.list | grep worker1 | awk '{print $2}')
        hardwareProfile: unknown         
        rootDeviceHints:
          deviceName: &quot;/dev/vda&quot;
metadata:
  name: ocp4
networking:
  clusterNetworks:
  - cidr: 10.254.0.0/16
    hostPrefix: 24
  networkType: OpenShiftSDN
  serviceNetwork:
  - 172.30.0.0/16
  machineCIDR: 192.168.7.0/24
compute:
- name: worker
  replicas: 2
controlPlane:
  name: master
  replicas: 3
  platform:
    baremetal: {}
pullSecret: '{&quot;auths&quot;:{&quot;registry.ocp4.redhat.ren:5443&quot;: {&quot;auth&quot;: &quot;ZHVtbXk6ZHVtbXk=&quot;,&quot;email&quot;: &quot;noemail@localhost&quot;},&quot;registry.ppa.redhat.ren:5443&quot;: {&quot;auth&quot;: &quot;ZHVtbXk6ZHVtbXk=&quot;,&quot;email&quot;: &quot;noemail@localhost&quot;}}}'
sshKey: |
$( cat /root/.ssh/helper_rsa.pub | sed 's/^/   /g' )
additionalTrustBundle: |
$( cat /data/install/redhat.ren.ca.crt | sed 's/^/   /g' )
imageContentSources:
- mirrors:
  - registry.ocp4.redhat.ren:5443/ocp4/openshift4
  source: quay.io/openshift-release-dev/ocp-release
- mirrors:
  - registry.ocp4.redhat.ren:5443/ocp4/openshift4
  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
EOF

# GO back to host
mkdir -p /data/install
cd /data/install
scp root@192.168.7.11:/data/install/install-config.yaml /data/install/

cd /data/install
for i in $(sudo virsh list --all | tail -n +3 | grep bootstrap | awk {'print $2'});
do
  sudo virsh destroy $i;
  sudo virsh undefine $i;
  sudo virsh vol-delete $i --pool default;
  sudo virsh vol-delete $i.ign --pool default;
  virsh pool-destroy $i
  virsh pool-delete $i
  virsh pool-undefine $i
done
/bin/rm -rf .openshift_install.log .openshift_install_state.json terraform* auth tls 
./openshift-baremetal-install --dir /data/install/ --log-level debug create cluster
# INFO Install complete!
# INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/data/install/auth/kubeconfig'
# INFO Access the OpenShift web-console here: https://console-openshift-console.apps.ocp4.redhat.ren
# INFO Login to the console with user: &quot;kubeadmin&quot;, and password: &quot;dTSbu-aIIZr-gxRxT-njrEr&quot;
</code></pre>
<p>安装的过程是全自动的，所以也不用干什么，在provisioning network的模式下，可以看到master激活了2个网卡。
<img src="ocp4/4.6/imgs/2020-12-13-20-19-52.png" alt="" /></p>
<p>接着，我们就可以去helper节点上，用我们熟悉的oc命令操作集群了。</p>
<pre><code class="language-bash"># on kvm host, copy back auth folder
rsync -arz /data/install/auth root@192.168.7.11:/data/install/

# Go back to helper
ansible localhost -m lineinfile -a 'path=$HOME/.bashrc regexp=&quot;^export KUBECONFIG&quot; line=&quot;export KUBECONFIG=/data/install/auth/kubeconfig&quot;'
source $HOME/.bashrc

oc get node
oc get pod -n openshift-machine-api
oc get BareMetalHost -n openshift-machine-api
oc get bmh -n openshift-machine-api
# NAME       STATUS   PROVISIONING STATUS      CONSUMER                    BMC                       HARDWARE PROFILE   ONLINE   ERROR
# master-0   OK       externally provisioned   ocp4-sbsqb-master-0         ipmi://192.168.7.1:6231                      true
# master-1   OK       externally provisioned   ocp4-sbsqb-master-1         ipmi://192.168.7.1:6232                      true
# master-2   OK       externally provisioned   ocp4-sbsqb-master-2         ipmi://192.168.7.1:6233                      true
# worker-0   OK       provisioned              ocp4-sbsqb-worker-0-kcz5t   ipmi://192.168.7.1:6234   unknown            true
# worker-1   OK       provisioned              ocp4-sbsqb-worker-0-5ktqw   ipmi://192.168.7.1:6235   unknown            true
# worker-2   OK       ready                                                ipmi://192.168.7.1:6236   unknown            false

oc get pod -n openshift-kni-infra
</code></pre>
<p>我们就能看到bm的输出了
<img src="ocp4/4.6/imgs/2020-12-13-21-24-39.png" alt="" /></p>
<p>可以看到web console上node的配置指向了bm
<img src="ocp4/4.6/imgs/2020-12-13-21-25-39.png" alt="" /></p>
<p>我们也可以看到久违的machine配置
<img src="ocp4/4.6/imgs/2020-12-13-21-26-17.png" alt="" /></p>
<h2 id="添加一个新节点-1"><a class="header" href="#添加一个新节点-1">添加一个新节点</a></h2>
<p>IPI 模式下，添加一个新节点非常方便，只要定义一个BareMetalHost就好了。</p>
<pre><code class="language-bash">cd /data/install/
cat &lt;&lt; EOF &gt; /data/install/bmh.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: worker-2-bmc-secret
type: Opaque
data:
  username: $(echo -ne &quot;admin&quot; | base64)
  password: $(echo -ne &quot;password&quot; | base64)
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: worker-2
spec:
  online: true
  bootMACAddress: $(cat mac.list | grep worker2 | awk '{print $2}')
  bmc:
    address: ipmi://192.168.7.1:6236
    credentialsName: worker-2-bmc-secret
    disableCertificateVerification: true
  hardwareProfile: unknown         
  rootDeviceHints:
    deviceName: /dev/vda
EOF
oc -n openshift-machine-api create -f bmh.yaml

# DO NOT USE, restore, delete the vm
oc -n openshift-machine-api delete -f bmh.yaml

oc get bmh -n openshift-machine-api
# NAME       STATUS   PROVISIONING STATUS      CONSUMER                    BMC                       HARDWARE PROFILE   ONLINE   ERROR
# master-0   OK       externally provisioned   ocp4-sbsqb-master-0         ipmi://192.168.7.1:6231                      true
# master-1   OK       externally provisioned   ocp4-sbsqb-master-1         ipmi://192.168.7.1:6232                      true
# master-2   OK       externally provisioned   ocp4-sbsqb-master-2         ipmi://192.168.7.1:6233                      true
# worker-0   OK       provisioned              ocp4-sbsqb-worker-0-kcz5t   ipmi://192.168.7.1:6234   unknown            true
# worker-1   OK       provisioned              ocp4-sbsqb-worker-0-5ktqw   ipmi://192.168.7.1:6235   unknown            true
# worker-2   OK       ready                                                ipmi://192.168.7.1:6236   unknown            false

oc get machinesets -n openshift-machine-api
# NAME                  DESIRED   CURRENT   READY   AVAILABLE   AGE
# ocp4-sbsqb-worker-0   2         2         2       2           99m

oc get machinesets -n openshift-machine-api -o json | jq -r .items[0].metadata.name

# 扩容worker到3副本，会触发worker-2的部署
oc scale --replicas=3 machineset $(oc get machinesets -n openshift-machine-api -o json | jq -r .items[0].metadata.name) -n openshift-machine-api

</code></pre>
<h2 id="排错技巧-1"><a class="header" href="#排错技巧-1">排错技巧</a></h2>
<pre><code class="language-bash">
# login to bootstrap to debug
# find the ip from kvm console
ssh -i ~/.ssh/helper_rsa core@192.168.7.75
journalctl -b -f -u release-image.service -u bootkube.service
journalctl -b -u release-image.service -u bootkube.service | grep -i baremetal
sudo -i
export KUBECONFIG=/etc/kubernetes/kubeconfig
oc get pod -n openshift-machine-api
oc get BareMetalHost -n openshift-machine-api

# debug why bootstrap can't be ping...
cat .openshift_install_state.json | jq  '.&quot;*bootstrap.Bootstrap&quot;'.Config.storage.files[].path

cat .openshift_install_state.json | jq -r '.&quot;*bootstrap.Bootstrap&quot;'.File.Data | base64 -d | jq -r . &gt; ign.json

cat .openshift_install_state.json | jq  -r '.&quot;*bootstrap.Bootstrap&quot;.Config.storage.files[].contents.source ' | sed 's/.*base64,//g' | base64 -d &gt; decode

cat .openshift_install_state.json | jq  -r '.&quot;*bootstrap.Bootstrap&quot;.Config.storage.files[] | .path, .contents.source ' | while read -r line ; do if [[ $line =~ .*base64,.* ]]; then echo $(echo $line | sed 's/.*base64,//g' | base64 -d) ; else echo $line; fi; done &gt; files


</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="nvidia-gpu-for-openshift-46-disconnected-英伟达gpu离线安装"><a class="header" href="#nvidia-gpu-for-openshift-46-disconnected-英伟达gpu离线安装">nvidia gpu for openshift 4.6 disconnected 英伟达GPU离线安装</a></h1>
<h2 id="简介-2"><a class="header" href="#简介-2">简介</a></h2>
<p>本次实验是openshift 边缘 GPU 场景的一部分，主要关注于nvidia gpu如何在离线的情况下安装。关于如何 gpu passthrough 到kvm，模拟边缘gpu主机，见<a href="ocp4/4.6/./4.6.disconnect.bm.upi.static.ip.on.rhel8.edge.html">这个文档</a></p>
<p>以下是讲解视频</p>
<p><a href="https://www.bilibili.com/video/BV1CX4y1P7AK/"><kbd><img src="ocp4/4.6/imgs/2021-01-21-16-10-17.png" width="600"></kbd></a></p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1CX4y1P7AK/">bilibili</a></li>
<li><a href="https://www.ixigua.com/6920198650257900039">xigua</a></li>
<li><a href="https://youtu.be/hrHsUdfZEBY">youtube</a></li>
</ul>
<p>以下是本次实验的架构图:</p>
<p><img src="ocp4/4.6/./dia/4.6.bm.upi.3node.drawio.svg" alt="" /></p>
<h2 id="制作-rhel8-repo--安装源"><a class="header" href="#制作-rhel8-repo--安装源">制作 rhel8 repo / 安装源</a></h2>
<p>nvidia gpu operator需要在线下载包，来编译driver，那么在离线场景，我们就需要先准备一个rhel8 的 repo。</p>
<pre><code class="language-bash">export PROXY=&quot;127.0.0.1:18801&quot;

subscription-manager --proxy=$PROXY release --list

subscription-manager --proxy=$PROXY release --set=8

subscription-manager --proxy=$PROXY repos --disable=&quot;*&quot;
subscription-manager --proxy=$PROXY repos \
    --enable=&quot;rhel-8-for-x86_64-baseos-rpms&quot; \
    --enable=&quot;rhel-8-for-x86_64-baseos-source-rpms&quot; \
    --enable=&quot;rhel-8-for-x86_64-appstream-rpms&quot; \
    --enable=&quot;rhel-8-for-x86_64-supplementary-rpms&quot; \
    --enable=&quot;codeready-builder-for-rhel-8-x86_64-rpms&quot; \
    --enable=&quot;rhocp-4.6-for-rhel-8-x86_64-rpms&quot; \
    --enable=&quot;rhel-8-for-x86_64-baseos-eus-rpms&quot; \
    # endline

mkdir -p /data/dnf/gaps
cd /data/dnf/gaps

# subscription-manager --proxy=$PROXY release --set=8.2
# subscription-manager --proxy=$PROXY release --set=8

# dnf -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm
dnf copr enable frostyx/modulemd-tools
dnf install -y modulemd-tools 
# dnf install -y https://kojipkgs.fedoraproject.org//packages/modulemd-tools/0.9/1.fc32/noarch/modulemd-tools-0.9-1.fc32.noarch.rpm

# 注意，这里需要的包，需要先部署一下gpu operator，然后看看driver的日志，里面装什么包，这里替换成相应的包，不同版本的gpu operator要求不同，所以这里的包也不一样。
/bin/rm -rf /data/dnf/gaps/*
# dnf download --resolve --releasever=8.2 --alldeps \
# --repo rhel-8-for-x86_64-baseos-eus-rpms,rhel-8-for-x86_64-baseos-rpms,rhel-8-for-x86_64-appstream-rpms,ubi-8-baseos,ubi-8-appstream \
# kernel-headers.x86_64 kernel-devel.x86_64 kernel-core.x86_64 systemd-udev.x86_64 elfutils-libelf.x86_64 elfutils-libelf-devel.x86_64 \
# kernel-headers-4.18.0-193.40.1.el8_2.x86_64 kernel-devel-4.18.0-193.40.1.el8_2.x86_64 kernel-core-4.18.0-193.40.1.el8_2.x86_64 systemd-udev-239-31.el8_2.2.x86_64 kernel-headers-4.18.0-193.41.1.el8_2.x86_64 kernel-devel-4.18.0-193.41.1.el8_2.x86_64 \
# elfutils-libelf-0.180-1.el8.x86_64

subscription-manager --proxy=$PROXY release --set=8.2

dnf download --resolve --releasever=8.2 --alldeps \
--repo rhel-8-for-x86_64-baseos-eus-rpms,rhel-8-for-x86_64-baseos-rpms,rhel-8-for-x86_64-appstream-rpms,ubi-8-baseos,ubi-8-appstream \
kernel-headers.x86_64 kernel-devel.x86_64 kernel-core.x86_64 systemd-udev.x86_64 elfutils-libelf.x86_64 elfutils-libelf-devel.x86_64 \
kernel-headers-4.18.0-193.41.1.el8_2.x86_64 kernel-devel-4.18.0-193.41.1.el8_2.x86_64 kernel-core-4.18.0-193.41.1.el8_2.x86_64

subscription-manager --proxy=$PROXY release --set=8

dnf download --resolve --alldeps \
--repo rhel-8-for-x86_64-baseos-rpms,rhel-8-for-x86_64-appstream-rpms,ubi-8-baseos,ubi-8-appstream \
elfutils-libelf.x86_64 elfutils-libelf-devel.x86_64 

# https://access.redhat.com/solutions/4907601
createrepo ./
repo2module . \
    --module-name foo \
    --module-stream devel \
    --module-version 123 \
    --module-context f32
createrepo_mod .

</code></pre>
<p>现在，本机的 /data/dnf/gaps/ 目录，就是repo的目录了，做一个ftp服务，把他暴露出去就好了。具体方法，<a href="ocp4/4.6//redhat/rhel/rhel8.build.kernel.repo.cache.html">参考这里</a></p>
<h2 id="修改英伟达驱动镜像--nvidia-driver-image"><a class="header" href="#修改英伟达驱动镜像--nvidia-driver-image">修改英伟达驱动镜像 / nvidia driver image</a></h2>
<p>默认 nvidia gpu driver pod 是需要联网下载各种包的，这里面还涉及到订阅，非常麻烦，而且离线无法使用。</p>
<p>我们刚才已经做了一个离线的repo仓库，那么我们就需要定制一下driver image，让他直接用离线的repo仓库就好了。</p>
<p>官方driver镜像下载： https://ngc.nvidia.com/catalog/containers/nvidia:driver/tags</p>
<p><img src="ocp4/4.6/imgs/2021-01-27-17-35-18.png" alt="" /></p>
<pre><code class="language-bash">
mkdir -p /data/install/
cd /data/install
# /bin/rm -rf /etc/yum.repos.d/* 
export YUMIP=&quot;192.168.7.1&quot;
cat &lt;&lt; EOF &gt; ./remote.repo
[gaps]
name=gaps
baseurl=ftp://${YUMIP}/dnf/gaps
enabled=1
gpgcheck=0

EOF

oc create configmap repo-config -n gpu-operator-resources --from-file=./remote.repo

</code></pre>
<p>可以使用oeprator UI 来安装ClusterPolicy，注意调整driver config 的 repo config : repo-config  -&gt;  /etc/yum.repos.d
<img src="ocp4/4.6/imgs/2021-02-08-11-38-14.png" alt="" /></p>
<h2 id="定制driver-config-image"><a class="header" href="#定制driver-config-image">定制driver config image</a></h2>
<p>如果我们对driver config image有特殊需求，那么这样定制</p>
<p>here is an reference: https://github.com/dmc5179/nvidia-driver</p>
<p>有人把 rpm 包直接装在driver image里面了，也是一个很好的思路。</p>
<pre><code class="language-bash">
# driver image
# nvidia-driver-daemonset
podman pull nvcr.io/nvidia/driver:450.80.02-rhcos4.6

# you can test the driver image, like this:
# podman run --rm -it --entrypoint='/bin/bash' nvcr.io/nvidia/driver:450.80.02-rhcos4.6

podman run --rm -it --entrypoint='/bin/bash' nvcr.io/nvidia/driver:460.32.03-rhcos4.6

mkdir -p /data/gpu/
cd /data/gpu
export YUMIP=&quot;192.168.7.1&quot;
cat &lt;&lt; EOF &gt; /data/gpu/remote.repo
[gaps]
name=gaps
baseurl=ftp://${YUMIP}/dnf/gaps
enabled=1
gpgcheck=0

EOF

cat &lt;&lt; EOF &gt; /data/gpu/Dockerfile
FROM nvcr.io/nvidia/driver:450.80.02-rhcos4.6

RUN /bin/rm -rf /etc/yum.repos.d/* 
COPY remote.repo /etc/yum.repos.d/remote.repo

EOF

var_date=$(date '+%Y-%m-%d-%H%M')
echo $var_date

buildah bud --format=docker -t docker.io/wangzheng422/imgs:nvidia-gpu-driver-$var_date-rhcos4.6 -f Dockerfile .

# podman run --rm -it --entrypoint='/bin/bash' docker.io/wangzheng422/imgs:nvidia-gpu-driver-2021-01-21-0942

buildah push docker.io/wangzheng422/imgs:nvidia-gpu-driver-$var_date-rhcos4.6
echo &quot;docker.io/wangzheng422/imgs:nvidia-gpu-driver-$var_date-rhcos4.6&quot;

# docker.io/wangzheng422/imgs:nvidia-gpu-driver-2021-02-05-1131-rhcos4.6

</code></pre>
<p>好了，我们最后制作好的镜像，使用tag的实话，注意要省略到后面的 rhcos4.6， 因为operator UI 会自动补全。</p>
<h2 id="开始离线安装gpu-operator"><a class="header" href="#开始离线安装gpu-operator">开始离线安装gpu operator</a></h2>
<p>参考nvidia官方的<a href="https://docs.nvidia.com/datacenter/kubernetes/openshift-on-gpu-install-guide/">安装文档</a></p>
<p>首先，我们要安装 node feature discovery (nfd)，在以前，nfd只能扫描独立worker节点，所以 3 节点的edge模式，那个时候不支持的。在后面的版本里面，nfd修复了这个漏洞，3节点的edge模式也支持了。
<img src="ocp4/4.6/imgs/2021-01-21-11-56-34.png" alt="" /></p>
<p>然后给nfd做一个配置，直接点击创建就可以，什么都不用改，注意namespace是openshift-operator
<img src="ocp4/4.6/imgs/2021-01-21-11-58-14.png" alt="" /></p>
<p>接下来，我们先创建一个namespace gpu-operator-resources, 然后安装 nvidia gpu operator
<img src="ocp4/4.6/imgs/2021-01-21-12-00-53.png" alt="" /></p>
<p>我们在gpu operator里面，创建一个cluster policy，注意要修改他的参数。这里给的例子是定制过driver镜像的，如果没定制，不用修改这个参数。
<img src="ocp4/4.6/imgs/2021-02-07-16-41-43.png" alt="" /></p>
<p>最后我们从 node label 上就能看见识别到的 gpu 了。
<img src="ocp4/4.6/imgs/2021-01-21-11-43-45.png" alt="" /></p>
<h2 id="测试一下"><a class="header" href="#测试一下">测试一下</a></h2>
<pre><code class="language-bash"># 先按照官方文档试试
oc project gpu-operator-resources

POD_NAME=$(oc get pods -o json | jq -r '.items[] | select( .metadata.name | contains(&quot;nvidia-driver-daemonset&quot;) ) | .metadata.name' | head )

oc exec -it $POD_NAME -- nvidia-smi 
# Thu Jan 21 04:12:36 2021
# +-----------------------------------------------------------------------------+
# | NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |
# |-------------------------------+----------------------+----------------------+
# | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
# | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
# |                               |                      |               MIG M. |
# |===============================+======================+======================|
# |   0  Tesla T4            On   | 00000000:05:00.0 Off |                  Off |
# | N/A   27C    P8    14W /  70W |      0MiB / 16127MiB |      0%      Default |
# |                               |                      |                  N/A |
# +-------------------------------+----------------------+----------------------+

# +-----------------------------------------------------------------------------+
# | Processes:                                                                  |
# |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
# |        ID   ID                                                   Usage      |
# |=============================================================================|
# |  No running processes found                                                 |
# +-----------------------------------------------------------------------------+

# 我们再启动个应用试试
# https://nvidia.github.io/gpu-operator/

# https://ngc.nvidia.com/catalog/containers/nvidia:tensorrt
# 我们按照这个官方文档，做一个测试镜像

# goto helper
cd /data/ocp4

cat &lt;&lt; EOF &gt; /data/ocp4/gpu.yaml
---
kind: Deployment
apiVersion: apps/v1
metadata:
  annotations:
  name: demo1
  labels:
    app: demo1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo1
  template:
    metadata:
      labels:
        app: demo1
    spec:
      nodeSelector:
        kubernetes.io/hostname: 'worker-0'
      restartPolicy: Always
      containers:
        - name: demo1
          image: &quot;nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda10.2-ubi8&quot;

EOF
oc project demo
oc apply -f gpu.yaml
# [Vector addition of 50000 elements]
# Copy input data from the host memory to the CUDA device
# CUDA kernel launch with 196 blocks of 256 threads
# Copy output data from the CUDA device to the host memory
# Test PASSED
# Done

oc delete -f gpu.yaml

# on build host
# https://ngc.nvidia.com/catalog/containers/nvidia:tensorrt
# podman run -it nvcr.io/nvidia/tensorrt:20.12-py3
mkdir -p /data/gpu
cd /data/gpu
cat &lt;&lt; EOF &gt; /data/gpu/Dockerfile
FROM docker.io/wangzheng422/imgs:tensorrt-ljj

CMD tail -f /dev/null

EOF
var_date=$(date '+%Y-%m-%d-%H%M')
echo $var_date

buildah bud --format=docker -t docker.io/wangzheng422/imgs:tensorrt-ljj-$var_date -f Dockerfile .

buildah push docker.io/wangzheng422/imgs:tensorrt-ljj-$var_date
echo &quot;docker.io/wangzheng422/imgs:tensorrt-ljj-$var_date&quot;

# docker.io/wangzheng422/imgs:tensorrt-ljj-2021-01-21-1151

# go back to helper node
cat &lt;&lt; EOF &gt; /data/ocp4/gpu.yaml
---
kind: Deployment
apiVersion: apps/v1
metadata:
  annotations:
  name: demo1
  labels:
    app: demo1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo1
  template:
    metadata:
      labels:
        app: demo1
    spec:
      nodeSelector:
        kubernetes.io/hostname: 'worker-0'
      restartPolicy: Always
      containers:
        - name: demo1
          image: docker.io/wangzheng422/imgs:tensorrt-ljj-2021-01-21-1151

EOF
oc project demo
oc apply -f gpu.yaml

# oc rsh into the pod, run sample program using gpu
# cd tensorrt/bin/
# ./sample_mnist
# you will see this correct result
# &amp;&amp;&amp;&amp; PASSED TensorRT.sample_mnist # ./sample_mnist

oc delete -f gpu.yaml


</code></pre>
<h1 id="tips"><a class="header" href="#tips">tips</a></h1>
<ol>
<li>如果发现nfd不能发现gpu型号，node reboot就好了</li>
<li>如果发现gpu feature discovery不正常， node reboot就好了</li>
</ol>
<pre><code class="language-bash">cat /proc/driver/nvidia/version

</code></pre>
<h2 id="reference"><a class="header" href="#reference">reference</a></h2>
<p>https://www.openshift.com/blog/simplifying-deployments-of-accelerated-ai-workloads-on-red-hat-openshift-with-nvidia-gpu-operator</p>
<p>https://www.openshift.com/blog/how-to-use-entitled-image-builds-to-build-drivercontainers-with-ubi-on-openshift</p>
<p>https://access.redhat.com/solutions/5232901</p>
<p>https://docs.nvidia.com/datacenter/kubernetes/openshift-on-gpu-install-guide/</p>
<p>https://access.redhat.com/solutions/4907601</p>
<p>https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/index.html</p>
<h2 id="以下弯路"><a class="header" href="#以下弯路">以下弯路</a></h2>
<pre><code class="language-bash"># add ubi support
cat &lt;&lt; EOF &gt; /etc/yum.repos.d/ubi.repo
[ubi-8-baseos]
name=ubi-8-baseos
baseurl=https://cdn-ubi.redhat.com/content/public/ubi/dist/ubi8/8/x86_64/baseos/os
enabled=1
gpgcheck=1

[ubi-8-appstream]
name=ubi-8-appstream
baseurl=https://cdn-ubi.redhat.com/content/public/ubi/dist/ubi8/8/x86_64/appstream/os
enabled=1
gpgcheck=1

[ubi-8-codeready-builder]
name=ubi-8-codeready-builder
baseurl=https://cdn-ubi.redhat.com/content/public/ubi/dist/ubi8/8/x86_64/codeready-builder/os/
enabled=1
gpgcheck=1

EOF

cd /data/dnf
dnf reposync -m --download-metadata --delete -n



cat &lt;&lt; EOF &gt; /data/ocp4/gpu.yaml
---
kind: Deployment
apiVersion: apps/v1
metadata:
  annotations:
  name: demo1
  labels:
    app: demo1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo1
  template:
    metadata:
      labels:
        app: demo1
    spec:
      nodeSelector:
        kubernetes.io/hostname: 'worker-0'
      restartPolicy: Always
      containers:
        - name: demo1
          image: nvidia/cuda:11.1.1-devel-centos8

EOF
oc project demo
oc apply -f gpu.yaml

oc delete -f gpu.yaml

cat &lt;&lt; EOF &gt; /data/gpu/Dockerfile
FROM nvcr.io/nvidia/tensorrt:20.12-py3

RUN /opt/tensorrt/python/python_setup.sh
RUN /opt/tensorrt/install_opensource.sh 
RUN /opt/tensorrt/install_opensource.sh -b master
# RUN cd /workspace/tensorrt/samples &amp;&amp; make -j4

CMD tail -f /dev/null

EOF

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="本文描述如何在项目现场补充缺失的离线镜像"><a class="header" href="#本文描述如何在项目现场补充缺失的离线镜像">本文描述如何在项目现场，补充缺失的离线镜像</a></h1>
<p>感谢 william shen， kevin lin 的帮助和提醒，大大简化了ocp4.3补充镜像的过程。</p>
<p>大致的流程是</p>
<ul>
<li>编辑 add.image.list 文件，把想要补充的镜像写进去，可以用#开始，代表注释，注意文件末尾加几个回车换行。</li>
<li>在外网主机，运行add.image.sh，会下载镜像到指定的目录，然后自行压缩成tgz</li>
<li>在内网工具机主机，上传压缩的tgz， 并解压缩</li>
<li>在内网工具机主机，cd /data/ocp4, 运行 add.image.load.sh， 加载镜像即可。</li>
</ul>
<pre><code class="language-bash"># 在外网云主机
# on vultr
# edit add.image.list

export MIRROR_DIR='/data/redhat-operator'
/bin/rm -rf ${MIRROR_DIR}

cd /data/ocp4
bash add.image.sh add.image.list ${MIRROR_DIR}
# bash add.image.sh is.openshift.list

# on 内网 工具机
# scp back /data/mirror_dir.tgz to /data/ocp4

bash add.image.load.sh /data/mirror_dir 'registry.redhat.ren:5443'
# bash add.image.load.sh /data/remote/4.3.3/is.samples/mirror_dir

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-补充-sample-镜像"><a class="header" href="#openshift-补充-sample-镜像">openshift 补充 sample 镜像</a></h1>
<p>openshift集群里面的openshift project，有很多的自带的image stream，这些image stream指向的是公网的镜像仓库地址，如果是离线环境，应该如何导入镜像，并更新image stream定义呢？</p>
<pre><code class="language-bash"># 导入镜像
# 解压缩is.sample.tgz  到 /data
pigz -dc is.samples.tgz | tar xf -
# 根据现场环境修改add.image.load.sh，并运行
bash add.image.load.sh /data/is.samples/mirror_dir/

# 修正image stream定义
# 根据现场环境，修改is.patch.sh
bash is.patch.sh

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-43-calico-离线部署"><a class="header" href="#openshift-43-calico-离线部署">openshift 4.3 calico 离线部署</a></h1>
<p>https://docs.projectcalico.org/getting-started/openshift/requirements</p>
<h2 id="image-prepare"><a class="header" href="#image-prepare">image prepare</a></h2>
<pre><code class="language-bash">
cd /data/ocp4

cat &lt;&lt; EOF &gt; add.image.list
quay.io/tigera/operator-init:v1.3.3
quay.io/tigera/operator:v1.3.3
docker.io/calico/ctl:v3.13.2
docker.io/calico/kube-controllers:v3.13.2
docker.io/calico/node:v3.13.2
docker.io/calico/typha:v3.13.2
docker.io/calico/pod2daemon-flexvol:v3.13.2
docker.io/calico/cni:v3.13.2
EOF

bash add.image.sh add.image.list

bash add.image.load.sh /data/down/mirror_dir

</code></pre>
<h2 id="install"><a class="header" href="#install">install</a></h2>
<pre><code class="language-bash">
# scp install-config.yaml into /root/ocp4
# sed -i 's/OpenShiftSDN/Calico/' install-config.yaml
openshift-install create manifests --dir=/root/ocp4
# scp calico/manifests to manifests
openshift-install create ignition-configs --dir=/root/ocp4

# follow 4.3.disconnect.operator.md to install

oc get tigerastatus

oc get pod -n tigera-operator

oc get pod -n calico-system

# 看看都用了什么image
oc project tigera-operator

oc get pod -o json | jq -r '.items[].spec.containers[].image' | sort | uniq
# quay.io/tigera/operator-init:v1.3.3
# quay.io/tigera/operator:v1.3.3

oc project calico-system

oc get pod -o json | jq -r '.items[].spec.containers[].image' | sort | uniq
# calico/ctl:v3.13.2
# docker.io/calico/kube-controllers:v3.13.2
# docker.io/calico/node:v3.13.2
# docker.io/calico/typha:v3.13.2

# docker.io/calico/pod2daemon-flexvol:v3.13.2
# docker.io/calico/cni:v3.13.2

# 安装控制命令行
oc apply -f calicoctl.yaml

oc exec calicoctl -n calico-system -it -- /calicoctl get node -o wide

oc exec calicoctl -n calico-system -it -- /calicoctl ipam show --show-blocks

oc exec calicoctl -n calico-system -it -- /calicoctl get ipPool -o wide

</code></pre>
<h2 id="calico-下创建-pod指定-ip-pool"><a class="header" href="#calico-下创建-pod指定-ip-pool">calico 下，创建 pod，指定 ip pool</a></h2>
<p>视频讲解</p>
<p><a href="https://www.bilibili.com/video/BV14Z4y1p7wa/"><kbd><img src="ocp4/4.3/imgs/2020-12-17-19-26-18.png" width="600"></kbd></a></p>
<ul>
<li>https://youtu.be/GJSFF7DDCe8</li>
<li>https://www.bilibili.com/video/BV14Z4y1p7wa/</li>
</ul>
<p>https://www.tigera.io/blog/calico-ipam-explained-and-enhanced/</p>
<pre><code class="language-bash"># 创建ip pool
cat &lt;&lt; EOF &gt; calico.ip.pool.yaml
---
apiVersion: projectcalico.org/v3
kind: IPPool
metadata:
  name: ip-pool-1
spec:
  cidr: 172.110.110.0/24
  ipipMode: Always
  natOutgoing: true
---
apiVersion: projectcalico.org/v3
kind: IPPool
metadata:
  name: ip-pool-2
spec:
  cidr: 172.110.220.0/24
  ipipMode: Always
  natOutgoing: true
EOF
cat calico.ip.pool.yaml | oc exec calicoctl -n calico-system -i -- /calicoctl apply -f -

# 检查ip pool的创建情况
oc exec calicoctl -n calico-system -it -- /calicoctl get ipPool -o wide

cat &lt;&lt; EOF &gt; calico.pod.yaml
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod1
  namespace: demo
  annotations:
    cni.projectcalico.org/ipv4pools: '[&quot;ip-pool-1&quot;]'
spec:
  nodeSelector:
    kubernetes.io/hostname: 'worker-0.ocp4.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;iperf3&quot;, &quot;-s&quot;, &quot;-p&quot; ]
      args: [ &quot;6666&quot; ]
      imagePullPolicy: Always
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod2
  namespace: demo
  annotations:
    cni.projectcalico.org/ipv4pools: '[&quot;ip-pool-1&quot;]'
spec:
  nodeSelector:
    kubernetes.io/hostname: 'worker-0.ocp4.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;iperf3&quot;, &quot;-s&quot;, &quot;-p&quot; ]
      args: [ &quot;6666&quot; ]
      imagePullPolicy: Always
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod3
  namespace: demo
  annotations:
    cni.projectcalico.org/ipv4pools: '[&quot;ip-pool-2&quot;]'
spec:
  nodeSelector:
    kubernetes.io/hostname: 'worker-0.ocp4.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;iperf3&quot;, &quot;-s&quot;, &quot;-p&quot; ]
      args: [ &quot;6666&quot; ]
      imagePullPolicy: Always
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod4
  namespace: demo
  annotations:
    cni.projectcalico.org/ipv4pools: '[&quot;ip-pool-1&quot;]'
spec:
  nodeSelector:
    kubernetes.io/hostname: 'worker-1.ocp4.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;iperf3&quot;, &quot;-s&quot;, &quot;-p&quot; ]
      args: [ &quot;6666&quot; ]
      imagePullPolicy: Always
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod5
  namespace: demo
  annotations:
    cni.projectcalico.org/ipv4pools: '[&quot;ip-pool-2&quot;]'
spec:
  nodeSelector:
    kubernetes.io/hostname: 'worker-1.ocp4.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;iperf3&quot;, &quot;-s&quot;, &quot;-p&quot; ]
      args: [ &quot;6666&quot; ]
      imagePullPolicy: Always
EOF
oc apply -f calico.pod.yaml

# 查看pod的IP分配，是按照我们指定的ip地址范围分配的
oc get pod -o wide -n demo
# [root@helper ocp4]# oc get pod -o wide -n demo
# NAME        READY   STATUS    RESTARTS   AGE     IP                NODE                       NOMINATED NODE   READINESS GATES
# demo-pod1   1/1     Running   0          8m52s   172.110.110.67    worker-0.ocp4.redhat.ren   &lt;none&gt;           &lt;none&gt;
# demo-pod2   1/1     Running   0          8m52s   172.110.110.68    worker-0.ocp4.redhat.ren   &lt;none&gt;           &lt;none&gt;
# demo-pod3   1/1     Running   0          8m52s   172.110.220.64    worker-0.ocp4.redhat.ren   &lt;none&gt;           &lt;none&gt;
# demo-pod4   1/1     Running   0          8m52s   172.110.110.128   worker-1.ocp4.redhat.ren   &lt;none&gt;           &lt;none&gt;
# demo-pod5   1/1     Running   0          8m52s   172.110.220.130   worker-1.ocp4.redhat.ren   &lt;none&gt;           &lt;none&gt;

# 获得除了demo-pod1以外的所有pod的ip地址
oc get pod -o json | jq -r '.items[] | select(.metadata.name != &quot;demo-pod1&quot;) | .status.podIP'

# 从demo-pod1上pind这些pod的ip地址，都能ping通。
for var_i in $(oc get pod -o json | jq -r '.items[] | select(.metadata.name != &quot;demo-pod1&quot;) | .status.podIP'); do
    oc exec -n demo demo-pod1 -it -- ping -c 5 ${var_i}
done

# clean up
oc delete -f calico.pod.yaml

cat calico.ip.pool.yaml | oc exec calicoctl -n calico-system -i -- /calicoctl delete -f -

</code></pre>
<h2 id="calico--multus"><a class="header" href="#calico--multus">calico + multus</a></h2>
<p>视频讲解</p>
<p><a href="https://www.bilibili.com/video/BV1zi4y147sk/"><kbd><img src="ocp4/4.3/imgs/2020-12-17-19-27-56.png" width="600"></kbd></a></p>
<ul>
<li>https://youtu.be/MQRv6UASZcA</li>
<li>https://www.bilibili.com/video/BV1zi4y147sk/</li>
<li>https://www.ixigua.com/i6825969911781655048/</li>
</ul>
<pre><code class="language-bash"># 创建multus macvlan需要的ip地址
cat &lt;&lt; EOF &gt; calico.macvlan.yaml
apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  additionalNetworks: 
  - name: multus-macvlan-0
    namespace: demo
    type: SimpleMacvlan
    simpleMacvlanConfig:
      ipamConfig:
        type: static
        staticIPAMConfig:
          addresses:
          - address: 10.123.110.11/24
          routes:
  - name: multus-macvlan-1
    namespace: demo
    type: SimpleMacvlan
    simpleMacvlanConfig:
      ipamConfig:
        type: static
        staticIPAMConfig:
          addresses:
          - address: 10.123.110.22/24

EOF
oc apply -f calico.macvlan.yaml

# 检查创建的ip地址
oc get Network.operator.openshift.io -o yaml

# 创建pod，并配置multus，使用macvlan
cat &lt;&lt; EOF &gt; calico.pod.yaml
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod1
  namespace: demo
  annotations:
    k8s.v1.cni.cncf.io/networks: '
      [{
        &quot;name&quot;: &quot;multus-macvlan-0&quot;
      }]'
spec:
  nodeSelector:
    kubernetes.io/hostname: 'worker-0.ocp4.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;iperf3&quot;, &quot;-s&quot;, &quot;-p&quot; ]
      args: [ &quot;6666&quot; ]
      imagePullPolicy: Always
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod2
  namespace: demo
  annotations:
    k8s.v1.cni.cncf.io/networks: '
      [{
        &quot;name&quot;: &quot;multus-macvlan-1&quot;
      }]'
spec:
  nodeSelector:
    kubernetes.io/hostname: 'worker-1.ocp4.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;iperf3&quot;, &quot;-s&quot;, &quot;-p&quot; ]
      args: [ &quot;6666&quot; ]
      imagePullPolicy: Always

EOF
oc apply -f calico.pod.yaml

# 查看demo-pod2上的ip地址
var_ips=$(oc get pod -o json | jq -r '.items[] | select(.metadata.name != &quot;demo-pod1&quot;) | .metadata.annotations[&quot;k8s.v1.cni.cncf.io/networks-status&quot;] | fromjson | .[].ips[0] ' )
echo -e &quot;$var_ips&quot;

# oc get pod -o json | jq -r ' .items[] | select(.metadata.name != &quot;demo-pod1&quot;) | { podname: .metadata.name, ip: ( .metadata.annotations[&quot;k8s.v1.cni.cncf.io/networks-status&quot;] | fromjson | .[].ips[0] ) } | [.podname, .ip] | @tsv'

# 从demo pod1上ping demo pod2上的2个ip地址
for var_i in $var_ips; do
  oc exec -n demo demo-pod1 -it -- ping -c 5 ${var_i}
done

# restore
oc delete -f calico.pod.yaml

cat &lt;&lt; EOF &gt; calico.macvlan.yaml
apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
EOF
oc apply -f calico.macvlan.yaml
</code></pre>
<h2 id="calico--static-ip"><a class="header" href="#calico--static-ip">calico + static ip</a></h2>
<p>https://docs.projectcalico.org/networking/use-specific-ip</p>
<p>视频讲解</p>
<p><a href="https://www.bilibili.com/video/BV1zz411q78i/"><kbd><img src="ocp4/4.3/imgs/2020-12-17-19-30-26.png" width="600"></kbd></a></p>
<ul>
<li>https://youtu.be/q8FtuOzBixA</li>
<li>https://www.bilibili.com/video/BV1zz411q78i/</li>
</ul>
<pre><code class="language-bash"># 创建测试用的静态ip deployment，和pod
cat &lt;&lt; EOF &gt; demo.yaml
---
kind: Deployment
apiVersion: apps/v1
metadata:
  annotations:
  name: demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo
  template:
    metadata:
      labels:
        app: demo  
      annotations:
        &quot;cni.projectcalico.org/ipAddrs&quot;: '[&quot;10.254.22.33&quot;]'
    spec:
      nodeSelector:
        # kubernetes.io/hostname: 'worker-1.ocp4.redhat.ren'
      restartPolicy: Always
      containers:
        - name: demo1
          image: &gt;- 
            registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
          env:
            - name: key
              value: value
          command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot; ]
          args: [ &quot;trap : TERM INT; sleep infinity &amp; wait&quot; ]
          imagePullPolicy: Always
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod1
  namespace: demo
spec:
  nodeSelector:
    kubernetes.io/hostname: 'worker-0.ocp4.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;iperf3&quot;, &quot;-s&quot;, &quot;-p&quot; ]
      args: [ &quot;6666&quot; ]
      imagePullPolicy: Always
EOF
oc apply -n demo -f demo.yaml

# 检查pod的ip地址
oc get pod -o wide
# NAME                    READY   STATUS    RESTARTS   AGE   IP              NODE                       NOMINATED NODE   READINESS GATES
# demo-8688cf4477-s26rs   1/1     Running   0          5s    10.254.22.33    worker-1.ocp4.redhat.ren   &lt;none&gt;           &lt;none&gt;
# demo-pod1               1/1     Running   0          6s    10.254.115.48   worker-0.ocp4.redhat.ren   &lt;none&gt;           &lt;none&gt;

# ping测试
oc exec -n demo demo-pod1 -it -- ping -c 5 10.254.22.33

# 移动pod到其他node
oc get pod -o wide

# ping测试
oc exec -n demo demo-pod1 -it -- ping -c 5 10.254.22.33

# clean up
oc delete -n demo -f demo.yaml

</code></pre>
<h2 id="calico--mtu"><a class="header" href="#calico--mtu">calico + mtu</a></h2>
<p>https://docs.projectcalico.org/networking/mtu</p>
<p>视频讲解</p>
<p><a href="https://www.bilibili.com/video/BV1Tk4y167Zs/"><kbd><img src="ocp4/4.3/imgs/2020-12-17-19-32-19.png" width="600"></kbd></a></p>
<ul>
<li>https://youtu.be/hTafoKlQiY0</li>
<li>https://www.bilibili.com/video/BV1Tk4y167Zs/</li>
</ul>
<pre><code class="language-bash"># 先检查一下已有的mtu
cat &lt;&lt; EOF &gt; demo.yaml
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod1
  namespace: demo
spec:
  nodeSelector:
    kubernetes.io/hostname: 'worker-0.ocp4.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;iperf3&quot;, &quot;-s&quot;, &quot;-p&quot; ]
      args: [ &quot;6666&quot; ]
      imagePullPolicy: Always
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod2
  namespace: demo
spec:
  nodeSelector:
    kubernetes.io/hostname: 'worker-1.ocp4.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;iperf3&quot;, &quot;-s&quot;, &quot;-p&quot; ]
      args: [ &quot;6666&quot; ]
      imagePullPolicy: Always
EOF
oc apply -n demo -f demo.yaml

# 检查 mtu，现在tunl上是1480，eth0上是1410
oc exec -it demo-pod1 -- ip a
# 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
#     link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
#     inet 127.0.0.1/8 scope host lo
#        valid_lft forever preferred_lft forever
#     inet6 ::1/128 scope host
#        valid_lft forever preferred_lft forever
# 2: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000
#     link/ipip 0.0.0.0 brd 0.0.0.0
# 4: eth0@if54: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1410 qdisc noqueue state UP group default
#     link/ether c2:e9:6a:c8:62:77 brd ff:ff:ff:ff:ff:ff link-netnsid 0
#     inet 10.254.115.50/32 scope global eth0
#        valid_lft forever preferred_lft forever
#     inet6 fe80::c0e9:6aff:fec8:6277/64 scope link
#        valid_lft forever preferred_lft forever

# 把mtu 从1410改成700
oc get installations.operator.tigera.io -o yaml

oc edit installations.operator.tigera.io
# spec:
#   calicoNetwork:
#     mtu: 700

# 重启calico node pod
# oc delete deploy calico-kube-controllers -n calico-system
# oc delete deploy calico-typha -n calico-system
# oc delete ds calico-node -n calico-system
oc delete -n demo -f demo.yaml

# 重启worker node

# 重新创建pod
# oc apply -n demo -f demo.yaml

# 查看mtu
oc exec -i demo-pod1 -- ip a
oc exec -i demo-pod2 -- ip a

# 各种ping测试
var_ip=$(oc get pod -o json | jq -r '.items[] | select(.metadata.name == &quot;demo-pod1&quot;) | .status.podIP')
echo $var_ip
# ICMP+IP 的包头有 28 bytes
#  the IP stack of your system adds ICMP and IP headers which equals to 28 bytes
oc exec -i demo-pod2 -- ping -M do -s $((600-28)) -c 5 $var_ip
oc exec -i demo-pod2 -- ping -M do -s $((700-28)) -c 5 $var_ip
oc exec -i demo-pod2 -- ping -M do -s $((800-28)) -c 5 $var_ip

# 把mtu从700恢复成1410
oc edit installations.operator.tigera.io
# spec:
#   calicoNetwork:

oc get installations.operator.tigera.io -o yaml

# 重启calico node pod
# oc delete deploy calico-kube-controllers -n calico-system
# oc delete deploy calico-typha -n calico-system
# oc delete ds calico-node -n calico-system
oc delete -n demo -f demo.yaml

# 重启worker node

# 重新创建pod
# oc apply -n demo -f demo.yaml

# 查看mtu
oc exec -i demo-pod1 -- ip a
oc exec -i demo-pod2 -- ip a

# 各种ping测试
var_ip=$(oc get pod -o json | jq -r '.items[] | select(.metadata.name == &quot;demo-pod1&quot;) | .status.podIP')
echo $var_ip
# ICMP+IP 的包头有 28 bytes
#  the IP stack of your system adds ICMP and IP headers which equals to 28 bytes
oc exec -i demo-pod2 -- ping -M do -s $((600-28)) -c 5 $var_ip
oc exec -i demo-pod2 -- ping -M do -s $((700-28)) -c 5 $var_ip
oc exec -i demo-pod2 -- ping -M do -s $((800-28)) -c 5 $var_ip

# restore
oc delete -n demo -f demo.yaml

</code></pre>
<h2 id="calico--ipv4v6-dual-stack"><a class="header" href="#calico--ipv4v6-dual-stack">calico + ipv4/v6 dual stack</a></h2>
<p>视频讲解</p>
<p><a href="https://www.bilibili.com/video/BV1va4y1e7c1/"><kbd><img src="ocp4/4.3/imgs/2020-12-17-19-33-34.png" width="600"></kbd></a></p>
<ul>
<li>https://youtu.be/ju4d7jWs7DQ</li>
<li>https://www.bilibili.com/video/BV1va4y1e7c1/</li>
<li>https://www.ixigua.com/i6827830624431112715/</li>
</ul>
<pre><code class="language-bash"># 在集群安装之前，配置文件写入ipv6地址信息
# install openshift with calico and ipv6 config
# networking:
#   clusterNetworks:
#   - cidr: 10.254.0.0/16
#     hostPrefix: 24
#   - cidr: fd00:192:168:7::/64
#     hostPrefix: 80

# 在安装集群的过程中，给主机添加ipv6地址，安装就可以顺利继续了
## add ipv6 address to hosts
# helper
nmcli con modify eth0 ipv6.address &quot;fd00:192:168:7::11/64&quot; ipv6.gateway fd00:192:168:7::1
nmcli con modify eth0 ipv6.method manual
nmcli con reload
nmcli con up eth0

# master0
nmcli con modify ens3 ipv6.address fd00:192:168:7::13/64 ipv6.gateway fd00:192:168:7::1 ipv6.method manual
nmcli con reload
nmcli con up ens3

# master1
nmcli con modify ens3 ipv6.address fd00:192:168:7::14/64 ipv6.gateway fd00:192:168:7::1 ipv6.method manual
nmcli con reload
nmcli con up ens3

# master2
nmcli con modify ens3 ipv6.address fd00:192:168:7::15/64 ipv6.gateway fd00:192:168:7::1 ipv6.method manual
nmcli con reload
nmcli con up ens3

# worker0
nmcli con modify ens3 ipv6.address fd00:192:168:7::16/64 ipv6.gateway fd00:192:168:7::1 ipv6.method manual
nmcli con reload
nmcli con up ens3

# worker1
nmcli con modify ens3 ipv6.address fd00:192:168:7::17/64 ipv6.gateway fd00:192:168:7::1 ipv6.method manual
nmcli con reload
nmcli con up ens3

oc apply -f calicoctl.yaml

oc exec calicoctl -n calico-system -it -- /calicoctl get node -o wide

oc exec calicoctl -n calico-system -it -- /calicoctl ipam show --show-blocks

oc exec calicoctl -n calico-system -it -- /calicoctl get ipPool -o wide

# 在openshift的开发者视图上部署一个tomcat

# 从浏览器上，直接访问route入口，测试ipv4的效果。

# 在master0上直接访问worker1上的pod ipv6地址
curl -g -6 'http://[fd00:192:168:7:697b:8c59:3298:b950]:8080/'

# 在集群外，直接访问worker0上的pod ipv6地址
ip -6 route add fd00:192:168:7:697b:8c59:3298::/112 via fd00:192:168:7::17 dev eth0
curl -g -6 'http://[fd00:192:168:7:697b:8c59:3298:b950]:8080/'

</code></pre>
<h2 id="calico--bgp"><a class="header" href="#calico--bgp">calico + bgp</a></h2>
<pre><code class="language-bash">
cat &lt;&lt; EOF &gt; calico.serviceip.yaml
apiVersion: projectcalico.org/v3
kind: BGPConfiguration
metadata:
  name: default
spec:
  serviceClusterIPs:
  - cidr: 10.96.0.0/16
EOF
cat calico.serviceip.yaml | oc exec calicoctl -n calico-system -i -- /calicoctl apply -f -

oc exec calicoctl -n calico-system -i -- /calicoctl patch bgpconfiguration default -p '{&quot;spec&quot;: {&quot;nodeToNodeMeshEnabled&quot;: true}}'

oc exec calicoctl -n calico-system -it -- /calicoctl get bgpconfig default -o yaml

oc exec calicoctl -n calico-system -it -- /calicoctl get node -o wide

oc exec calicoctl -n calico-system -it -- /calicoctl ipam show --show-blocks

oc exec calicoctl -n calico-system -it -- /calicoctl get ipPool -o wide


oc exec calicoctl -n calico-system -it -- /calicoctl get workloadEndpoint

oc exec calicoctl -n calico-system -it -- /calicoctl get BGPPeer

cat &lt;&lt; EOF &gt; calico.bgp.yaml
---
apiVersion: projectcalico.org/v3
kind: BGPPeer
metadata:
  name: my-global-peer
spec:
  peerIP: 192.168.7.11
  asNumber: 64513
EOF
cat calico.bgp.yaml | oc exec calicoctl -n calico-system -i -- /calicoctl apply -f -

# on helper
# https://www.vultr.com/docs/configuring-bgp-using-quagga-on-vultr-centos-7
yum install quagga
systemctl start zebra
systemctl start bgpd
cp /usr/share/doc/quagga-*/bgpd.conf.sample /etc/quagga/bgpd.conf
vtysh
show running-config
configure terminal
no router bgp 7675
router bgp 64513
no auto-summary
no synchronization
neighbor 192.168.7.13 remote-as 64512
neighbor 192.168.7.13 description &quot;calico&quot;
neighbor 192.168.7.13 attribute-unchanged next-hop
neighbor 192.168.7.13 ebgp-multihop 255
neighbor 192.168.7.13 next-hop-self
# no neighbor 192.168.7.13 next-hop-self
neighbor 192.168.7.13 activate
interface eth0
exit
exit
write
show running-config
show ip bgp summary

# 测试一下
cat &lt;&lt; EOF &gt; calico.ip.pool.yaml
---
apiVersion: projectcalico.org/v3
kind: IPPool
metadata:
  name: ip-pool-1
spec:
  cidr: 172.110.110.0/24
  ipipMode: Always
  natOutgoing: false
---
apiVersion: projectcalico.org/v3
kind: IPPool
metadata:
  name: ip-pool-2
spec:
  cidr: 172.110.220.0/24
  ipipMode: Always
  natOutgoing: false
EOF
cat calico.ip.pool.yaml | oc exec calicoctl -n calico-system -i -- /calicoctl apply -f -

oc exec calicoctl -n calico-system -it -- /calicoctl get ipPool -o wide

cat &lt;&lt; EOF &gt; calico.pod.yaml
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod1
  namespace: demo
  annotations:
    cni.projectcalico.org/ipv4pools: '[&quot;ip-pool-1&quot;]'
spec:
  nodeSelector:
    kubernetes.io/hostname: 'worker-0.ocp4.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;iperf3&quot;, &quot;-s&quot;, &quot;-p&quot; ]
      args: [ &quot;6666&quot; ]
      imagePullPolicy: Always
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod2
  namespace: demo
  annotations:
    cni.projectcalico.org/ipv4pools: '[&quot;ip-pool-1&quot;]'
spec:
  nodeSelector:
    kubernetes.io/hostname: 'worker-0.ocp4.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;iperf3&quot;, &quot;-s&quot;, &quot;-p&quot; ]
      args: [ &quot;6666&quot; ]
      imagePullPolicy: Always
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod3
  namespace: demo
  annotations:
    cni.projectcalico.org/ipv4pools: '[&quot;ip-pool-2&quot;]'
spec:
  nodeSelector:
    kubernetes.io/hostname: 'worker-0.ocp4.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;iperf3&quot;, &quot;-s&quot;, &quot;-p&quot; ]
      args: [ &quot;6666&quot; ]
      imagePullPolicy: Always
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod4
  namespace: demo
  annotations:
    cni.projectcalico.org/ipv4pools: '[&quot;ip-pool-1&quot;]'
spec:
  nodeSelector:
    kubernetes.io/hostname: 'worker-1.ocp4.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;iperf3&quot;, &quot;-s&quot;, &quot;-p&quot; ]
      args: [ &quot;6666&quot; ]
      imagePullPolicy: Always
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod5
  namespace: demo
  annotations:
    cni.projectcalico.org/ipv4pools: '[&quot;ip-pool-2&quot;]'
spec:
  nodeSelector:
    kubernetes.io/hostname: 'worker-1.ocp4.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;iperf3&quot;, &quot;-s&quot;, &quot;-p&quot; ]
      args: [ &quot;6666&quot; ]
      imagePullPolicy: Always
EOF
oc apply -f calico.pod.yaml

</code></pre>
<h2 id="run-caliconode-with-backendnone"><a class="header" href="#run-caliconode-with-backendnone">run calico/node with —backend=none</a></h2>
<p>CALICO_NETWORKING_BACKEND none</p>
<p>https://docs.projectcalico.org/reference/node/configuration</p>
<p><img src="ocp4/4.3/imgs/2020-11-16-22-50-39.png" alt="" /></p>
<h2 id="backups"><a class="header" href="#backups">backups</a></h2>
<pre><code class="language-bash">
skopeo copy docker://quay.io/tigera/operator-init:v1.3.3 docker://registry.redhat.ren:5443/tigera/operator-init:v1.3.3
skopeo copy docker://quay.io/tigera/operator:v1.3.3 docker://registry.redhat.ren:5443/tigera/operator:v1.3.3

skopeo copy docker://docker.io/calico/ctl:v3.13.2 docker://registry.redhat.ren:5443/calico/ctl:v3.13.2
skopeo copy docker://docker.io/calico/kube-controllers:v3.13.2 docker://registry.redhat.ren:5443/calico/kube-controllers:v3.13.2
skopeo copy docker://docker.io/calico/node:v3.13.2 docker://registry.redhat.ren:5443/calico/node:v3.13.2
skopeo copy docker://docker.io/calico/typha:v3.13.2 docker://registry.redhat.ren:5443/calico/typha:v3.13.2
skopeo copy docker://docker.io/calico/pod2daemon-flexvol:v3.13.2 docker://registry.redhat.ren:5443/calico/pod2daemon-flexvol:v3.13.2
skopeo copy docker://docker.io/calico/cni:v3.13.2 docker://registry.redhat.ren:5443/calico/cni:v3.13.2

curl https://docs.projectcalico.org/manifests/ocp/crds/01-crd-installation.yaml -o manifests/01-crd-installation.yaml
curl https://docs.projectcalico.org/manifests/ocp/crds/01-crd-tigerastatus.yaml -o manifests/01-crd-tigerastatus.yaml
curl https://docs.projectcalico.org/manifests/ocp/crds/calico/kdd/02-crd-bgpconfiguration.yaml -o manifests/02-crd-bgpconfiguration.yaml
curl https://docs.projectcalico.org/manifests/ocp/crds/calico/kdd/02-crd-bgppeer.yaml -o manifests/02-crd-bgppeer.yaml
curl https://docs.projectcalico.org/manifests/ocp/crds/calico/kdd/02-crd-blockaffinity.yaml -o manifests/02-crd-blockaffinity.yaml
curl https://docs.projectcalico.org/manifests/ocp/crds/calico/kdd/02-crd-clusterinformation.yaml -o manifests/02-crd-clusterinformation.yaml
curl https://docs.projectcalico.org/manifests/ocp/crds/calico/kdd/02-crd-felixconfiguration.yaml -o manifests/02-crd-felixconfiguration.yaml
curl https://docs.projectcalico.org/manifests/ocp/crds/calico/kdd/02-crd-globalnetworkpolicy.yaml -o manifests/02-crd-globalnetworkpolicy.yaml
curl https://docs.projectcalico.org/manifests/ocp/crds/calico/kdd/02-crd-globalnetworkset.yaml -o manifests/02-crd-globalnetworkset.yaml
curl https://docs.projectcalico.org/manifests/ocp/crds/calico/kdd/02-crd-hostendpoint.yaml -o manifests/02-crd-hostendpoint.yaml
curl https://docs.projectcalico.org/manifests/ocp/crds/calico/kdd/02-crd-ipamblock.yaml -o manifests/02-crd-ipamblock.yaml
curl https://docs.projectcalico.org/manifests/ocp/crds/calico/kdd/02-crd-ipamconfig.yaml -o manifests/02-crd-ipamconfig.yaml
curl https://docs.projectcalico.org/manifests/ocp/crds/calico/kdd/02-crd-ipamhandle.yaml -o manifests/02-crd-ipamhandle.yaml
curl https://docs.projectcalico.org/manifests/ocp/crds/calico/kdd/02-crd-ippool.yaml -o manifests/02-crd-ippool.yaml
curl https://docs.projectcalico.org/manifests/ocp/crds/calico/kdd/02-crd-networkpolicy.yaml -o manifests/02-crd-networkpolicy.yaml
curl https://docs.projectcalico.org/manifests/ocp/crds/calico/kdd/02-crd-networkset.yaml -o manifests/02-crd-networkset.yaml
curl https://docs.projectcalico.org/manifests/ocp/tigera-operator/00-namespace-tigera-operator.yaml -o manifests/00-namespace-tigera-operator.yaml
curl https://docs.projectcalico.org/manifests/ocp/tigera-operator/02-rolebinding-tigera-operator.yaml -o manifests/02-rolebinding-tigera-operator.yaml
curl https://docs.projectcalico.org/manifests/ocp/tigera-operator/02-role-tigera-operator.yaml -o manifests/02-role-tigera-operator.yaml
curl https://docs.projectcalico.org/manifests/ocp/tigera-operator/02-serviceaccount-tigera-operator.yaml -o manifests/02-serviceaccount-tigera-operator.yaml
curl https://docs.projectcalico.org/manifests/ocp/tigera-operator/02-configmap-calico-resources.yaml -o manifests/02-configmap-calico-resources.yaml
curl https://docs.projectcalico.org/manifests/ocp/tigera-operator/02-configmap-tigera-install-script.yaml -o manifests/02-configmap-tigera-install-script.yaml
curl https://docs.projectcalico.org/manifests/ocp/tigera-operator/02-tigera-operator.yaml -o manifests/02-tigera-operator.yaml
curl https://docs.projectcalico.org/manifests/ocp/01-cr-installation.yaml -o manifests/01-cr-installation.yaml

curl https://docs.projectcalico.org/manifests/calicoctl.yaml -o manifests/calicoctl.yaml

oc get Network.operator.openshift.io -o yaml
  # defaultNetwork:
  #   calicoSDNConfig:
  #     mtu: 700
  #   openshiftSDNConfig:
  #     mtu: 700
oc api-resources | grep -i calico
oc api-resources | grep -i tigera

oc get FelixConfiguration -o yaml

oc exec calicoctl -n calico-system -it -- /calicoctl get bgpconfig default

cat &lt;&lt; EOF &gt; calico.serviceip.yaml
apiVersion: projectcalico.org/v3
kind: BGPConfiguration
metadata:
  name: default
spec:
  serviceClusterIPs:
  - cidr: 10.96.0.0/16
  - cidr: fd00:192:168:7:1:1::/112
EOF
cat calico.serviceip.yaml | oc exec calicoctl -n calico-system -i -- /calicoctl apply -f -

oc exec calicoctl -n calico-system -it -- /calicoctl get workloadEndpoint
oc exec calicoctl -n calico-system -it -- /calicoctl get BGPPeer

cat &lt;&lt; EOF &gt; calico.bgp.yaml
---
apiVersion: projectcalico.org/v3
kind: BGPPeer
metadata:
  name: my-global-peer
spec:
  peerIP: 192.168.7.11
  asNumber: 64513
---
apiVersion: projectcalico.org/v3
kind: BGPPeer
metadata:
  name: my-global-peer-v6
spec:
  peerIP: fd00:192:168:7::11
  asNumber: 64513
EOF
cat calico.bgp.yaml | oc exec calicoctl -n calico-system -i -- /calicoctl apply -f -

# on helper
# https://www.vultr.com/docs/configuring-bgp-using-quagga-on-vultr-centos-7
yum install quagga
systemctl start zebra
systemctl start bgpd
cp /usr/share/doc/quagga-*/bgpd.conf.sample /etc/quagga/bgpd.conf
vtysh
show running-config
configure terminal
no router bgp 7675
router bgp 64513
no auto-summary
no synchronization
neighbor 192.168.7.13 remote-as 64512
neighbor 192.168.7.13 description &quot;calico&quot;
neighbor fd00:192:168:7::13 remote-as 64512
neighbor fd00:192:168:7::13 description &quot;calico&quot;
interface eth0
?? no ipv6 nd suppress-ra
exit
exit
write
show running-config
show ip bgp summary

# https://access.redhat.com/documentation/en-us/openshift_container_platform/4.3/html/networking/cluster-network-operator

oc get Network.operator.openshift.io -o yaml
oc edit Network.operator.openshift.io cluster
  # - cidr: fd01:192:168:7:11:/64
  #   hostPrefix: 80

oc get network.config/cluster
oc edit network.config/cluster

oc get installations.operator.tigera.io -o yaml
oc edit installations.operator.tigera.io
    # nodeAddressAutodetectionV6:
    #   firstFound: true
    - blockSize: 122
      cidr: fd01:192:168:7:11:/80
      encapsulation: None
      natOutgoing: Disabled
      nodeSelector: all()

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift4-集群升级"><a class="header" href="#openshift4-集群升级">openshift4 集群升级</a></h1>
<p>4.2的集群升级很简单，更新一下镜像仓库，然后运行一个命令，等着就好了。</p>
<pre><code class="language-bash"># on base host
cat &lt;&lt; EOF &gt; /etc/docker-distribution/registry/config.yml
version: 0.1
log:
  fields:
    service: registry
storage:
    cache:
        layerinfo: inmemory
    filesystem:
        rootdirectory: /data/4.2.7/registry
    delete:
        enabled: true
http:
    addr: :443
    tls:
       certificate: /etc/crts/redhat.ren.crt
       key: /etc/crts/redhat.ren.key
EOF

systemctl restart docker-distribution

# on helper node
# oc patch OperatorHub cluster --type json  -p '[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/disableAllDefaultSources&quot;, &quot;value&quot;: true}]'

oc patch OperatorHub cluster --type json  -p '[{&quot;op&quot;: &quot;remove&quot;, &quot;path&quot;: &quot;/spec/disableAllDefaultSources&quot;}]'

oc patch -n openshift-cluster-samples-operator  configs.samples.operator.openshift.io cluster -p '{&quot;items[0]&quot;:{&quot;spec&quot;:{&quot;managementState&quot;:&quot;Removed&quot;}}}'  --type=merge

oc adm upgrade --allow-explicit-upgrade --allow-upgrade-with-warnings=true --force=true --to-image=registry.redhat.ren/ocp4/openshift4:4.2.7 

</code></pre>
<p><img src="ocp4/4.2/imgs/2019-11-27-13-01-13.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift4-缩小---sysroot-分区大小"><a class="header" href="#openshift4-缩小---sysroot-分区大小">openshift4 缩小 / &amp; sysroot 分区大小</a></h1>
<p>openshift4默认安装的时候，会把sda/vda整个硬盘占满，如果我们是baremetal按照，一般会配置SSD/NVME, 1T大小，这样非常浪费。我们完全可以把硬盘空间节省下来，分一些分区，给local storage operator用。</p>
<p>视频讲解:</p>
<p><a href="https://www.bilibili.com/video/BV1UL4y1Y7H7/"><kbd><img src="ocp4/4.8/imgs/2021-09-07-13-47-13.png" width="600"></kbd></a></p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1UL4y1Y7H7/">bilibili</a></li>
<li><a href="https://youtu.be/0n_u0TlsniY">youtube</a></li>
</ul>
<pre><code class="language-bash"># backup the ignition file you want
/bin/cp -f /var/www/html/ignition/worker-1.ign /var/www/html/ignition/worker-1.ign.bak

# 修改 /data/ocp4/partition.sh ，
# 主要是修改里面的root分区大小，默认是200G
# 然后是想要创建的数据分区的个数和大小参数，
# 默认会创建5个10G分区，5个5G分区。
bash /data/ocp4/partition.sh

butane /data/ocp4/root-partition.bu -r -o /data/install/partition-ric.ign

/bin/cp -f /var/www/html/ignition/worker-1.ign.bak /var/www/html/ignition/worker-1.ign

# merge the 2 ignition files
jq -s '.[0] * .[1]' /var/www/html/ignition/worker-1.ign /data/install/partition-ric.ign | jq -c . &gt; /var/www/html/ignition/worker-1.ign.new

/bin/cp -f /var/www/html/ignition/worker-1.ign.new /var/www/html/ignition/worker-1.ign

# then install using iso

# login to worker-1
lsblk
# NAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
# sr0      11:0    1 1024M  0 rom
# vda     252:0    0    1T  0 disk
# ├─vda1  252:1    0    1M  0 part
# ├─vda2  252:2    0  127M  0 part
# ├─vda3  252:3    0  384M  0 part /boot
# ├─vda4  252:4    0  200G  0 part /sysroot
# ├─vda5  252:5    0   10G  0 part
# ├─vda6  252:6    0   10G  0 part
# ├─vda7  252:7    0   10G  0 part
# ├─vda8  252:8    0   10G  0 part
# ├─vda9  252:9    0   10G  0 part
# ├─vda10 252:10   0    5G  0 part
# ├─vda11 252:11   0    5G  0 part
# ├─vda12 252:12   0    5G  0 part
# ├─vda13 252:13   0    5G  0 part
# └─vda14 252:14   0    5G  0 part /var/lib/kubelet/pods/a364c83a-deae-4431-b7c3-bcef8457aed6/volumes/kubernetes.io~local-volume/local-pv-9fa7f

# let's check what we created
cat /data/ocp4/root-partition.bu
# variant: openshift
# version: 4.8.0
# metadata:
#   name: root-storage
#   labels:
#     machineconfiguration.openshift.io/role: worker
# storage:
#   disks:
#     - device: /dev/vda
#       wipe_table: false
#       partitions:
#         - number: 4
#           label: root
#           size_mib: 204800
#           resize: true
#         - label: data_10G_1
#           size_mib: 10240
#         - label: data_10G_2
#           size_mib: 10240
#         - label: data_10G_3
#           size_mib: 10240
#         - label: data_10G_4
#           size_mib: 10240
#         - label: data_10G_5
#           size_mib: 10240
#         - label: data_5G_1
#           size_mib: 5120
#         - label: data_5G_2
#           size_mib: 5120
#         - label: data_5G_3
#           size_mib: 5120
#         - label: data_5G_4
#           size_mib: 5120
#         - label: data_5G_5
#           size_mib: 5120

cat /data/install/partition-ric.ign | jq .
# {
#   &quot;ignition&quot;: {
#     &quot;version&quot;: &quot;3.2.0&quot;
#   },
#   &quot;storage&quot;: {
#     &quot;disks&quot;: [
#       {
#         &quot;device&quot;: &quot;/dev/vda&quot;,
#         &quot;partitions&quot;: [
#           {
#             &quot;label&quot;: &quot;root&quot;,
#             &quot;number&quot;: 4,
#             &quot;resize&quot;: true,
#             &quot;sizeMiB&quot;: 204800
#           },
#           {
#             &quot;label&quot;: &quot;data_10G_1&quot;,
#             &quot;sizeMiB&quot;: 10240
#           },
#           {
#             &quot;label&quot;: &quot;data_10G_2&quot;,
#             &quot;sizeMiB&quot;: 10240
#           },
#           {
#             &quot;label&quot;: &quot;data_10G_3&quot;,
#             &quot;sizeMiB&quot;: 10240
#           },
#           {
#             &quot;label&quot;: &quot;data_10G_4&quot;,
#             &quot;sizeMiB&quot;: 10240
#           },
#           {
#             &quot;label&quot;: &quot;data_10G_5&quot;,
#             &quot;sizeMiB&quot;: 10240
#           },
#           {
#             &quot;label&quot;: &quot;data_5G_1&quot;,
#             &quot;sizeMiB&quot;: 5120
#           },
#           {
#             &quot;label&quot;: &quot;data_5G_2&quot;,
#             &quot;sizeMiB&quot;: 5120
#           },
#           {
#             &quot;label&quot;: &quot;data_5G_3&quot;,
#             &quot;sizeMiB&quot;: 5120
#           },
#           {
#             &quot;label&quot;: &quot;data_5G_4&quot;,
#             &quot;sizeMiB&quot;: 5120
#           },
#           {
#             &quot;label&quot;: &quot;data_5G_5&quot;,
#             &quot;sizeMiB&quot;: 5120
#           }
#         ],
#         &quot;wipeTable&quot;: false
#       }
#     ]
#   }
# }
</code></pre>
<h1 id="local-storage-operator"><a class="header" href="#local-storage-operator">local storage operator</a></h1>
<p>我们有了很多分区，那么赶快来测试一下如何把他们变成 PV 吧</p>
<pre><code class="language-yaml">apiVersion: &quot;local.storage.openshift.io/v1&quot;
kind: &quot;LocalVolume&quot;
metadata:
  name: &quot;local-disks&quot;
  namespace: &quot;openshift-local-storage&quot; 
spec:
  nodeSelector: 
    nodeSelectorTerms:
    - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - worker-1
  storageClassDevices:
    - storageClassName: &quot;local-sc&quot; 
      volumeMode: Filesystem 
      fsType: xfs 
      devicePaths: 
        - /dev/vda5
        - /dev/vda14
</code></pre>
<p>我们可以看到配置已经生效
<img src="ocp4/4.8/imgs/2021-09-06-19-34-52.png" alt="" />
系统已经帮我们创建好了PV
<img src="ocp4/4.8/imgs/2021-09-06-19-49-49.png" alt="" /></p>
<p>我们创建pod，创建和使用pvc，然后弄点数据，然后删掉pod，删掉pvc。然后重新创建pod，创建和使用pvc，看看里面的数据是否会清空。</p>
<pre><code class="language-bash">cat &lt;&lt; EOF &gt;&gt; /data/install/pvc-demo.yaml
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: local-pvc-demo
spec:
  accessModes:
  - ReadWriteOnce
  volumeMode: Filesystem 
  resources:
    requests:
      storage: 2Gi 
  storageClassName: local-sc 
---
kind: Pod
apiVersion: v1
metadata:
  annotations:
  name: demo1
spec:
  nodeSelector:
    kubernetes.io/hostname: 'worker-1'
  restartPolicy: Always
  containers:
    - name: demo1
      image: &gt;- 
        quay.io/wangzheng422/qimgs:centos7-test
      env:
        - name: key
          value: value
      command: 
        - sleep
        - infinity
      imagePullPolicy: Always
      volumeMounts:
        - mountPath: /data
          name: demo 
          readOnly: false
  volumes:
    - name: demo 
      persistentVolumeClaim:
        claimName: local-pvc-demo 
EOF
oc create -n default -f /data/install/pvc-demo.yaml

</code></pre>
<p>我们能看到 PVC 已经创建
<img src="ocp4/4.8/imgs/2021-09-06-21-38-08.png" alt="" />
PV 也已经挂载
<img src="ocp4/4.8/imgs/2021-09-06-21-38-27.png" alt="" /></p>
<pre><code class="language-bash">oc rsh pod/demo1
df -h
# Filesystem      Size  Used Avail Use% Mounted on
# overlay         200G  8.4G  192G   5% /
# tmpfs            64M     0   64M   0% /dev
# tmpfs            24G     0   24G   0% /sys/fs/cgroup
# shm              64M     0   64M   0% /dev/shm
# tmpfs            24G   64M   24G   1% /etc/hostname
# /dev/vda14      5.0G   68M  5.0G   2% /data
# /dev/vda4       200G  8.4G  192G   5% /etc/hosts
# tmpfs            24G   20K   24G   1% /run/secrets/kubernetes.io/serviceaccount
# tmpfs            24G     0   24G   0% /proc/acpi
# tmpfs            24G     0   24G   0% /proc/scsi
# tmpfs            24G     0   24G   0% /sys/firmware

echo wzh &gt; /data/1
cat /data/1
# wzh

# destroy the pvc and pod
oc delete -n default -f /data/install/pvc-demo.yaml

# recreate 
oc create -n default -f /data/install/pvc-demo.yaml

</code></pre>
<p>PVC重新创建了
<img src="ocp4/4.8/imgs/2021-09-06-21-54-31.png" alt="" />
PV也重新挂在了
<img src="ocp4/4.8/imgs/2021-09-06-21-54-48.png" alt="" /></p>
<p>我们发现，PV release以后，重新挂载，之前的存储内容，就都没有了。</p>
<pre><code class="language-bash">oc rsh pod/demo1
sh-4.2# cd /data
sh-4.2# ls
sh-4.2# ls -hl
total 0

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift4-离线升级服务--disconnected-update-service"><a class="header" href="#openshift4-离线升级服务--disconnected-update-service">openshift4 离线升级服务 / disconnected update service</a></h1>
<p>openshift4默认的集群管理界面，会向公网的升级服务请求升级信息，如果在离线安装的情况，这个升级信息是拿不到的，于是集群的管理界面就会一堆报错，很难看。现在openshift4有一个update server operator，这个可以在集群内部创建一个离线的update server，提供升级信息，这样集群的管理界面就不会那么难看啦。</p>
<p>本次实验的部署架构：</p>
<p><img src="ocp4/4.8/dia/4.8.update.3node.drawio.svg" alt="" /></p>
<p>视频讲解:</p>
<p><a href="https://www.bilibili.com/video/BV1av411w7zT/"><kbd><img src="ocp4/4.8/imgs/2021-09-08-21-29-52.png" width="600"></kbd></a></p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1av411w7zT/">bilibili</a></li>
<li><a href="https://youtu.be/hrFuHNASvjM">youtube</a></li>
</ul>
<p>based on: </p>
<ul>
<li>https://www.openshift.com/blog/openshift-update-service-update-manager-for-your-cluster</li>
<li>https://docs.openshift.com/container-platform/4.8/updating/installing-update-service.html</li>
</ul>
<p>离线安装以后，不配置的话，系统管理页面是这个鬼样子：</p>
<p><img src="ocp4/4.8/imgs/2021-09-08-15-23-38.png" alt="" /></p>
<pre><code class="language-bash"># search OpenShift Update Service in operator hub, and install

# build a update container
mkdir -p /data/update
cd /data/update
cat &lt;&lt; EOF &gt; /data/update/Dockerfile
FROM registry.access.redhat.com/ubi8

RUN curl -L -o cincinnati-graph-data.tar.gz https://github.com/openshift/cincinnati-graph-data/archive/master.tar.gz

CMD exec /bin/bash -c &quot;tar xvzf cincinnati-graph-data.tar.gz -C /var/lib/cincinnati/graph-data/ --strip-components=1&quot;
EOF

var_date=$(date '+%Y-%m-%d-%H%M')
echo $var_date

buildah bud -f ./Dockerfile -t quay.io/wangzheng422/graph-data-image:$var_date
podman push quay.io/wangzheng422/graph-data-image:$var_date

echo quay.io/wangzheng422/graph-data-image:$var_date
# quay.io/wangzheng422/graph-data-image:2021-09-07-0709

cat &lt;&lt; EOF &gt; /data/install/update.yaml
apiVersion: updateservice.operator.openshift.io/v1
kind: UpdateService
metadata:
  namespace: openshift-update-service
  name: sample
spec:
  graphDataImage: 'nexus.ocp4.redhat.ren:8083/wangzheng422/graph-data-image:2021-09-07-0709'
  releases: 'registry.ocp4.redhat.ren:5443/ocp4/release'
  replicas: 1
EOF
oc create -f /data/install/update.yaml

# to restore
oc delete -f /data/install/update.yaml

# 部署完了update service 以后，发现报错
# 发现update service operator依赖有password的registry
# 我们之前默认安装的registry是没有密码的，就不行
# 所以重新部署一个需要密码认证的registry就可以了。

oc get secret/pull-secret -n openshift-config -o json | jq '.data.&quot;.dockerconfigjson&quot;' | jq -r . | base64 -d | jq .
# {
#   &quot;auths&quot;: {
#     &quot;registry.ocp4.redhat.ren:5443&quot;: {
#       &quot;username&quot;: &quot;admin&quot;,
#       &quot;password&quot;: &quot;redhat&quot;,
#       &quot;auth&quot;: &quot;YWRtaW46cmVkaGF0&quot;,
#       &quot;email&quot;: &quot;admin@redhat.ren&quot;
#     }
#   }
# }

oc delete cm ca.for.registry -n openshift-config
oc create configmap ca.for.registry -n openshift-config \
    --from-file=registry.ocp4.redhat.ren..5443=/etc/crts/redhat.ren.ca.crt \
    --from-file=updateservice-registry=/etc/crts/redhat.ren.ca.crt

oc patch image.config.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;additionalTrustedCA&quot;:{&quot;name&quot;:&quot;ca.for.registry&quot;}}}'  --type=merge

# oc patch image.config.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;additionalTrustedCA&quot;:{&quot;name&quot;:&quot;ca.for.registry&quot;}}}'  --type=merge

# our router's https certs is self-sign, 
# update service will report error on this certs
# so we create a http route, to avoid this error
cat &lt;&lt; EOF &gt; /data/install/update-wzh-route.yaml
kind: Route
apiVersion: route.openshift.io/v1
metadata:
  name: update-wzh
  namespace: openshift-update-service
  labels:
    app: sample-policy-engine
spec:
  to:
    kind: Service
    name: sample-policy-engine
    weight: 100
  port:
    targetPort: policy-engine
EOF
oc create -f /data/install/update-wzh-route.yaml

oc patch clusterversion version --type='json' -p='[{&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/upstream&quot;, &quot;value&quot;: &quot;http://update-wzh-openshift-update-service.apps.ocp4.redhat.ren/api/upgrades_info/v1/graph&quot;}]'

oc get clusterversion version -o yaml | more

</code></pre>
<p>可以在operator的图形界面中，配置离线的update service参数
<img src="ocp4/4.8/imgs/2021-09-07-16-06-15.png" alt="" /></p>
<p>离线update service配置好了以后，看上去就非常舒适了。
<img src="ocp4/4.8/imgs/2021-09-07-22-15-41.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="windows-node-in-openshift-48"><a class="header" href="#windows-node-in-openshift-48">windows node in openshift 4.8</a></h1>
<p>在本文中，我们将安装一个win10节点，并加入到openshift 4.8集群中去。之后会部署一个演示应用。</p>
<p>经过测试，我们发现，当前的win10当作worker节点，还是不太适合，原因如下：</p>
<ul>
<li>windows要求容器的基础镜像版本，和宿主机的版本严格一致，这样就不能向rhel一样，在rhel8上运行rhel7的容器，在部署的时候会造成很大困惑。</li>
<li>windows的容器，不能运行GUI app。虽然也有很多.net的web服务应用，但是更多的老旧windows应用，应该还是包含GUI的程序。这样大大的限制了windows容器的应用访问。</li>
<li>docker for windows版本，只能设置proxy，不能为第三方镜像仓库设置mirror，这样对于离线部署，就很难受了。</li>
<li>目前版本，对静态IP部署还不友好，需要手动配置windows网卡。</li>
<li>目前版本的稳定性还有待加强，会出现k8s的服务崩溃现象，只能做开发测试，体验用，当然如果我们用windows server来做，稳定性会好很多。</li>
</ul>
<p>本次部署的架构图：</p>
<p><img src="ocp4/4.8/./dia/4.8.win.1node.drawio.svg" alt="" /></p>
<p>视频讲解:</p>
<p><a href="https://www.bilibili.com/video/BV1Hf4y1c79D/"><kbd><img src="ocp4/4.8/imgs/2021-10-10-09-47-25.png" width="600"></kbd></a></p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1Hf4y1c79D/">bilibili</a></li>
<li><a href="https://youtu.be/Zs7Z-HyEyEI">youtube</a></li>
</ul>
<h1 id="安装-win10"><a class="header" href="#安装-win10">安装 win10</a></h1>
<p>安装win10，需要注意选择正确的版本，因为win10的docker镜像版本，要求和宿主机一致。 <a href="https://hub.docker.com/_/microsoft-windows">在这里查看 win10 docker image version</a>.</p>
<p>在本文撰写的时候，版本是win10 20H2 20H2, <a href="https://www.itechtics.com/download-windows-10-20h2/">在这里找下载这个版本的ISO</a>.</p>
<p>选择好版本，我们就要开始安装了。</p>
<pre><code class="language-bash"># 先要准备一下 virtio 的驱动，因为 win10 里面没有， 安装的时候找不到硬盘。
podman pull registry.redhat.io/container-native-virtualization/virtio-win
podman run --rm -it --name swap registry.redhat.io/container-native-virtualization/virtio-win bash
podman create --name swap registry.redhat.io/container-native-virtualization/virtio-win ls
podman cp swap:/disk/virtio-win.iso - &gt; virtio-win.iso.tar
gzip virtio-win.iso.tar
podman rm swap

# 直接创建kvm, 自动开始安装啦。
export KVM_DIRECTORY=/data/kvm
virt-install --name=ocp4-windows --vcpus=6,cores=6 --ram=12288 \
--cpu=host-model \
--disk path=/data/nvme/ocp4-windows.qcow2,bus=virtio,size=100 \
--os-variant win10 --network bridge=baremetal,model=virtio \
--graphics vnc,port=59017 \
--boot menu=on \
--cdrom ${KVM_DIRECTORY}/win10.iso \
--disk ${KVM_DIRECTORY}/virtio-win.iso,device=cdrom

</code></pre>
<p>win10的话，必须选择专业版。</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-07-51-18.png" alt="" /></p>
<p>选择自定义安装，因为我们要加载硬盘驱动</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-07-51-56.png" alt="" /></p>
<p>选择加载驱动程序</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-07-52-32.png" alt="" /></p>
<p>选择正确的驱动程序位置</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-07-53-21.png" alt="" /></p>
<p>选择驱动，下一步</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-07-53-41.png" alt="" /></p>
<p>默认安装整个硬盘</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-07-54-30.png" alt="" /></p>
<p>安装就自动进行</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-07-54-48.png" alt="" /></p>
<p>安装完成后，进入系统，把剩下的驱动，一口气都装了。</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-08-22-33.png" alt="" /></p>
<p>系统识别出了网卡，那就设置IP地址吧</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-08-28-31.png" alt="" /></p>
<p>我们需要装ssh服务端，从 设置-应用 中找</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-08-29-47.png" alt="" /></p>
<p>点击可选功能</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-08-30-50.png" alt="" /></p>
<!-- 
![](imgs/2021-09-28-08-31-29.png) -->
<p>点击添加功能</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-08-32-27.png" alt="" /></p>
<p>搜索ssh服务器，并安装</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-08-32-57.png" alt="" /></p>
<!-- ![](imgs/2021-09-28-08-42-21.png) -->
<p>安装完了ssh是这样样子的</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-08-44-09.png" alt="" /></p>
<p>我们还需要打开防火墙端口，从网络配置进入</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-09-28-21.png" alt="" /></p>
<p>选择高级设置</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-09-29-03.png" alt="" /></p>
<p>新建入站规则</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-09-53-32.png" alt="" /></p>
<p>根据文档要求，打开 22, 10250 端口</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-09-55-25.png" alt="" /></p>
<p>允许连接</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-09-56-22.png" alt="" /></p>
<p>所有网络位置都允许</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-09-57-00.png" alt="" /></p>
<p>给起个名字</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-09-57-41.png" alt="" /></p>
<!-- ![](imgs/2021-09-28-10-04-17.png) -->
<p>ssh服务不是自动启动了，我们设置成自动启动</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-10-29-16.png" alt="" /></p>
<p>选择自动</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-10-30-08.png" alt="" /></p>
<p>从外面，就能ssh到windows了</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-10-31-24.png" alt="" /></p>
<p>我把实验用的win10，打包到了一个镜像里面，需要的可以下载使用。</p>
<p>用户名密码是: wzh / redhat</p>
<pre><code class="language-bash">ssh wzh@worker-1
# Microsoft Windows [版本 10.0.19043.1237]
# (c) Microsoft Corporation。保留所有权利。

# wzh@DESKTOP-FUIF19L C:\Users\wzh&gt;

</code></pre>
<h2 id="设置-ssh-key-auth"><a class="header" href="#设置-ssh-key-auth">设置 ssh key auth</a></h2>
<p>我们需要设置ssh使用key的方式自动登录，那么要有几个特殊的步骤。</p>
<p>首先，是<a href="https://superuser.com/questions/106360/how-to-enable-execution-of-powershell-scripts">解除win10的powershell的限制</a></p>
<pre><code>Set-ExecutionPolicy unrestricted
</code></pre>
<p>接下来准备2个文件</p>
<ul>
<li><a href="https://docs.microsoft.com/en-us/windows/wsl/install-manual#step-4---download-the-linux-kernel-update-package">wsl2 kernel update</a></li>
<li><a href="https://docs.docker.com/desktop/windows/install/">docker install file</a></li>
</ul>
<p><a href="https://www.concurrency.com/blog/may-2019/key-based-authentication-for-openssh-on-windows">参考这个文章</a>，写一个允许ssh自动key登录的脚本，我们在里面还加上了自动激活hyper-v, windows container的步骤。</p>
<pre><code class="language-bash"># the script here also enable hyper-v and windows container
cat &lt;&lt; 'EOF' &gt; /data/install/win-ssh.ps1
$acl = Get-Acl C:\ProgramData\ssh\administrators_authorized_keys
$acl.SetAccessRuleProtection($true, $false)
$administratorsRule = New-Object system.security.accesscontrol.filesystemaccessrule(&quot;Administrators&quot;,&quot;FullControl&quot;,&quot;Allow&quot;)
$systemRule = New-Object system.security.accesscontrol.filesystemaccessrule(&quot;SYSTEM&quot;,&quot;FullControl&quot;,&quot;Allow&quot;)
$acl.SetAccessRule($administratorsRule)
$acl.SetAccessRule($systemRule)
$acl | Set-Acl

Enable-WindowsOptionalFeature -Online -FeatureName $(&quot;Microsoft-Hyper-V&quot;, &quot;Containers&quot;) -All
EOF

# 把脚本, key, 还有安装文件，复制到win10上 
scp /data/install/win-ssh.ps1 wzh@worker-1:c:\\win-ssh.ps1

scp /root/.ssh/id_rsa.pub wzh@worker-1:C:\\ProgramData\\ssh\\administrators_authorized_keys

scp /data/down/Docker\ Desktop\ Installer.exe wzh@worker-1:c:\\docker-install.exe

scp /data/down/wsl_update_x64.msi wzh@worker-1:c:\\wsl_update_x64.msi
</code></pre>
<!-- ![](imgs/2021-09-28-17-36-54.png)

![](imgs/2021-09-28-17-36-18.png) -->
<p>用管理员权限，打开power shell</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-17-57-54.png" alt="" /></p>
<p>运行我们的脚本</p>
<p><img src="ocp4/4.8/imgs/2021-09-28-17-59-35.png" alt="" /></p>
<!-- Restart your openssh server on win10, then you can ssh to you win10 without password. :) -->
<p>重启win10, 然后你就可以用key自动登录啦。</p>
<!-- ## enable hyper-v & windows container

https://docs.microsoft.com/en-us/virtualization/hyper-v-on-windows/quick-start/enable-hyper-v

https://docs.microsoft.com/en-us/virtualization/windowscontainers/quick-start/set-up-environment?tabs=Windows-10

![](imgs/2021-09-28-19-20-05.png)

![](imgs/2021-09-28-19-21-06.png)

![](imgs/2021-09-28-19-25-49.png) -->
<!-- https://hub.docker.com/editions/community/docker-ce-desktop-windows -->
<p>安装docker，并切换到windows container。</p>
<p>第一次启动docker，会说什么wsl2 linux kernel要更新，可以用我提供的文件，直接更新，也可以直接切换windows container，不用理会那个报警。</p>
<!-- remember to switch to "windows container" -->
<p><img src="ocp4/4.8/imgs/2021-09-28-19-59-26.png" alt="" /></p>
<!-- also set the docker's registry mirror, and insecure registry

![](imgs/2021-09-29-17-51-50.png) -->
<p><a href="https://blog.miniasp.com/post/2019/03/22/Enable-process-isolation-by-default-on-Windows-containers/">设置 docker for windows, 使用 process 来隔离</a>, 因为kvm上的某种未知配置错误，默认hyper-v形式的隔离，启动不了容器，我们换成process来隔离.</p>
<pre><code class="language-json">{
  &quot;registry-mirrors&quot;: [],
  &quot;insecure-registries&quot;: [],
  &quot;debug&quot;: true,
  &quot;experimental&quot;: false,
  &quot;exec-opts&quot;: [
    &quot;isolation=process&quot;
  ]
}
</code></pre>
<p>配置界面长这样</p>
<p><img src="ocp4/4.8/imgs/2021-09-30-20-55-00.png" alt="" /></p>
<p>记得改一下windows的主机名</p>
<p><img src="ocp4/4.8/imgs/2021-09-29-11-06-50.png" alt="" /></p>
<h2 id="backup-win10-kvm"><a class="header" href="#backup-win10-kvm">backup win10 kvm</a></h2>
<p>我们备份一下win10 kvm，并上传quay.io，方便以后重新做实验。</p>
<p><a href="https://schh.medium.com/backup-and-restore-kvm-vms-21c049e707c1">我们可以参考这里，来备份和回复kvm。</a></p>
<pre><code class="language-bash"># poweroff you win7 vm

mkdir -p /data/nvme/bak

cd /data/nvme

virsh dumpxml ocp4-windows &gt; /data/nvme/bak/ocp4-windows.xml
pigz -c ocp4-windows.qcow2 &gt; /data/nvme/bak/ocp4-windows.qcow2.gz

cd /data/nvme/bak

var_date=$(date '+%Y-%m-%d-%H%M')
echo $var_date

buildah from --name onbuild-container scratch
buildah copy onbuild-container ocp4-windows.xml  /
buildah copy onbuild-container ocp4-windows.qcow2.gz  /
buildah umount onbuild-container 
buildah commit --rm onbuild-container quay.io/wangzheng422/qimgs:win7-ssh-$var_date
# buildah rm onbuild-container
# rm -f nexus-image.tgz 
echo &quot;quay.io/wangzheng422/qimgs:win7-ssh-$var_date&quot;
buildah push quay.io/wangzheng422/qimgs:win7-ssh-$var_date

# so, we got a image contain win10, and feature enabled.
# this is for win10 versin 10.0.19043.1237
# quay.io/wangzheng422/qimgs:win7-ssh-2021-09-30-1340

</code></pre>
<!-- you can use the image above to extract the win10 image, and run locally, to try the windows node. -->
<p>你可以使用上面的这个版本的镜像，拉取到本地，并从中取出win10虚拟机，然后自己尝试啦。</p>
<h1 id="安装-ocp-使用-ovn-with-hybrid-mode"><a class="header" href="#安装-ocp-使用-ovn-with-hybrid-mode">安装 ocp, 使用 ovn with hybrid mode</a></h1>
<p>参考官方文档：</p>
<ul>
<li>https://docs.openshift.com/container-platform/4.8/windows_containers/byoh-windows-instance.html</li>
<li>https://docs.openshift.com/container-platform/4.8/windows_containers/enabling-windows-container-workloads.html</li>
</ul>
<pre><code class="language-bash">
# vi install-config.yaml 
cat &lt;&lt; EOF &gt; /data/install/install-config.yaml 
apiVersion: v1
baseDomain: redhat.ren
compute:
- hyperthreading: Enabled
  name: worker
  replicas: 0
controlPlane:
  hyperthreading: Enabled
  name: master
  replicas: 1
metadata:
  name: ocp4
networking:
  clusterNetworks:
  - cidr: 10.128.0.0/16
    hostPrefix: 23
  networkType: OVNKubernetes
  serviceNetwork:
  - 172.30.0.0/16
platform:
  none: {}
pullSecret: '{&quot;auths&quot;:{&quot;registry.ocp4.redhat.ren:5443&quot;: {&quot;auth&quot;: &quot;ZHVtbXk6ZHVtbXk=&quot;,&quot;email&quot;: &quot;noemail@localhost&quot;},&quot;registry.ppa.redhat.ren:5443&quot;: {&quot;auth&quot;: &quot;ZHVtbXk6ZHVtbXk=&quot;,&quot;email&quot;: &quot;noemail@localhost&quot;}}}'
sshKey: |
$( cat /root/.ssh/id_rsa.pub | sed 's/^/   /g' )
additionalTrustBundle: |
$( cat /etc/crts/redhat.ren.ca.crt | sed 's/^/   /g' )
imageContentSources:
- mirrors:
  - registry.ocp4.redhat.ren:5443/ocp4/openshift4
  - registry.ocp4.redhat.ren:5443/ocp4/release
  source: quay.io/openshift-release-dev/ocp-release
- mirrors:
  - registry.ocp4.redhat.ren:5443/ocp4/openshift4
  - registry.ocp4.redhat.ren:5443/ocp4/release
  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
EOF

cat &lt;&lt; EOF &gt; /data/install/manifests/cluster-network-03-config.yml
apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  defaultNetwork:
    ovnKubernetesConfig:
      hybridOverlayConfig:
        hybridClusterNetwork: 
        - cidr: 10.132.0.0/16
          hostPrefix: 23
        hybridOverlayVXLANPort: 9898 
EOF

</code></pre>
<p>安装windows machien config operator
<img src="ocp4/4.8/imgs/2021-09-28-20-16-19.png" alt="" /></p>
<pre><code class="language-bash"># 导入ssh key
oc create secret generic cloud-private-key --from-file=private-key.pem=/root/.ssh/id_rsa \
    -n openshift-windows-machine-config-operator

# 配置win10自动登录用户名和ip地址
cat &lt;&lt; EOF &gt; /data/install/win-node.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: windows-instances
  namespace: openshift-windows-machine-config-operator
data:
  192.168.7.17: |- 
    username=wzh
EOF
oc create -f /data/install/win-node.yaml

# to restore
oc delete -f /data/install/win-node.yaml

# csr is automatically approved
oc get csr
# NAME                                       AGE   SIGNERNAME                                    REQUESTOR                                                                         CONDITION
# csr-ff7q5                                  63m   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper         Approved,Issued
# csr-gzlpq                                  53s   kubernetes.io/kubelet-serving                 system:node:worker-1                                                              Approved,Issued
# csr-rgdzv                                  59s   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper         Approved,Issued
# csr-zkw8c                                  63m   kubernetes.io/kubelet-serving                 system:node:master-0                                                              Approved,Issued
# system:openshift:openshift-authenticator   59m   kubernetes.io/kube-apiserver-client           system:serviceaccount:openshift-authentication-operator:authentication-operator   Approved,Issued
</code></pre>
<p>估计是当前实现的bug，或者其他原因，windows的默认网卡，上面的协议会被disable掉，造成windows node加入集群失败，目前暂时手动的把这些协议都enable，只留一个不激活。当然，你也可以只enable ipv4的配置，也是可以的。
<img src="ocp4/4.8/imgs/2021-09-29-11-20-20.png" alt="" /></p>
<p>之后就等着好了，openshift会自动上传程序和配置，并配置好windows node，加入集群，成功以后，我们就能看到如下的日志。</p>
<pre><code class="language-log">{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1633004643.789956,&quot;logger&quot;:&quot;controllers.configmap&quot;,&quot;msg&quot;:&quot;processing&quot;,&quot;instances in&quot;:&quot;windows-instances&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1633004674.0080738,&quot;logger&quot;:&quot;wc 192.168.7.17&quot;,&quot;msg&quot;:&quot;configuring&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1633004675.3135288,&quot;logger&quot;:&quot;wc 192.168.7.17&quot;,&quot;msg&quot;:&quot;transferring files&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1633004693.670281,&quot;logger&quot;:&quot;wc 192.168.7.17&quot;,&quot;msg&quot;:&quot;configured&quot;,&quot;service&quot;:&quot;windows_exporter&quot;,&quot;args&quot;:&quot;--collectors.enabled cpu,cs,logical_disk,net,os,service,system,textfile,container,memory,cpu_info\&quot;&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1633004697.0266535,&quot;logger&quot;:&quot;controllers.CertificateSigningRequests&quot;,&quot;msg&quot;:&quot;CSR approved&quot;,&quot;CSR&quot;:&quot;csr-rgdzv&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1633004703.104529,&quot;logger&quot;:&quot;controllers.CertificateSigningRequests&quot;,&quot;msg&quot;:&quot;CSR approved&quot;,&quot;CSR&quot;:&quot;csr-gzlpq&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1633004726.9497287,&quot;logger&quot;:&quot;wc 192.168.7.17&quot;,&quot;msg&quot;:&quot;configured kubelet&quot;,&quot;cmd&quot;:&quot;C:\\k\\\\wmcb.exe initialize-kubelet --ignition-file C:\\Windows\\Temp\\worker.ign --kubelet-path C:\\k\\kubelet.exe --node-ip=192.168.7.17&quot;,&quot;output&quot;:&quot;Bootstrapping completed successfully&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1633004757.078427,&quot;logger&quot;:&quot;wc 192.168.7.17&quot;,&quot;msg&quot;:&quot;configure&quot;,&quot;service&quot;:&quot;hybrid-overlay-node&quot;,&quot;args&quot;:&quot;--node worker-1 --hybrid-overlay-vxlan-port=9898 --k8s-kubeconfig c:\\k\\kubeconfig --windows-service --logfile C:\\var\\log\\hybrid-overlay\\hybrid-overlay.log\&quot; depend= kubelet&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1633004880.6788793,&quot;logger&quot;:&quot;wc 192.168.7.17&quot;,&quot;msg&quot;:&quot;configured&quot;,&quot;service&quot;:&quot;hybrid-overlay-node&quot;,&quot;args&quot;:&quot;--node worker-1 --hybrid-overlay-vxlan-port=9898 --k8s-kubeconfig c:\\k\\kubeconfig --windows-service --logfile C:\\var\\log\\hybrid-overlay\\hybrid-overlay.log\&quot; depend= kubelet&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1633004928.5883121,&quot;logger&quot;:&quot;wc 192.168.7.17&quot;,&quot;msg&quot;:&quot;configured kubelet for CNI&quot;,&quot;cmd&quot;:&quot;C:\\k\\wmcb.exe configure-cni --cni-dir=\&quot;C:\\k\\cni\\ --cni-config=\&quot;C:\\k\\cni\\config\\cni.conf&quot;,&quot;output&quot;:&quot;CNI configuration completed successfully&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1633004941.3937094,&quot;logger&quot;:&quot;wc 192.168.7.17&quot;,&quot;msg&quot;:&quot;configured&quot;,&quot;service&quot;:&quot;kube-proxy&quot;,&quot;args&quot;:&quot;--windows-service --v=4 --proxy-mode=kernelspace --feature-gates=WinOverlay=true --hostname-override=worker-1 --kubeconfig=c:\\k\\kubeconfig --cluster-cidr=10.132.0.0/24 --log-dir=C:\\var\\log\\kube-proxy\\ --logtostderr=false --network-name=OVNKubernetesHybridOverlayNetwork --source-vip=10.132.0.14 --enable-dsr=false --feature-gates=IPv6DualStack=false\&quot; depend= hybrid-overlay-node&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1633004956.4613981,&quot;logger&quot;:&quot;nc 192.168.7.17&quot;,&quot;msg&quot;:&quot;instance has been configured as a worker node&quot;,&quot;version&quot;:&quot;3.1.0+06e96071&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1633004956.4949114,&quot;logger&quot;:&quot;metrics&quot;,&quot;msg&quot;:&quot;Prometheus configured&quot;,&quot;endpoints&quot;:&quot;windows-exporter&quot;,&quot;port&quot;:9182,&quot;name&quot;:&quot;metrics&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1633004956.5283544,&quot;logger&quot;:&quot;controllers.configmap&quot;,&quot;msg&quot;:&quot;processing&quot;,&quot;instances in&quot;:&quot;windows-instances&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1633004956.5387952,&quot;logger&quot;:&quot;controllers.configmap&quot;,&quot;msg&quot;:&quot;instance is up to date&quot;,&quot;node&quot;:&quot;worker-1&quot;,&quot;version&quot;:&quot;3.1.0+06e96071&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1633004956.5493839,&quot;logger&quot;:&quot;metrics&quot;,&quot;msg&quot;:&quot;Prometheus configured&quot;,&quot;endpoints&quot;:&quot;windows-exporter&quot;,&quot;port&quot;:9182,&quot;name&quot;:&quot;metrics&quot;}
</code></pre>
<p>我们能看到 windows节点了。</p>
<pre><code class="language-bash">oc get node
# NAME       STATUS   ROLES           AGE     VERSION
# master-0   Ready    master,worker   19h     v1.21.1+a620f50
# worker-1   Ready    worker          4m50s   v1.21.1-1398+98073871f173ba

oc get node --show-labels
# NAME       STATUS   ROLES           AGE     VERSION                       LABELS
# master-0   Ready    master,worker   4h13m   v1.21.1+a620f50               beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master-0,kubernetes.io/os=linux,node-role.kubernetes.io/master=,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos
# worker-1   Ready    worker          5m25s   v1.21.1-1398+98073871f173ba   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=windows,kubernetes.io/arch=amd64,kubernetes.io/hostname=worker-1,kubernetes.io/os=windows,node-role.kubernetes.io/worker=,node.kubernetes.io/windows-build=10.0.19042,node.openshift.io/os_id=Windows,windowsmachineconfig.openshift.io/byoh=true

# 看了windows节点不占用machine config pool
oc get mcp
# NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
# master   rendered-master-607708e411d75c10e680d8bf5e24de6f   True      False      False      1              1                   1                     0                      19h
# worker   rendered-worker-cacf7f7f871c77ae92070b0a44fe0b91   True      False      False      0              0                   0                     0                      19h
</code></pre>
<h2 id="探索一下装了什么"><a class="header" href="#探索一下装了什么">探索一下装了什么</a></h2>
<p>进入win10，可以看到C:\下面，有一个k目录，还有一个var目录，k目录下面就是配置和可执行程序啦。</p>
<pre><code class="language-cmd">wzh@WORKER-1 c:\&gt;dir
 驱动器 C 中的卷没有标签。
 卷的序列号是 C607-13D4

 c:\ 的目录

2021/09/28  19:37       535,444,968 Docker Desktop Installer.exe
2021/09/29  11:12    &lt;DIR&gt;          k
2019/12/07  17:14    &lt;DIR&gt;          PerfLogs
2021/09/28  19:57    &lt;DIR&gt;          Program Files
2021/04/09  21:57    &lt;DIR&gt;          Program Files (x86)
2021/09/29  11:12    &lt;DIR&gt;          Temp
2021/09/28  08:25    &lt;DIR&gt;          Users
2021/09/29  11:11    &lt;DIR&gt;          var
2021/09/28  17:51               428 win-ssh.ps1
2021/09/28  16:34    &lt;DIR&gt;          Windows
               2 个文件    535,445,396 字节
               8 个目录 19,381,813,248 可用字节

wzh@WORKER-1 c:\&gt;dir k
 驱动器 C 中的卷没有标签。
 卷的序列号是 C607-13D4

 c:\k 的目录

2021/09/29  11:12    &lt;DIR&gt;          .
2021/09/29  11:12    &lt;DIR&gt;          ..
2021/09/29  11:12            10,908 bootstrap-kubeconfig
2021/09/29  11:12    &lt;DIR&gt;          cni
2021/09/29  11:12    &lt;DIR&gt;          etc
2021/09/29  11:12        47,493,632 hybrid-overlay-node.exe
2021/09/29  11:12        47,809,536 kube-proxy.exe
2021/09/29  11:12            10,132 kubeconfig
2021/09/29  11:12             5,875 kubelet-ca.crt
2021/09/29  11:12               739 kubelet.conf
2021/09/29  11:12       117,698,048 kubelet.exe
2021/09/29  11:12    &lt;DIR&gt;          usr
2021/09/29  11:12        16,986,112 windows_exporter.exe
2021/09/29  11:12        16,331,776 wmcb.exe
               9 个文件    246,346,758 字节
               5 个目录 19,381,317,632 可用字节

wzh@WORKER-1 c:\&gt;dir var\log
 驱动器 C 中的卷没有标签。
 卷的序列号是 C607-13D4

 c:\var\log 的目录

2021/09/29  11:12    &lt;DIR&gt;          .
2021/09/29  11:12    &lt;DIR&gt;          ..
2021/09/29  11:12    &lt;DIR&gt;          containers
2021/09/29  11:12    &lt;DIR&gt;          hybrid-overlay
2021/09/29  11:16    &lt;DIR&gt;          kube-proxy
2021/09/29  11:12    &lt;DIR&gt;          kubelet
2021/09/29  11:12    &lt;DIR&gt;          pods
               0 个文件              0 字节
               7 个目录 19,381,059,584 可用字节

wzh@WORKER-1 c:\&gt;dir var\lib
 驱动器 C 中的卷没有标签。
 卷的序列号是 C607-13D4

 c:\var\lib 的目录

2021/09/28  20:36    &lt;DIR&gt;          .
2021/09/28  20:36    &lt;DIR&gt;          ..
2021/09/28  20:36    &lt;DIR&gt;          dockershim
2021/09/28  20:38    &lt;DIR&gt;          kubelet
               0 个文件              0 字节
               4 个目录 19,381,043,200 可用字节

</code></pre>
<h2 id="删除windows节点"><a class="header" href="#删除windows节点">删除windows节点</a></h2>
<p>除了官方文档说的，改config map之外，发现，最好还是重启一下windows node为好。</p>
<p>改了config map，耐心等着，最后oc get node，就会看到windows node没有了。</p>
<p>从operator的日志里面，可以看到如下的日志信息。</p>
<pre><code class="language-log">{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1632916600.248877,&quot;logger&quot;:&quot;controllers.configmap&quot;,&quot;msg&quot;:&quot;processing&quot;,&quot;instances in&quot;:&quot;windows-instances&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1632916610.646764,&quot;logger&quot;:&quot;wc 192.168.7.17&quot;,&quot;msg&quot;:&quot;deconfiguring&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1632916641.877409,&quot;logger&quot;:&quot;wc 192.168.7.17&quot;,&quot;msg&quot;:&quot;deconfigured&quot;,&quot;service&quot;:&quot;windows_exporter&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1632916672.9587948,&quot;logger&quot;:&quot;wc 192.168.7.17&quot;,&quot;msg&quot;:&quot;deconfigured&quot;,&quot;service&quot;:&quot;kube-proxy&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1632916703.9290483,&quot;logger&quot;:&quot;wc 192.168.7.17&quot;,&quot;msg&quot;:&quot;deconfigured&quot;,&quot;service&quot;:&quot;hybrid-overlay-node&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1632916734.8715909,&quot;logger&quot;:&quot;wc 192.168.7.17&quot;,&quot;msg&quot;:&quot;deconfigured&quot;,&quot;service&quot;:&quot;kubelet&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1632916734.8733184,&quot;logger&quot;:&quot;wc 192.168.7.17&quot;,&quot;msg&quot;:&quot;removing directories&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1632916735.4904935,&quot;logger&quot;:&quot;wc 192.168.7.17&quot;,&quot;msg&quot;:&quot;removing HNS networks&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1632916924.5720427,&quot;logger&quot;:&quot;nc 192.168.7.17&quot;,&quot;msg&quot;:&quot;instance has been deconfigured&quot;,&quot;node&quot;:&quot;worker-1&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1632916924.6041753,&quot;logger&quot;:&quot;metrics&quot;,&quot;msg&quot;:&quot;Prometheus configured&quot;,&quot;endpoints&quot;:&quot;windows-exporter&quot;,&quot;port&quot;:9182,&quot;name&quot;:&quot;metrics&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1632916924.6054258,&quot;logger&quot;:&quot;controllers.configmap&quot;,&quot;msg&quot;:&quot;processing&quot;,&quot;instances in&quot;:&quot;windows-instances&quot;}
{&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1632916924.6281445,&quot;logger&quot;:&quot;metrics&quot;,&quot;msg&quot;:&quot;Prometheus configured&quot;,&quot;endpoints&quot;:&quot;windows-exporter&quot;,&quot;port&quot;:9182,&quot;name&quot;:&quot;metrics&quot;}
</code></pre>
<h2 id="resize-qcow2-disk"><a class="header" href="#resize-qcow2-disk">resize qcow2 disk</a></h2>
<p>https://computingforgeeks.com/how-to-extend-increase-kvm-virtual-machine-disk-size/</p>
<pre><code class="language-bash">qemu-img info /data/nvme/ocp4-windows.qcow2
# image: /data/nvme/ocp4-windows.qcow2
# file format: qcow2
# virtual size: 50 GiB (53687091200 bytes)
# disk size: 43.3 GiB
# cluster_size: 65536
# Format specific information:
#     compat: 1.1
#     lazy refcounts: true
#     refcount bits: 16
#     corrupt: false

qemu-img resize /data/nvme/ocp4-windows.qcow2 +20G
# Image resized.


</code></pre>
<h1 id="windows-workload"><a class="header" href="#windows-workload">windows workload</a></h1>
<p>似乎现在的 docker for windows 并不支持给 mcr.microsoft.com 做镜像代理，只能配置一个proxy，这个太讨厌了，等以后迁移到 podman 或者 containerd 吧。所以我们现在基本上属于联网或者半联网的部署模式。</p>
<p><a href="https://hub.docker.com/_/microsoft-windows-base-os-images">在这里查找windows镜像的版本</a>。</p>
<pre><code class="language-bash"># pod pause的镜像
# mcr.microsoft.com/oss/kubernetes/pause:3.4.1

# 创建runtime class
cat &lt;&lt; EOF &gt; /data/install/win-runtime.yaml
apiVersion: node.k8s.io/v1beta1
kind: RuntimeClass
metadata:
  name: runtime-class-win10
handler: 'docker'
scheduling:
  nodeSelector: 
    kubernetes.io/os: 'windows'
    kubernetes.io/arch: 'amd64'
    node.kubernetes.io/windows-build: '10.0.19042'
  tolerations: 
  - effect: NoSchedule
    key: os
    operator: Equal
    value: &quot;Windows&quot;
EOF
oc create -f /data/install/win-runtime.yaml

# https://hub.docker.com/_/microsoft-windows
# mcr.microsoft.com/windows:20H2
cat &lt;&lt; 'EOF' &gt; /data/install/win-dep.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: win-webserver
  name: win-webserver
spec:
  selector:
    matchLabels:
      app: win-webserver
  replicas: 1
  template:
    metadata:
      labels:
        app: win-webserver
      name: win-webserver
    spec:
      tolerations:
      - key: &quot;os&quot;
        value: &quot;Windows&quot;
        Effect: &quot;NoSchedule&quot;
      containers:
      - name: windowswebserver
        image: mcr.microsoft.com/windows:20H2
        imagePullPolicy: IfNotPresent
        command:
        - powershell.exe
        - -command
        - $listener = New-Object System.Net.HttpListener; $listener.Prefixes.Add('http://*:80/'); $listener.Start();Write-Host('Listening at http://*:80/'); while ($listener.IsListening) { $context = $listener.GetContext(); $response = $context.Response; $content='&lt;html&gt;&lt;body&gt;&lt;H1&gt;Red Hat OpenShift + Windows Container Workloads&lt;/H1&gt;&lt;/body&gt;&lt;/html&gt;'; $buffer = [System.Text.Encoding]::UTF8.GetBytes($content); $response.ContentLength64 = $buffer.Length; $response.OutputStream.Write($buffer, 0, $buffer.Length); $response.Close(); };
        securityContext:
          windowsOptions:
            runAsUserName: &quot;ContainerAdministrator&quot;
      nodeSelector:
        beta.kubernetes.io/os: windows
EOF
oc create -f /data/install/win-dep.yaml

# to restore
oc delete -f /data/install/win-dep.yaml

cat &lt;&lt; EOF &gt; /data/install/win-svc.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: win-webserver
  labels:
    app: win-webserver
spec:
  ports:
    # the port that this service should serve on
  - port: 80
    targetPort: 80
  selector:
    app: win-webserver
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: win-webserver
spec:
  port:
    targetPort: 80
  to:
    kind: Service
    name: win-webserver
---
EOF
oc create -f /data/install/win-svc.yaml

# try windows server core, if you run on windows server
# otherwize, it will failed, say os not match with host: 
# &quot;The container operating system does not match the host operating system.&quot;
# https://hub.docker.com/_/microsoft-windows-servercore
# mcr.microsoft.com/windows/servercore:20H2

cat &lt;&lt; EOF &gt; /data/install/test-pod.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mypod
  labels:
    app: mypod
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mypod
  template:
    metadata:
      labels:
        app: mypod
    spec:
      containers:
      - name: mypod
        image: quay.io/wangzheng422/qimgs:centos7-test
        command:
          - sleep
          - infinity
EOF
oc create -f /data/install/test-pod.yaml

oc get all
# NAME                                READY   STATUS    RESTARTS   AGE
# pod/mypod-6b8b7b46cb-rrfmd          1/1     Running   1          21h
# pod/win-webserver-9f98c76d4-8nb2q   1/1     Running   0          110s

# NAME                    TYPE           CLUSTER-IP      EXTERNAL-IP                            PORT(S)   AGE
# service/kubernetes      ClusterIP      172.30.0.1      &lt;none&gt;                                 443/TCP   26h
# service/openshift       ExternalName   &lt;none&gt;          kubernetes.default.svc.cluster.local   &lt;none&gt;    25h
# service/win-webserver   ClusterIP      172.30.240.75   &lt;none&gt;                                 80/TCP    21h

# NAME                            READY   UP-TO-DATE   AVAILABLE   AGE
# deployment.apps/mypod           1/1     1            1           21h
# deployment.apps/win-webserver   1/1     1            1           110s

# NAME                                      DESIRED   CURRENT   READY   AGE
# replicaset.apps/mypod-6b8b7b46cb          1         1         1       21h
# replicaset.apps/win-webserver-9f98c76d4   1         1         1       110s

# NAME                                     HOST/PORT                                    PATH   SERVICES        PORT   TERMINATION   WILDCARD
# route.route.openshift.io/win-webserver   win-webserver-default.apps.ocp4.redhat.ren          win-webserver   80                   None

curl win-webserver-default.apps.ocp4.redhat.ren &amp;&amp; echo
# &lt;html&gt;&lt;body&gt;&lt;H1&gt;Red Hat OpenShift + Windows Container Workloads&lt;/H1&gt;&lt;/body&gt;&lt;/html&gt;

</code></pre>
<pre><code class="language-cmd">oc exec -it pod/win-webserver-9f98c76d4-8nb2q -- cmd

Microsoft Windows [Version 10.0.19042.1237]
(c) Microsoft Corporation. All rights reserved.

C:\&gt;tasklist

Image Name                     PID Session Name        Session#    Mem Usage
========================= ======== ================ =========== ============
System Idle Process              0                            0          8 K
System                           4                            0        148 K
smss.exe                      9992                            0      1,760 K
csrss.exe                     6788 Services                   3      4,524 K
wininit.exe                   7096 Services                   3      5,260 K
services.exe                  6456 Services                   3      6,668 K
lsass.exe                     3324 Services                   3     12,536 K
fontdrvhost.exe               5736 Services                   3      2,860 K
svchost.exe                   4948 Services                   3     12,896 K
svchost.exe                   6960 Services                   3      8,180 K
svchost.exe                   3332 Services                   3     16,952 K
svchost.exe                    756 Services                   3     53,864 K
svchost.exe                   5924 Services                   3      9,728 K
svchost.exe                   6412 Services                   3      8,012 K
svchost.exe                   5628 Services                   3      6,740 K
svchost.exe                   9488 Services                   3      4,688 K
svchost.exe                   8912 Services                   3     12,896 K
CExecSvc.exe                  5616 Services                   3      4,020 K
svchost.exe                   5916 Services                   3     28,600 K
svchost.exe                   2780 Services                   3      4,404 K
powershell.exe                2816 Services                   3     78,156 K
CompatTelRunner.exe           3056 Services                   3      2,852 K
svchost.exe                   9412 Services                   3     11,104 K
conhost.exe                   7748 Services                   3     10,824 K
svchost.exe                   3636 Services                   3      7,404 K
conhost.exe                   1288 Services                   3      3,800 K
cmd.exe                       5112 Services                   3      2,884 K
svchost.exe                   4492 Services                   3      8,900 K
MicrosoftEdgeUpdate.exe       8808 Services                   3      1,760 K
svchost.exe                   7612 Services                   3     10,112 K
conhost.exe                   4944 Services                   3      5,176 K
cmd.exe                       9848 Services                   3      5,140 K
MoUsoCoreWorker.exe           3016 Services                   3     17,220 K
WmiPrvSE.exe                  7924 Services                   3      9,340 K
WmiPrvSE.exe                  5976 Services                   3      9,384 K
spoolsv.exe                   6204 Services                   3      6,580 K
conhost.exe                   6184 Services                   3      5,208 K
cmd.exe                       5680 Services                   3      4,428 K
tasklist.exe                  8424 Services                   3      8,812 K
</code></pre>
<p>在win10上，我们能从docker界面上，看到有2个container启动了。
<img src="ocp4/4.8/imgs/2021-10-08-17-18-09.png" alt="" /></p>
<p>同样，在docker界面上，我们能看到他下载了2个镜像，并且正在使用中。
<img src="ocp4/4.8/imgs/2021-10-08-17-19-05.png" alt="" /></p>
<h1 id="排错"><a class="header" href="#排错">排错</a></h1>
<p>如果发现有异常，首先要做的是，查看kubelet, kubeproxy, hybrid-overlay-node 这3个服务，是不是还在运行，当前的版本，似乎这几个服务，很容易崩溃。</p>
<p>之后，就是看看默认网卡的ipv4配置，是否被禁用了，估计未来兼容性好了，就不用操心这个了。</p>
<!-- if anything wrong, first, check kubelet, kubeproxy, hybrid-overlay-node services on windows node, those services are easy to crash right now.

Then go to check nic ipv4 config, by default, it will change to dhcp, but in our static-ip deployment, it should change to some static ip config. -->
<pre><code class="language-bash"># on windows cmd
netsh interface dump

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-49-加载第三方驱动--内核模块"><a class="header" href="#openshift-49-加载第三方驱动--内核模块">openshift 4.9 加载第三方驱动 / 内核模块</a></h1>
<p>我们在项目中，会遇到特种硬件，比如 fpga 卡，软件供应商为这个 fpga 卡提供了驱动/内核模块，我们需要把这个驱动加载到系统中。本文就讲述，如何在 openshift 4.9 里面，通过 deployment / pod 的方式，想系统注入这个驱动/内核模块。</p>
<p>在本次实验中，物理机上有一块fpga卡，我们得到了对应的驱动 nr_drv_wr.ko ，这个驱动加载以后，会创建一个网卡，我们要初始化这个网卡。</p>
<p>好了，就让我们来看看是怎么做的吧。</p>
<h1 id="制作镜像"><a class="header" href="#制作镜像">制作镜像</a></h1>
<p>我们把驱动拷贝到镜像里面，还把自动加载脚本也复制到镜像里面。自动加载脚本里面，有一个小技巧，就是 ko 文件，需要打上正确的selinux 标签，否则 insmod 会报错。</p>
<pre><code class="language-bash">
mkdir -p /data/wzh/fpga
cd /data/wzh/fpga

cat &lt;&lt; 'EOF' &gt; ./ocp4.install.sh
#!/bin/bash

set -e
set -x

if  chroot /host lsmod  | grep nr_drv &gt; /dev/null 2&gt;&amp;1
then
    echo NR Driver Module had loaded!
else
    echo Inserting NR Driver Module
    # chroot /host rmmod nr_drv &gt; /dev/null 2&gt;&amp;1

    if [ $(uname -r) == &quot;4.18.0-305.19.1.rt7.91.el8_4.x86_64&quot; ];
    then
        echo insmod nr_drv_wr.ko ...
        /bin/cp -f nr_drv_wr.ko /host/tmp/nr_drv_wr.ko
        chroot /host chcon -t modules_object_t /tmp/nr_drv_wr.ko
        chroot /host insmod /tmp/nr_drv_wr.ko load_xeth=1
        /bin/rm -f /host/tmp/nr_drv_wr.ko

        CON_NAME=`chroot /host nmcli -g GENERAL.CONNECTION dev show xeth`

        chroot /host nmcli connection modify &quot;$CON_NAME&quot; con-name xeth
        chroot /host nmcli connection modify xeth ipv4.method disabled ipv6.method disabled
        chroot /host nmcli dev conn xeth
    else
        echo insmod nr_drv_ko Failed!
    fi

fi
EOF

cat &lt;&lt; EOF &gt; ./fpga.dockerfile
FROM docker.io/busybox:1.34

USER root
COPY Driver.PKG /Driver.PKG

COPY ocp4.install.sh /ocp4.install.sh
RUN chmod +x /ocp4.install.sh

WORKDIR /
EOF

buildah bud -t registry.ocp4.redhat.ren:5443/nep/fgpa-driver:v07 -f fpga.dockerfile .

buildah push registry.ocp4.redhat.ren:5443/nep/fgpa-driver:v07

</code></pre>
<h1 id="openshift-部署"><a class="header" href="#openshift-部署">openshift 部署</a></h1>
<p>部署之前，我们先给service account加上特权模式，我们这个实验，在default project里面，用了default service account，所以命令就在下面，但是到了具体项目中，一般是要创建单独的project，并且创建单独的service account的。</p>
<p>然后我们用了几个小技巧，首先用init container，把驱动复制进pod，传递给真正运行的容器，然后我们无限睡眠，保持这个pod运行，这么做是因为，如果容器正常退出了，deployment会自动重启，但是我们这里不想自动重启，所以我们无限睡眠，保持这个pod运行。好在这个 pod 消耗很小。</p>
<p>未来可能会优化成用 job / static pod 的方式来运行。</p>
<pre><code class="language-bash">
oc adm policy add-scc-to-user privileged -z default -n default

cat &lt;&lt; EOF &gt; /data/install/fpga.driver.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fpga-driver
  # namespace: default
  labels:
    app: fpga-driver
spec:
  replicas: 1
  selector:
    matchLabels:
      app: fpga-driver
  template:
    metadata:
      labels:
        app: fpga-driver
    spec:
      hostPID: true
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: &quot;app&quot;
                    operator: In
                    values:
                    - fpga-driver
              topologyKey: &quot;kubernetes.io/hostname&quot;
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/hostname
                operator: In
                values:
                - worker-0
      # restartPolicy: Never
      initContainers:
      - name: copy
        image: registry.ocp4.redhat.ren:5443/nep/fgpa-driver:v07
        command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;tar zvxf /Driver.PKG --strip 1 -C /nep/driver/ &amp;&amp; /bin/cp -f /ocp4.install.sh /nep/driver/ &quot;]
        imagePullPolicy: Always
        volumeMounts:
        - name: driver-files
          mountPath: /nep/driver/
      containers:
      - name: driver
        image: registry.redhat.io/rhel8/support-tools:8.4
        # imagePullPolicy: Always
        command: [ &quot;/usr/bin/bash&quot;,&quot;-c&quot;,&quot;cd /nep/driver/ &amp;&amp; bash ./ocp4.install.sh &amp;&amp; sleep infinity &quot; ]
        # command: [ &quot;/usr/bin/bash&quot;,&quot;-c&quot;,&quot;tail -f /dev/null || true &quot; ]
        resources:
          requests:
            cpu: 10m
            memory: 20Mi
        securityContext:
          privileged: true
          # runAsUser: 0
          seLinuxOptions:
            level: &quot;s0&quot;
        volumeMounts:
        - name: driver-files
          mountPath: /nep/driver/
        - name: host
          mountPath: /host
      volumes: 
      - name: driver-files
        emptyDir: {}
      - name: host
        hostPath:
          path: /
          type: Directory
EOF
oc create -f /data/install/fpga.driver.yaml

# to restore
oc delete -f /data/install/fpga.driver.yaml


</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="helm-chart--helm-operator-制作"><a class="header" href="#helm-chart--helm-operator-制作">helm chart / helm operator 制作</a></h1>
<p><a href="https://github.com/wangzheng422/baicell-helm-operator">2021.12 helm chart/helm operator</a></p>
<h2 id="build-helm-operator"><a class="header" href="#build-helm-operator">build helm operator</a></h2>
<pre><code class="language-bash">mkdir -p /data/down
cd /data/down
wget https://mirror.openshift.com/pub/openshift-v4/clients/operator-sdk/latest/operator-sdk-linux-x86_64.tar.gz
tar zvxf operator-sdk-linux-x86_64.tar.gz
install operator-sdk /usr/local/bin/

operator-sdk init --plugins helm --help

mkdir -p /data/helm
cd /data/helm

# 初始化项目
operator-sdk init \
    --plugins=helm \
    --project-name nep-helm-operator \
    --domain=nep.com \
    --group=apps \
    --version=v1alpha1 \
    --kind=VBBU 

make bundle
# operator-sdk generate kustomize manifests -q

# Display name for the operator (required):
# &gt; nep vBBU

# Description for the operator (required):
# &gt; nep vRAN application including fpga driver, vCU, vDU

# Provider's name for the operator (required):
# &gt; nep

# Any relevant URL for the provider name (optional):
# &gt; na.nep.com

# Comma-separated list of keywords for your operator (required):
# &gt; nep,vbbu,vran,vcu,vdu

# Comma-separated list of maintainers and their emails (e.g. 'name1:email1, name2:email2') (required):
# &gt;
# No list provided.
# Comma-separated list of maintainers and their emails (e.g. 'name1:email1, name2:email2') (required):
# &gt; wangzheng:wangzheng422@foxmail.com
# cd config/manager &amp;&amp; /data/helm/bin/kustomize edit set image controller=quay.io/nep/nep-helm-operator:latest
# /data/helm/bin/kustomize build config/manifests | operator-sdk generate bundle -q --overwrite --version 0.0.1
# INFO[0001] Creating bundle.Dockerfile
# INFO[0001] Creating bundle/metadata/annotations.yaml
# INFO[0001] Bundle metadata generated suceessfully
# operator-sdk bundle validate ./bundle
# INFO[0000] All validation tests have completed successfully

cd /data/helm/helm-charts/vbbu
helm lint

dnf install -y podman-docker

cd /data/helm/
make docker-build
# docker build -t quay.io/nep/nep-helm-operator:v01 .
# Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# STEP 1/5: FROM registry.redhat.io/openshift4/ose-helm-operator:v4.9
# STEP 2/5: ENV HOME=/opt/helm
# --&gt; 1eec2f9c094
# STEP 3/5: COPY watches.yaml ${HOME}/watches.yaml
# --&gt; 1836589a08c
# STEP 4/5: COPY helm-charts  ${HOME}/helm-charts
# --&gt; b6cd9f24e47
# STEP 5/5: WORKDIR ${HOME}
# COMMIT quay.io/nep/nep-helm-operator:v01
# --&gt; 1f9bcc4cecc
# Successfully tagged quay.io/nep/nep-helm-operator:v01
# 1f9bcc4cecc55e68170e2a6f45dad7b318018df8bf3989bd990f567e3ccdfcd9

make docker-push
# docker push quay.io/nep/nep-helm-operator:v01
# Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# Getting image source signatures
# Copying blob 8cd9b2cfbe06 skipped: already exists
# Copying blob 5bc03dec6239 skipped: already exists
# Copying blob 525ed45dbdb1 skipped: already exists
# Copying blob 758ace4ace74 skipped: already exists
# Copying blob deb6b0f93acd skipped: already exists
# Copying blob ac83cd3b61fd skipped: already exists
# Copying blob 12f964d7475b [--------------------------------------] 0.0b / 0.0b
# Copying config 1f9bcc4cec [--------------------------------------] 0.0b / 4.0KiB
# Writing manifest to image destination
# Copying config 1f9bcc4cec [--------------------------------------] 0.0b / 4.0KiB
# Writing manifest to image destination
# Storing signatures

make bundle-build BUNDLE_IMG=quay.io/nep/nep-helm-operator:bundle-v01
# docker build -f bundle.Dockerfile -t quay.io/nep/nep-helm-operator:bundle-v01 .
# Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# STEP 1/14: FROM scratch
# STEP 2/14: LABEL operators.operatorframework.io.bundle.mediatype.v1=registry+v1
# --&gt; Using cache b67edfbd23d6ba9c3f484a1e01f9da79fbffdc44e913423e2f616e477df372e1
# --&gt; b67edfbd23d
# STEP 3/14: LABEL operators.operatorframework.io.bundle.manifests.v1=manifests/
# --&gt; Using cache f2eef5180d3c9c63f40a98880ec95088b8395845e0f90960a194326d77a6f3b4
# --&gt; f2eef5180d3
# STEP 4/14: LABEL operators.operatorframework.io.bundle.metadata.v1=metadata/
# --&gt; Using cache 6fc10718a71e30d31cc652b47ac27ca87901ff4fda17a25e2d6bc53344e50673
# --&gt; 6fc10718a71
# STEP 5/14: LABEL operators.operatorframework.io.bundle.package.v1=nep-helm-operator
# --&gt; Using cache 6664d1d6c64c0954c18a432194845551e5a0c6f9bba33175d77c8791e2b0f6e0
# --&gt; 6664d1d6c64
# STEP 6/14: LABEL operators.operatorframework.io.bundle.channels.v1=alpha
# --&gt; Using cache 32878b9e903851bb51b6c0635c77112b4244f4ce7e9d8a7b0a0d8cf7fe7bbe0e
# --&gt; 32878b9e903
# STEP 7/14: LABEL operators.operatorframework.io.metrics.builder=operator-sdk-v1.10.1-ocp
# --&gt; Using cache c5482c80a3287494a5f35ee8df782f4499ad6def2aaa55652e5fc57d4dfa8f0d
# --&gt; c5482c80a32
# STEP 8/14: LABEL operators.operatorframework.io.metrics.mediatype.v1=metrics+v1
# --&gt; Using cache 68822f2fae03c5efc8b980882f66e870d8942d80dbf697e3d784c46f95c50437
# --&gt; 68822f2fae0
# STEP 9/14: LABEL operators.operatorframework.io.metrics.project_layout=helm.sdk.operatorframework.io/v1
# --&gt; Using cache a85519d2774008b3071baf6098ec59561102ef1f337acd19b2c7ef739ebae89e
# --&gt; a85519d2774
# STEP 10/14: LABEL operators.operatorframework.io.test.mediatype.v1=scorecard+v1
# --&gt; Using cache 17a1b08e1dca2295f98e3288d592a08636d15d7461e25e11744a499160a1546c
# --&gt; 17a1b08e1dc
# STEP 11/14: LABEL operators.operatorframework.io.test.config.v1=tests/scorecard/
# --&gt; Using cache 9b6a20b0ff75b501a321fe4fbdfd1d284763e65596dc85675f119e5e3de69657
# --&gt; 9b6a20b0ff7
# STEP 12/14: COPY bundle/manifests /manifests/
# --&gt; Using cache ff3aa5b299dae11f464d8ad56f4ae5130974e1cebd0cf273bc03aba11fcb7377
# --&gt; ff3aa5b299d
# STEP 13/14: COPY bundle/metadata /metadata/
# --&gt; Using cache 19395ef3259bbb4e1f5da9616195139698a3ef18e7f904a2a1cd7515cd9829f3
# --&gt; 19395ef3259
# STEP 14/14: COPY bundle/tests/scorecard /tests/scorecard/
# --&gt; Using cache 2268eb0a731f424f70e5b46222a1accd5344560ac9ab609ca3ccb5a4d0cd6669
# COMMIT quay.io/nep/nep-helm-operator:bundle-v01
# --&gt; 2268eb0a731
# Successfully tagged quay.io/nep/nep-helm-operator:bundle-v01
# Successfully tagged quay.io/nep/nep-helm-operator-bundle:v0.0.1
# 2268eb0a731f424f70e5b46222a1accd5344560ac9ab609ca3ccb5a4d0cd6669


make bundle-push BUNDLE_IMG=quay.io/nep/nep-helm-operator:bundle-v01
# make docker-push IMG=quay.io/nep/nep-helm-operator:bundle-v01
# make[1]: Entering directory '/data/helm'
# docker push quay.io/nep/nep-helm-operator:bundle-v01
# Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# Getting image source signatures
# Copying blob 24b54377030e skipped: already exists
# Copying blob 1929cd83db02 skipped: already exists
# Copying blob 44ef63131a17 [--------------------------------------] 0.0b / 0.0b
# Copying config 2268eb0a73 done
# Writing manifest to image destination
# Copying config 2268eb0a73 [--------------------------------------] 0.0b / 3.3KiB
# Writing manifest to image destination
# Storing signatures
# make[1]: Leaving directory '/data/helm'

make catalog-build CATALOG_IMG=quay.io/nep/nep-helm-operator:catalog-v01  BUNDLE_IMG=quay.io/nep/nep-helm-operator:bundle-v01 
# ./bin/opm index add --mode semver --tag quay.io/nep/nep-helm-operator:catalog-v01 --bundles quay.io/nep/nep-helm-operator:bundle-v01
# INFO[0000] building the index                            bundles=&quot;[quay.io/nep/nep-helm-operator:bundle-v01]&quot;
# INFO[0000] resolved name: quay.io/nep/nep-helm-operator:bundle-v01
# INFO[0000] fetched                                       digest=&quot;sha256:1365e5913f05b733124a2a88c3113899db0c42f62b5758477577ef2117aff09f&quot;
# INFO[0000] fetched                                       digest=&quot;sha256:be008c9c2b4f2c031b301174608accb8622c8d843aba2d1af4d053d8b00373c2&quot;
# INFO[0000] fetched                                       digest=&quot;sha256:2268eb0a731f424f70e5b46222a1accd5344560ac9ab609ca3ccb5a4d0cd6669&quot;
# INFO[0000] fetched                                       digest=&quot;sha256:d8e28b323fec2e4de5aecfb46c4ce3e315e20f49b78f43eb7a1d657798695655&quot;
# INFO[0000] fetched                                       digest=&quot;sha256:c19ac761be31fa163ea3da95cb63fc0c2aaca3b316bfb049f6ee36f77522d323&quot;
# INFO[0001] unpacking layer: {application/vnd.docker.image.rootfs.diff.tar.gzip sha256:d8e28b323fec2e4de5aecfb46c4ce3e315e20f49b78f43eb7a1d657798695655 2985 [] map[] &lt;nil&gt;}
# INFO[0001] unpacking layer: {application/vnd.docker.image.rootfs.diff.tar.gzip sha256:c19ac761be31fa163ea3da95cb63fc0c2aaca3b316bfb049f6ee36f77522d323 398 [] map[] &lt;nil&gt;}
# INFO[0001] unpacking layer: {application/vnd.docker.image.rootfs.diff.tar.gzip sha256:be008c9c2b4f2c031b301174608accb8622c8d843aba2d1af4d053d8b00373c2 438 [] map[] &lt;nil&gt;}
# INFO[0001] Could not find optional dependencies file     dir=bundle_tmp582129875 file=bundle_tmp582129875/metadata load=annotations
# INFO[0001] found csv, loading bundle                     dir=bundle_tmp582129875 file=bundle_tmp582129875/manifests load=bundle
# INFO[0001] loading bundle file                           dir=bundle_tmp582129875/manifests file=apps.nep.com_vbbus.yaml load=bundle
# INFO[0001] loading bundle file                           dir=bundle_tmp582129875/manifests file=nep-helm-operator-controller-manager-metrics-service_v1_service.yaml load=bundle
# INFO[0001] loading bundle file                           dir=bundle_tmp582129875/manifests file=nep-helm-operator-manager-config_v1_configmap.yaml load=bundle
# INFO[0001] loading bundle file                           dir=bundle_tmp582129875/manifests file=nep-helm-operator-metrics-reader_rbac.authorization.k8s.io_v1_clusterrole.yaml load=bundle
# INFO[0001] loading bundle file                           dir=bundle_tmp582129875/manifests file=nep-helm-operator.clusterserviceversion.yaml load=bundle
# INFO[0001] Generating dockerfile                         bundles=&quot;[quay.io/nep/nep-helm-operator:bundle-v01]&quot;
# INFO[0001] writing dockerfile: index.Dockerfile322782265  bundles=&quot;[quay.io/nep/nep-helm-operator:bundle-v01]&quot;
# INFO[0001] running podman build                          bundles=&quot;[quay.io/nep/nep-helm-operator:bundle-v01]&quot;
# INFO[0001] [podman build --format docker -f index.Dockerfile322782265 -t quay.io/nep/nep-helm-operator:catalog-v01 .]  bundles=&quot;[quay.io/nep/nep-helm-operator:bundle-v01]&quot;

make catalog-push CATALOG_IMG=quay.io/nep/nep-helm-operator:catalog-v01
# make docker-push IMG=quay.io/nep/nep-helm-operator:catalog-v01
# make[1]: Entering directory '/data/helm'
# docker push quay.io/nep/nep-helm-operator:catalog-v01
# Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.
# Getting image source signatures
# Copying blob 8a20ae5d4166 done
# Copying blob a98a386b6ec2 skipped: already exists
# Copying blob 4e7f383eb531 skipped: already exists
# Copying blob bc276c40b172 skipped: already exists
# Copying blob b15904f6a114 skipped: already exists
# Copying blob 86aadf4df7dc skipped: already exists
# Copying config 5d5d1c219c done
# Writing manifest to image destination
# Storing signatures
# make[1]: Leaving directory '/data/helm'

export OPERATOR_VERION=v04

make docker-build IMG=quay.io/nep/nep-helm-operator:$OPERATOR_VERION

make docker-push IMG=quay.io/nep/nep-helm-operator:$OPERATOR_VERION

make bundle IMG=quay.io/nep/nep-helm-operator:$OPERATOR_VERION

make bundle-build bundle-push catalog-build catalog-push \
    BUNDLE_IMG=quay.io/nep/nep-helm-operator:bundle-$OPERATOR_VERION \
    CATALOG_IMG=quay.io/nep/nep-helm-operator:catalog-$OPERATOR_VERION

# on openshift helper node
cat &lt;&lt; EOF &gt; /data/install/nep.catalog.yaml
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: nep
  namespace: openshift-marketplace
spec:
  displayName: nep
  publisher: nep
  sourceType: grpc
  image: ghcr.io/wangzheng422/nep-helm-operator:catalog-2021-12-03-0504
  updateStrategy:
    registryPoll:
      interval: 10m
EOF
oc create -f /data/install/nep.catalog.yaml
# to restore
oc delete -f /data/install/nep.catalog.yaml

</code></pre>
<h2 id="helm-repository"><a class="header" href="#helm-repository">helm repository</a></h2>
<p>https://medium.com/@mattiaperi/create-a-public-helm-chart-repository-with-github-pages-49b180dbb417</p>
<pre><code class="language-bash"># try to build the repo, and add it into github action
# mkdir -p /data/helm/helm-repo
cd /data/helm/helm-repo

helm package ../helm-charts/*

helm repo index --url https://wangzheng422.github.io/nep-helm-operator/ .

# try to use the repo
helm repo add myhelmrepo https://wangzheng422.github.io/nep-helm-operator/

helm repo list
# NAME            URL
# myhelmrepo      https://wangzheng422.github.io/nep-helm-operator/

helm search repo vbbu
# NAME            CHART VERSION   APP VERSION     DESCRIPTION
# myhelmrepo/vbbu 0.1.0           1.16.0          A Helm chart for Kubernetes

# for ocp, if you are disconnected
cat &lt;&lt; EOF &gt; /data/install/helm.nep.yaml
apiVersion: helm.openshift.io/v1beta1
kind: HelmChartRepository
metadata:
  name: nep-helm-charts-wzh
spec:
 # optional name that might be used by console
  name: nep-helm-charts-wzh
  connectionConfig:
    url: http://nexus.ocp4.redhat.ren:8082/repository/wangzheng422.github.io/
EOF
oc create -f /data/install/helm.nep.yaml

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="metallb-layer2-mode-on-openshift-48"><a class="header" href="#metallb-layer2-mode-on-openshift-48">MetalLB layer2 mode on openshift 4.8</a></h1>
<p>openshift对外提供服务，默认是router的方式，里面是一个haproxy，但是默认只是支持http/https，定制一下，可以支持tcp。这种配置方法不是很直观，特别是tcp的支持也很鸡肋。</p>
<p>我们已经知道metalLB可以帮助service之间暴露external IP，并且通过BGP的方式广播出去，但是在PoC的时候，BGP路由器还是比较难搞，好在metalLB还提供了layer2的方式，更简单的对外暴露external IP.</p>
<p>本次实验部署架构图：
<img src="ocp4/4.8/./dia/4.8.metal.l2.drawio.svg" alt="" /></p>
<h1 id="安装-metallb"><a class="header" href="#安装-metallb">安装 MetalLB</a></h1>
<p>安装MetalLB非常简单</p>
<p>https://metallb.universe.tf/installation/clouds/#metallb-on-openshift-ocp</p>
<pre><code class="language-bash">
mkdir -p /data/install/metallb
cd /data/install/metallb

wget https://raw.githubusercontent.com/metallb/metallb/v0.10.2/manifests/namespace.yaml
wget https://raw.githubusercontent.com/metallb/metallb/v0.10.2/manifests/metallb.yaml

sed -i '/runAsUser: 65534/d' ./metallb.yaml

oc create -f /data/install/metallb/namespace.yaml
oc adm policy add-scc-to-user privileged -n metallb-system -z speaker
oc create -f /data/install/metallb/metallb.yaml

# to restore
oc delete -f /data/install/metallb/metallb.yaml

</code></pre>
<h1 id="配置-metallb"><a class="header" href="#配置-metallb">配置 MetalLB</a></h1>
<pre><code class="language-bash"># on helper
cat &lt;&lt; EOF &gt; /data/install/metal-bgp.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: my-ip-space
      protocol: layer2
      addresses:
        - 192.168.7.150-192.168.7.200
EOF
oc create -f /data/install/metal-bgp.yaml

# to restore
oc delete -f /data/install/metal-bgp.yaml

</code></pre>
<h1 id="创建测试应用"><a class="header" href="#创建测试应用">创建测试应用</a></h1>
<pre><code class="language-bash"># back to helper vm

cat &lt;&lt; EOF &gt; /data/install/demo.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: test-0
  labels:
    env: test
spec:
  restartPolicy: OnFailure
  nodeSelector:
    kubernetes.io/hostname: 'master-0'
  containers:
  - name: php
    image: &quot;quay.io/wangzheng422/php:demo.02&quot;
---
apiVersion: v1
kind: Pod
metadata:
  name: test-1
  labels:
    env: test
spec:
  restartPolicy: OnFailure
  nodeSelector:
    kubernetes.io/hostname: 'worker-0'
  containers:
  - name: php
    image: &quot;quay.io/wangzheng422/php:demo.02&quot;
---
kind: Service
apiVersion: v1
metadata:
  name: demo
spec:
  type: LoadBalancer
  ports:
    - name: &quot;http&quot;
      protocol: TCP
      port: 80
      targetPort: 80
  selector:
    env: test
EOF
oc create -f /data/install/demo.yaml

# to restore
oc delete -f /data/install/demo.yaml

oc get all
# NAME                         READY   STATUS              RESTARTS   AGE
# pod/mypod-787d79b456-4f4xr   1/1     Running             4          4d17h
# pod/test-0                   0/1     ContainerCreating   0          4s
# pod/test-1                   1/1     Running             0          4s

# NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP                            PORT(S)        AGE
# service/demo         LoadBalancer   172.30.178.14   192.168.7.150                          80:30781/TCP   4s
# service/kubernetes   ClusterIP      172.30.0.1      &lt;none&gt;                                 443/TCP        5d16h
# service/openshift    ExternalName   &lt;none&gt;          kubernetes.default.svc.cluster.local   &lt;none&gt;         5d16h

# NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
# deployment.apps/mypod   1/1     1            1           4d17h

# NAME                               DESIRED   CURRENT   READY   AGE
# replicaset.apps/mypod-787d79b456   1         1         1       4d17h

oc get pod -o wide
# NAME                     READY   STATUS    RESTARTS   AGE     IP            NODE       NOMINATED NODE   READINESS GATES
# mypod-787d79b456-4f4xr   1/1     Running   4          4d17h   10.254.1.19   worker-0   &lt;none&gt;           &lt;none&gt;
# test-0                   1/1     Running   0          9m36s   10.254.0.74   master-0   &lt;none&gt;           &lt;none&gt;
# test-1                   1/1     Running   0          9m36s   10.254.1.65   worker-0   &lt;none&gt;           &lt;none&gt;

oc get svc/demo -o yaml
# apiVersion: v1
# kind: Service
# metadata:
#   creationTimestamp: &quot;2021-08-31T06:39:39Z&quot;
#   name: demo
#   namespace: default
#   resourceVersion: &quot;2277414&quot;
#   uid: 6f36e7a4-ee2e-4f86-802e-6053debecfb2
# spec:
#   clusterIP: 172.30.178.14
#   clusterIPs:
#   - 172.30.178.14
#   externalTrafficPolicy: Cluster
#   ipFamilies:
#   - IPv4
#   ipFamilyPolicy: SingleStack
#   ports:
#   - name: http
#     nodePort: 30781
#     port: 80
#     protocol: TCP
#     targetPort: 80
#   selector:
#     env: test
#   sessionAffinity: None
#   type: LoadBalancer
# status:
#   loadBalancer:
#     ingress:
#     - ip: 192.168.7.150

for i in {1..10}
do
   curl 192.168.7.150 &amp;&amp; echo
done
# Hello!&lt;br&gt;Welcome to RedHat Developer&lt;br&gt;Enjoy all of the ad-free articles&lt;br&gt;10.254.1.65
# Hello!&lt;br&gt;Welcome to RedHat Developer&lt;br&gt;Enjoy all of the ad-free articles&lt;br&gt;10.254.1.65
# Hello!&lt;br&gt;Welcome to RedHat Developer&lt;br&gt;Enjoy all of the ad-free articles&lt;br&gt;10.254.1.65
# Hello!&lt;br&gt;Welcome to RedHat Developer&lt;br&gt;Enjoy all of the ad-free articles&lt;br&gt;10.254.1.65
# Hello!&lt;br&gt;Welcome to RedHat Developer&lt;br&gt;Enjoy all of the ad-free articles&lt;br&gt;10.254.0.74
# Hello!&lt;br&gt;Welcome to RedHat Developer&lt;br&gt;Enjoy all of the ad-free articles&lt;br&gt;10.254.1.65
# Hello!&lt;br&gt;Welcome to RedHat Developer&lt;br&gt;Enjoy all of the ad-free articles&lt;br&gt;10.254.0.74
# Hello!&lt;br&gt;Welcome to RedHat Developer&lt;br&gt;Enjoy all of the ad-free articles&lt;br&gt;10.254.1.65
# Hello!&lt;br&gt;Welcome to RedHat Developer&lt;br&gt;Enjoy all of the ad-free articles&lt;br&gt;10.254.0.74
# Hello!&lt;br&gt;Welcome to RedHat Developer&lt;br&gt;Enjoy all of the ad-free articles&lt;br&gt;10.254.1.65

arp -a
# ? (10.88.0.3) at 9a:b9:62:83:0f:75 [ether] on cni-podman0
# master-2.ocp4.redhat.ren (192.168.7.15) at &lt;incomplete&gt; on enp1s0
# ? (10.88.0.2) at 4e:de:d9:d5:f8:f1 [ether] on cni-podman0
# master-1.ocp4.redhat.ren (192.168.7.14) at &lt;incomplete&gt; on enp1s0
# ? (192.168.7.150) at 52:54:00:d2:ba:43 [ether] on enp1s0
# worker-1.ocp4.redhat.ren (192.168.7.17) at &lt;incomplete&gt; on enp1s0
# _gateway (172.21.6.254) at 00:17:94:73:12:c2 [ether] on enp1s0
# master-0.ocp4.redhat.ren (192.168.7.13) at 52:54:00:d2:ba:43 [ether] on enp1s0
# worker-0.ocp4.redhat.ren (192.168.7.16) at 90:b1:1c:44:d6:0f [ether] on enp1s0
# bootstrap.ocp4.redhat.ren (192.168.7.12) at &lt;incomplete&gt; on enp1s0

</code></pre>
<h1 id="到worker-0上看看-nft-规则"><a class="header" href="#到worker-0上看看-nft-规则">到worker-0上，看看 nft 规则</a></h1>
<pre><code class="language-bash"># go to worker-0 to analyze the nat rules
nft list ruleset | grep 192.168.7.150
                # meta l4proto tcp ip daddr 192.168.7.150  tcp dport 80 counter packets 0 bytes 0 jump KUBE-FW-CTBMGJDNUDRWEDVR

nft list ruleset | grep KUBE-FW-CTBMGJDNUDRWEDVR -A 5
#                 meta l4proto tcp ip daddr 192.168.7.150  tcp dport 80 counter packets 0 bytes 0 jump KUBE-FW-CTBMGJDNUDRWEDVR
#                 meta l4proto tcp @nh,96,16 != 2814 ip daddr 172.30.35.8  tcp dport 80 counter packets 0 bytes 0 jump KUBE-MARK-MASQ
#                 meta l4proto tcp ip daddr 172.30.35.8  tcp dport 80 counter packets 0 bytes 0 jump KUBE-SVC-T3U64PSX3UGU57NF
#                 meta l4proto tcp @nh,96,16 != 2814 ip daddr 172.30.152.93  tcp dport 80 counter packets 0 bytes 0 jump KUBE-MARK-MASQ
#                 meta l4proto tcp ip daddr 172.30.152.93  tcp dport 80 counter packets 0 bytes 0 jump KUBE-SVC-ZOXDBRX7A3I2MI4S
#                 meta l4proto tcp @nh,96,16 != 2814 ip daddr 172.30.99.142  tcp dport 8443 counter packets 0 bytes 0 jump KUBE-MARK-MASQ
# --
#         chain KUBE-FW-CTBMGJDNUDRWEDVR {
#                  counter packets 0 bytes 0 jump KUBE-MARK-MASQ
#                  counter packets 0 bytes 0 jump KUBE-SVC-CTBMGJDNUDRWEDVR
#                  counter packets 0 bytes 0 jump KUBE-MARK-DROP
#         }


nft list ruleset | grep KUBE-SVC-CTBMGJDNUDRWEDVR -A 3
#                 meta l4proto tcp ip daddr 172.30.178.14  tcp dport 80 counter packets 0 bytes 0 jump KUBE-SVC-CTBMGJDNUDRWEDVR
#                 meta l4proto tcp ip daddr 192.168.7.150  tcp dport 80 counter packets 0 bytes 0 jump KUBE-FW-CTBMGJDNUDRWEDVR
#                 meta l4proto tcp @nh,96,16 != 2814 ip daddr 172.30.35.8  tcp dport 80 counter packets 0 bytes 0 jump KUBE-MARK-MASQ
#                 meta l4proto tcp ip daddr 172.30.35.8  tcp dport 80 counter packets 0 bytes 0 jump KUBE-SVC-T3U64PSX3UGU57NF
# --
#                 meta l4proto tcp  tcp dport 30781 counter packets 0 bytes 0 jump KUBE-SVC-CTBMGJDNUDRWEDVR
#         }

#         chain KUBE-SVC-HH47JV2DWEPNMQEX {
# --
#         chain KUBE-SVC-CTBMGJDNUDRWEDVR {
#                   counter packets 0 bytes 0 jump KUBE-SEP-CGMBWTJH33MIKSJY
#                  counter packets 0 bytes 0 jump KUBE-SEP-V5VBCVCJRZSWQ4D6
#         }
# --
#                  counter packets 0 bytes 0 jump KUBE-SVC-CTBMGJDNUDRWEDVR
#                  counter packets 0 bytes 0 jump KUBE-MARK-DROP
#         }

nft list ruleset | grep KUBE-SEP-CGMBWTJH33MIKSJY -A 3
#                   counter packets 0 bytes 0 jump KUBE-SEP-CGMBWTJH33MIKSJY
#                  counter packets 0 bytes 0 jump KUBE-SEP-V5VBCVCJRZSWQ4D6
#         }

# --
#         chain KUBE-SEP-CGMBWTJH33MIKSJY {
#                 ip saddr 10.254.0.74  counter packets 0 bytes 0 jump KUBE-MARK-MASQ
#                 meta l4proto tcp   counter packets 0 bytes 0 dnat to 10.254.0.74:80
#         }

nft list ruleset | grep KUBE-SEP-V5VBCVCJRZSWQ4D6 -A 3
#                  counter packets 0 bytes 0 jump KUBE-SEP-V5VBCVCJRZSWQ4D6
#         }

#         chain KUBE-FW-CTBMGJDNUDRWEDVR {
# --
#         chain KUBE-SEP-V5VBCVCJRZSWQ4D6 {
#                 ip saddr 10.254.1.65  counter packets 0 bytes 0 jump KUBE-MARK-MASQ
#                 meta l4proto tcp   counter packets 0 bytes 0 dnat to 10.254.1.65:80
#         }


nft --handle --numeric list ruleset | grep random
                #  counter packets 0 bytes 0 masquerade  random-fully  # handle 13

</code></pre>
<h2 id="看看iptables的规则"><a class="header" href="#看看iptables的规则">看看iptables的规则</a></h2>
<pre><code class="language-bash">iptables -L -v -n -t nat | grep 192.168.7.150
    # 0     0 KUBE-FW-CTBMGJDNUDRWEDVR  tcp  --  *      *       0.0.0.0/0            192.168.7.150        /* default/demo:http loadbalancer IP */ tcp dpt:80

iptables -L -v -n -t nat | grep KUBE-FW-CTBMGJDNUDRWEDVR -A 5
#     0     0 KUBE-FW-CTBMGJDNUDRWEDVR  tcp  --  *      *       0.0.0.0/0            192.168.7.150        /* default/demo:http loadbalancer IP */ tcp dpt:80
#     0     0 KUBE-MARK-MASQ  tcp  --  *      *      !10.254.0.0/16        172.30.210.66        /* openshift-kube-scheduler-operator/metrics:https cluster IP */ tcp dpt:443
#     0     0 KUBE-SVC-HH47JV2DWEPNMQEX  tcp  --  *      *       0.0.0.0/0            172.30.210.66        /* openshift-kube-scheduler-operator/metrics:https cluster IP */ tcp dpt:443
#     0     0 KUBE-MARK-MASQ  tcp  --  *      *      !10.254.0.0/16        172.30.55.237        /* openshift-apiserver-operator/metrics:https cluster IP */ tcp dpt:443
#     0     0 KUBE-SVC-CIUYVLZDADCHPTYT  tcp  --  *      *       0.0.0.0/0            172.30.55.237        /* openshift-apiserver-operator/metrics:https cluster IP */ tcp dpt:443
#     0     0 KUBE-MARK-MASQ  tcp  --  *      *      !10.254.0.0/16        172.30.134.31        /* openshift-pipelines/tekton-pipelines-controller:probes cluster IP */ tcp dpt:8080
# --
# Chain KUBE-FW-CTBMGJDNUDRWEDVR (1 references)
#  pkts bytes target     prot opt in     out     source               destination
#     0     0 KUBE-MARK-MASQ  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/demo:http loadbalancer IP */
#     0     0 KUBE-SVC-CTBMGJDNUDRWEDVR  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/demo:http loadbalancer IP */
#     0     0 KUBE-MARK-DROP  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/demo:http loadbalancer IP */

iptables -L -v -n -t nat | grep KUBE-SVC-CTBMGJDNUDRWEDVR -A 4
#     0     0 KUBE-SVC-CTBMGJDNUDRWEDVR  tcp  --  *      *       0.0.0.0/0            172.30.178.14        /* default/demo:http cluster IP */ tcp dpt:80
#     0     0 KUBE-FW-CTBMGJDNUDRWEDVR  tcp  --  *      *       0.0.0.0/0            192.168.7.150        /* default/demo:http loadbalancer IP */ tcp dpt:80
#     0     0 KUBE-MARK-MASQ  tcp  --  *      *      !10.254.0.0/16        172.30.210.66        /* openshift-kube-scheduler-operator/metrics:https cluster IP */ tcp dpt:443
#     0     0 KUBE-SVC-HH47JV2DWEPNMQEX  tcp  --  *      *       0.0.0.0/0            172.30.210.66        /* openshift-kube-scheduler-operator/metrics:https cluster IP */ tcp dpt:443
#     0     0 KUBE-MARK-MASQ  tcp  --  *      *      !10.254.0.0/16        172.30.55.237        /* openshift-apiserver-operator/metrics:https cluster IP */ tcp dpt:443
# --
#     0     0 KUBE-SVC-CTBMGJDNUDRWEDVR  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/demo:http */ tcp dpt:30781

# Chain KUBE-SVC-HH47JV2DWEPNMQEX (1 references)
#  pkts bytes target     prot opt in     out     source               destination
#     0     0 KUBE-SEP-XIWZUKNCQE6LJCFA  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* openshift-kube-scheduler-operator/metrics:https */
# --
# Chain KUBE-SVC-CTBMGJDNUDRWEDVR (3 references)
#  pkts bytes target     prot opt in     out     source               destination
#     0     0 KUBE-SEP-CGMBWTJH33MIKSJY  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/demo:http */ statistic mode random probability 0.50000000000
#     0     0 KUBE-SEP-V5VBCVCJRZSWQ4D6  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/demo:http */

# --
#     0     0 KUBE-SVC-CTBMGJDNUDRWEDVR  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/demo:http loadbalancer IP */
#     0     0 KUBE-MARK-DROP  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/demo:http loadbalancer IP */

# Chain KUBE-SEP-V5VBCVCJRZSWQ4D6 (1 references)
#  pkts bytes target     prot opt in     out     source               destination

iptables -L -v -n -t nat | grep KUBE-SEP-CGMBWTJH33MIKSJY -A 3
#     0     0 KUBE-SEP-CGMBWTJH33MIKSJY  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/demo:http */ statistic mode random probability 0.50000000000
#     0     0 KUBE-SEP-V5VBCVCJRZSWQ4D6  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/demo:http */

# Chain KUBE-FW-CTBMGJDNUDRWEDVR (1 references)
# --
# Chain KUBE-SEP-CGMBWTJH33MIKSJY (1 references)
#  pkts bytes target     prot opt in     out     source               destination
#     0     0 KUBE-MARK-MASQ  all  --  *      *       10.254.0.74          0.0.0.0/0            /* default/demo:http */
#     0     0 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/demo:http */ tcp to:10.254.0.74:80

iptables -L -v -n -t nat | grep KUBE-SEP-V5VBCVCJRZSWQ4D6 -A 3
#     0     0 KUBE-SEP-V5VBCVCJRZSWQ4D6  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/demo:http */

# Chain KUBE-FW-CTBMGJDNUDRWEDVR (1 references)
#  pkts bytes target     prot opt in     out     source               destination
# --
# Chain KUBE-SEP-V5VBCVCJRZSWQ4D6 (1 references)
#  pkts bytes target     prot opt in     out     source               destination
#     0     0 KUBE-MARK-MASQ  all  --  *      *       10.254.1.65          0.0.0.0/0            /* default/demo:http */
#     0     0 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/demo:http */ tcp to:10.254.1.65:80


</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="metallb-bgp-mode-on-openshift-48"><a class="header" href="#metallb-bgp-mode-on-openshift-48">MetalLB BGP mode on openshift 4.8</a></h1>
<p>openshift对外提供服务，默认是router的方式，里面是一个haproxy，但是默认只是支持http/https，定制一下，可以支持tcp。这种配置方法不是很直观，特别是tcp的支持也很鸡肋。我们希望的方式，是k8s service直接暴露一个对外服务ip，并且通过bgp广播出去。今天，我们就看看metalLB项目如何帮助我们达到这个目的。</p>
<p>本次实验部署架构图：
<img src="ocp4/4.8/./dia/4.8.metal.drawio.svg" alt="" /></p>
<p>视频讲解:</p>
<p><a href="https://www.bilibili.com/video/BV1Sf4y1N7eE/"><kbd><img src="ocp4/4.8/imgs/2021-08-30-22-40-36.png" width="600"></kbd></a></p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1Sf4y1N7eE/">bilibili</a></li>
<li><a href="https://youtu.be/a5Icpvy-vyw">youtube</a></li>
</ul>
<h1 id="安装-metallb-1"><a class="header" href="#安装-metallb-1">安装 MetalLB</a></h1>
<p>安装MetalLB非常简单</p>
<p>https://metallb.universe.tf/installation/clouds/#metallb-on-openshift-ocp</p>
<pre><code class="language-bash">
mkdir -p /data/install/metallb
cd /data/install/metallb

wget https://raw.githubusercontent.com/metallb/metallb/v0.10.2/manifests/namespace.yaml
wget https://raw.githubusercontent.com/metallb/metallb/v0.10.2/manifests/metallb.yaml

sed -i '/runAsUser: 65534/d' ./metallb.yaml

oc create -f namespace.yaml
oc adm policy add-scc-to-user privileged -n metallb-system -z speaker
oc create -f metallb.yaml
</code></pre>
<h1 id="创建路由器"><a class="header" href="#创建路由器">创建路由器</a></h1>
<p>我们用一个 kvm 来模拟 bgp 路由器</p>
<ul>
<li>https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_networking/setting-your-routing-protocols_configuring-and-managing-networking#intro-to-frr_setting-your-routing-protocols</li>
<li>https://www.cisco.com/c/en/us/td/docs/ios-xml/ios/iproute_bgp/configuration/xe-16/irg-xe-16-book/bgp-dynamic-neighbors.html</li>
<li>https://ipbgp.com/2018/02/07/quagga/</li>
<li>https://docs.frrouting.org/en/latest/bgp.html</li>
</ul>
<pre><code class="language-bash"># to setup a router vm for testing
# go to kvm host
cd /data/kvm

wget https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.8/scripts/helper-ks-rocky.cfg

sed -i '0,/^network.*/s/^network.*/network  --bootproto=static --device=enp1s0 --gateway=172.21.6.254 --ip=172.21.6.10  --netmask=255.255.255.0 --nameserver=172.21.1.1  --ipv6=auto --activate/' helper-ks-rocky.cfg

sed -i '0,/^network  --hostname.*/s/^network  --hostname.*/network  --hostname=bgp-router/' helper-ks-rocky.cfg

virt-install --name=&quot;bgp-router&quot; --vcpus=2 --ram=2048 \
--cpu=host-model \
--disk path=/data/nvme/bgp-router.qcow2,bus=virtio,size=30 \
--os-variant rhel8.4 --network bridge=baremetal,model=virtio \
--graphics vnc,port=49000 \
--boot menu=on --location /data/kvm/Rocky-8.4-x86_64-minimal.iso \
--initrd-inject helper-ks-rocky.cfg --extra-args &quot;inst.ks=file:/helper-ks-rocky.cfg&quot; 

# in the bgp-router vm
nmcli con mod enp1s0 +ipv4.addresses &quot;192.168.7.10/24&quot;
nmcli con up enp1s0

systemctl disable --now firewalld

dnf install -y frr

sed -i 's/bgpd=no/bgpd=yes/g' /etc/frr/daemons
systemctl enable --now frr

# 进入路由器配置界面
vtysh
# 以下是 bgp 路由器配置
router bgp 64512
 neighbor metallb peer-group
 neighbor metallb remote-as 64512
 bgp listen limit 200
 bgp listen range 192.168.7.0/24 peer-group metallb
</code></pre>
<h1 id="配置-metallb-和-bgp-router-进行配对"><a class="header" href="#配置-metallb-和-bgp-router-进行配对">配置 MetalLB 和 bgp-router 进行配对</a></h1>
<pre><code class="language-bash"># on helper
cat &lt;&lt; EOF &gt; /data/install/metal-bgp.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    peers:
    - my-asn: 64512
      peer-asn: 64512
      peer-address: 192.168.7.10
    address-pools:
    - name: my-ip-space
      protocol: bgp
      avoid-buggy-ips: true
      addresses:
      - 198.51.100.0/24
EOF
oc create -f /data/install/metal-bgp.yaml

# to restore
oc delete -f /data/install/metal-bgp.yaml
</code></pre>
<h1 id="回到-bgp-router-看看路由情况"><a class="header" href="#回到-bgp-router-看看路由情况">回到 bgp-router 看看路由情况</a></h1>
<pre><code># back to bgp-router vm
vtysh

bgp-router# show ip bgp summary

IPv4 Unicast Summary:
BGP router identifier 192.168.7.10, local AS number 64512 vrf-id 0
BGP table version 0
RIB entries 0, using 0 bytes of memory
Peers 2, using 43 KiB of memory
Peer groups 1, using 64 bytes of memory

Neighbor        V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd   PfxSnt
*192.168.7.13   4      64512         2         2        0    0    0 00:00:25            0        0
*192.168.7.16   4      64512         2         2        0    0    0 00:00:25            0        0

Total number of neighbors 2
* - dynamic neighbor
2 dynamic neighbor(s), limit 200
</code></pre>
<p>我们看到，集群里面的2个node，分别和路由器建立的peer关系。</p>
<h1 id="创建测试应用-1"><a class="header" href="#创建测试应用-1">创建测试应用</a></h1>
<pre><code class="language-bash"># back to helper vm

cat &lt;&lt; EOF &gt; /data/install/demo.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: test-0
  labels:
    env: test
spec:
  restartPolicy: OnFailure
  nodeSelector:
    kubernetes.io/hostname: 'master-0'
  containers:
  - name: php
    image: &quot;quay.io/wangzheng422/php:demo.02&quot;
---
apiVersion: v1
kind: Pod
metadata:
  name: test-1
  labels:
    env: test
spec:
  restartPolicy: OnFailure
  nodeSelector:
    kubernetes.io/hostname: 'worker-0'
  containers:
  - name: php
    image: &quot;quay.io/wangzheng422/php:demo.02&quot;
---
kind: Service
apiVersion: v1
metadata:
  name: demo
spec:
  type: LoadBalancer
  ports:
    - name: &quot;http&quot;
      protocol: TCP
      port: 80
      targetPort: 80
  selector:
    env: test
EOF
oc create -f /data/install/demo.yaml

# to restore
oc delete -f /data/install/demo.yaml

oc get all
# NAME                         READY   STATUS    RESTARTS   AGE
# pod/mypod-787d79b456-4f4xr   1/1     Running   3          3d23h
# pod/test-0                   1/1     Running   0          2m28s
# pod/test-1                   1/1     Running   0          2m28s

# NAME                 TYPE           CLUSTER-IP     EXTERNAL-IP                            PORT(S)        AGE
# service/demo         LoadBalancer   172.30.82.87   198.51.100.1                           80:32203/TCP   2m28s
# service/kubernetes   ClusterIP      172.30.0.1     &lt;none&gt;                                 443/TCP        4d22h
# service/openshift    ExternalName   &lt;none&gt;         kubernetes.default.svc.cluster.local   &lt;none&gt;         4d22h

# NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
# deployment.apps/mypod   1/1     1            1           3d23h

# NAME                               DESIRED   CURRENT   READY   AGE
# replicaset.apps/mypod-787d79b456   1         1         1       3d23h

oc get pod -o wide
# NAME                     READY   STATUS    RESTARTS   AGE     IP             NODE       NOMINATED NODE   READINESS GATES
# mypod-787d79b456-4f4xr   1/1     Running   3          4d      10.254.1.2     worker-0   &lt;none&gt;           &lt;none&gt;
# test-0                   1/1     Running   0          8m38s   10.254.0.66    master-0   &lt;none&gt;           &lt;none&gt;
# test-1                   1/1     Running   0          8m38s   10.254.1.230   worker-0   &lt;none&gt;           &lt;none&gt;

oc get svc/demo -o yaml
# apiVersion: v1
# kind: Service
# metadata:
#   creationTimestamp: &quot;2021-08-30T12:42:21Z&quot;
#   name: demo
#   namespace: default
#   resourceVersion: &quot;2046159&quot;
#   uid: 1af07435-5234-4062-994d-4715453118c6
# spec:
#   clusterIP: 172.30.82.87
#   clusterIPs:
#   - 172.30.82.87
#   externalTrafficPolicy: Cluster
#   ipFamilies:
#   - IPv4
#   ipFamilyPolicy: SingleStack
#   ports:
#   - name: http
#     nodePort: 32203
#     port: 80
#     protocol: TCP
#     targetPort: 80
#   selector:
#     env: test
#   sessionAffinity: None
#   type: LoadBalancer
# status:
#   loadBalancer:
#     ingress:
#     - ip: 198.51.100.1
</code></pre>
<h1 id="回到-bgp-router-看看路由更新情况"><a class="header" href="#回到-bgp-router-看看路由更新情况">回到 bgp-router 看看路由更新情况</a></h1>
<pre><code># back to bgp-router

bgp-router# show ip bgp summary

IPv4 Unicast Summary:
BGP router identifier 192.168.7.10, local AS number 64512 vrf-id 0
BGP table version 1
RIB entries 1, using 192 bytes of memory
Peers 2, using 43 KiB of memory
Peer groups 1, using 64 bytes of memory

Neighbor        V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd   PfxSnt
*192.168.7.13   4      64512        73        72        0    0    0 00:35:16            1        0
*192.168.7.16   4      64512        73        72        0    0    0 00:35:16            1        0

Total number of neighbors 2
* - dynamic neighbor
2 dynamic neighbor(s), limit 200

bgp-router# show ip bgp neighbors 192.168.7.13 routes
BGP table version is 1, local router ID is 192.168.7.10, vrf id 0
Default local pref 100, local AS 64512
Status codes:  s suppressed, d damped, h history, * valid, &gt; best, = multipath,
               i internal, r RIB-failure, S Stale, R Removed
Nexthop codes: @NNN nexthop's vrf id, &lt; announce-nh-self
Origin codes:  i - IGP, e - EGP, ? - incomplete

   Network          Next Hop            Metric LocPrf Weight Path
*&gt;i198.51.100.1/32  192.168.7.13                    0      0 ?

Displayed  1 routes and 2 total paths
bgp-router#
bgp-router# show ip bgp neighbors 192.168.7.16 routes
BGP table version is 1, local router ID is 192.168.7.10, vrf id 0
Default local pref 100, local AS 64512
Status codes:  s suppressed, d damped, h history, * valid, &gt; best, = multipath,
               i internal, r RIB-failure, S Stale, R Removed
Nexthop codes: @NNN nexthop's vrf id, &lt; announce-nh-self
Origin codes:  i - IGP, e - EGP, ? - incomplete

   Network          Next Hop            Metric LocPrf Weight Path
*=i198.51.100.1/32  192.168.7.16                    0      0 ?

Displayed  1 routes and 2 total paths
</code></pre>
<p>在路由器的shell界面上看看</p>
<pre><code class="language-bash">ip r
# default via 172.21.6.254 dev enp1s0 proto static metric 100
# 172.21.6.0/24 dev enp1s0 proto kernel scope link src 172.21.6.10 metric 100
# 192.168.7.0/24 dev enp1s0 proto kernel scope link src 192.168.7.10 metric 100
# 198.51.100.1 proto bgp metric 20
#         nexthop via 192.168.7.13 dev enp1s0 weight 1
#         nexthop via 192.168.7.16 dev enp1s0 weight 1

[root@bgp-router ~]# curl 198.51.100.1 &amp;&amp; echo
Hello!&lt;br&gt;Welcome to RedHat Developer&lt;br&gt;Enjoy all of the ad-free articles&lt;br&gt;10.254.0.66
[root@bgp-router ~]# curl 198.51.100.1 &amp;&amp; echo
Hello!&lt;br&gt;Welcome to RedHat Developer&lt;br&gt;Enjoy all of the ad-free articles&lt;br&gt;10.254.0.66
[root@bgp-router ~]# curl 198.51.100.1 &amp;&amp; echo
Hello!&lt;br&gt;Welcome to RedHat Developer&lt;br&gt;Enjoy all of the ad-free articles&lt;br&gt;10.254.0.66
[root@bgp-router ~]# curl 198.51.100.1 &amp;&amp; echo
Hello!&lt;br&gt;Welcome to RedHat Developer&lt;br&gt;Enjoy all of the ad-free articles&lt;br&gt;10.254.0.66
[root@bgp-router ~]# curl 198.51.100.1 &amp;&amp; echo
Hello!&lt;br&gt;Welcome to RedHat Developer&lt;br&gt;Enjoy all of the ad-free articles&lt;br&gt;10.254.1.230
[root@bgp-router ~]# curl 198.51.100.1 &amp;&amp; echo
Hello!&lt;br&gt;Welcome to RedHat Developer&lt;br&gt;Enjoy all of the ad-free articles&lt;br&gt;10.254.0.66
[root@bgp-router ~]# curl 198.51.100.1 &amp;&amp; echo
Hello!&lt;br&gt;Welcome to RedHat Developer&lt;br&gt;Enjoy all of the ad-free articles&lt;br&gt;10.254.0.66
[root@bgp-router ~]# curl 198.51.100.1 &amp;&amp; echo
Hello!&lt;br&gt;Welcome to RedHat Developer&lt;br&gt;Enjoy all of the ad-free articles&lt;br&gt;10.254.0.66
[root@bgp-router ~]# curl 198.51.100.1 &amp;&amp; echo
Hello!&lt;br&gt;Welcome to RedHat Developer&lt;br&gt;Enjoy all of the ad-free articles&lt;br&gt;10.254.1.230
[root@bgp-router ~]# curl 198.51.100.1 &amp;&amp; echo
Hello!&lt;br&gt;Welcome to RedHat Developer&lt;br&gt;Enjoy all of the ad-free articles&lt;br&gt;10.254.0.66
</code></pre>
<h1 id="到worker-0上看看-nft-规则-1"><a class="header" href="#到worker-0上看看-nft-规则-1">到worker-0上，看看 nft 规则</a></h1>
<pre><code class="language-bash"># go to worker-0 to analyze the nat rules
nft list ruleset | grep 198.51
                # meta l4proto tcp ip daddr 198.51.100.1  tcp dport 80 counter packets 0 bytes 0 jump KUBE-FW-CTBMGJDNUDRWEDVR

nft list ruleset | grep KUBE-FW-CTBMGJDNUDRWEDVR -A 5
#                 meta l4proto tcp ip daddr 198.51.100.1  tcp dport 80 counter packets 0 bytes 0 jump KUBE-FW-CTBMGJDNUDRWEDVR
#                 meta l4proto tcp @nh,96,16 != 2814 ip daddr 172.30.145.124  tcp dport 443 counter packets 0 bytes 0 jump KUBE-MARK-MASQ
#                 meta l4proto tcp ip daddr 172.30.145.124  tcp dport 443 counter packets 0 bytes 0 jump KUBE-SVC-L54HVQEJKTL2PXFK
#                 meta l4proto tcp @nh,96,16 != 2814 ip daddr 172.30.16.253  tcp dport 8443 counter packets 0 bytes 0 jump KUBE-MARK-MASQ
#                 meta l4proto tcp ip daddr 172.30.16.253  tcp dport 8443 counter packets 0 bytes 0 jump KUBE-SVC-YVQ2VVJT4ABSS56R
#                 meta l4proto tcp @nh,96,16 != 2814 ip daddr 172.30.185.119  tcp dport 9091 counter packets 0 bytes 0 jump KUBE-MARK-MASQ
# --
#         chain KUBE-FW-CTBMGJDNUDRWEDVR {
#                  counter packets 0 bytes 0 jump KUBE-MARK-MASQ
#                  counter packets 0 bytes 0 jump KUBE-SVC-CTBMGJDNUDRWEDVR
#                  counter packets 0 bytes 0 jump KUBE-MARK-DROP
#         }


nft list ruleset | grep KUBE-SVC-CTBMGJDNUDRWEDVR -A 3
#                 meta l4proto tcp ip daddr 172.30.82.87  tcp dport 80 counter packets 0 bytes 0 jump KUBE-SVC-CTBMGJDNUDRWEDVR
#                 meta l4proto tcp ip daddr 198.51.100.1  tcp dport 80 counter packets 11 bytes 660 jump KUBE-FW-CTBMGJDNUDRWEDVR
#                 meta l4proto tcp @nh,96,16 != 2814 ip daddr 172.30.145.124  tcp dport 443 counter packets 0 bytes 0 jump KUBE-MARK-MASQ
#                 meta l4proto tcp ip daddr 172.30.145.124  tcp dport 443 counter packets 0 bytes 0 jump KUBE-SVC-L54HVQEJKTL2PXFK
# --
#                 meta l4proto tcp  tcp dport 32203 counter packets 0 bytes 0 jump KUBE-SVC-CTBMGJDNUDRWEDVR
#         }

#         chain KUBE-SVC-DCLNKYLNAMROIJRV {
# --
#         chain KUBE-SVC-CTBMGJDNUDRWEDVR {
#                   counter packets 9 bytes 540 jump KUBE-SEP-BKD3LMWAJNKW5GNU
#                  counter packets 2 bytes 120 jump KUBE-SEP-M5WVBCWAFJ2J2M2U
#         }
# --
#                  counter packets 11 bytes 660 jump KUBE-SVC-CTBMGJDNUDRWEDVR
#                  counter packets 0 bytes 0 jump KUBE-MARK-DROP
#         }

nft list ruleset | grep KUBE-SEP-BKD3LMWAJNKW5GNU -A 3
#                   counter packets 9 bytes 540 jump KUBE-SEP-BKD3LMWAJNKW5GNU
#                  counter packets 2 bytes 120 jump KUBE-SEP-M5WVBCWAFJ2J2M2U
#         }

# --
#         chain KUBE-SEP-BKD3LMWAJNKW5GNU {
#                 ip saddr 10.254.0.66  counter packets 0 bytes 0 jump KUBE-MARK-MASQ
#                 meta l4proto tcp   counter packets 9 bytes 540 dnat to 10.254.0.66:80
#         }

nft list ruleset | grep KUBE-SEP-M5WVBCWAFJ2J2M2U -A 3
#                  counter packets 2 bytes 120 jump KUBE-SEP-M5WVBCWAFJ2J2M2U
#         }

#         chain KUBE-FW-CTBMGJDNUDRWEDVR {
# --
#         chain KUBE-SEP-M5WVBCWAFJ2J2M2U {
#                 ip saddr 10.254.1.230  counter packets 0 bytes 0 jump KUBE-MARK-MASQ
#                 meta l4proto tcp   counter packets 2 bytes 120 dnat to 10.254.1.230:80
#         }

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="kata--sandbox-container-in-openshift-48"><a class="header" href="#kata--sandbox-container-in-openshift-48">Kata / sandbox container in openshift 4.8</a></h1>
<p>红帽 openshift 4.8 容器平台，最新支持了kata，或者叫沙盒容器, 是在物理机上启动vm，然后在vm里面启动容器进程的技术，初衷是为了进一步提高安全性，消除用户对容器是否存在逃逸问题的顾虑，虽然还是TP阶段，但是已经可以一探究竟啦。</p>
<p>https://docs.openshift.com/container-platform/4.8/sandboxed_containers/understanding-sandboxed-containers.html</p>
<p>视频讲解:</p>
<p><a href="https://www.bilibili.com/video/BV1Mg411V7ZX/"><kbd><img src="ocp4/4.8/imgs/2021-08-29-15-02-56.png" width="600"></kbd></a></p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1Mg411V7ZX/">bilibili</a></li>
<li><a href="https://youtu.be/ygvpGZXnIRs">youtube</a></li>
</ul>
<p>首先我们来安装它，在operator hub里面选择sandbox container，点击安装。
<img src="ocp4/4.8/imgs/2021-08-26-13-07-15.png" alt="" /></p>
<p>然后在operator里面创建一个kata config，默认就可以，现在是TP阶段，也没什么花活。
<img src="ocp4/4.8/imgs/2021-08-26-13-13-26.png" alt="" /></p>
<p>创建好了以后，kata operator就会在系统里面创建一些配置，我们来一个一个看一下。</p>
<pre><code class="language-bash"># 首先是runtime class，这个是指出了pod可以使用kata作为runtime, 
# 注意礼貌的overhead，这个配置的意思，是kata有qemu作为虚拟机，所以会有一些额外的消耗，
# 这些消耗在scheduling的时候，需要计算，这里就把这个计算量静态的配置进去。。。
# 虽然我觉得这个不太灵活，但是目前就是这样的。
oc get runtimeclass/kata -o yaml
# apiVersion: node.k8s.io/v1
# handler: kata
# kind: RuntimeClass
# metadata:
#   name: kata
# overhead:
#   podFixed:
#     cpu: 250m
#     memory: 350Mi
# scheduling:
#   nodeSelector:
#     node-role.kubernetes.io/worker: &quot;&quot;

# ocp会把kata通过machine config的方式，配置到节点里面去
oc get mc
# NAME                                               GENERATEDBYCONTROLLER                      IGNITIONVERSION   AGE
# 00-master                                          723a8a4992f42530af95202e51e5a940d2a3d169   3.2.0             15h
# 00-worker                                          723a8a4992f42530af95202e51e5a940d2a3d169   3.2.0             15h
# 01-master-container-runtime                        723a8a4992f42530af95202e51e5a940d2a3d169   3.2.0             15h
# 01-master-kubelet                                  723a8a4992f42530af95202e51e5a940d2a3d169   3.2.0             15h
# 01-worker-container-runtime                        723a8a4992f42530af95202e51e5a940d2a3d169   3.2.0             15h
# 01-worker-kubelet                                  723a8a4992f42530af95202e51e5a940d2a3d169   3.2.0             15h
# 50-enable-sandboxed-containers-extension                                                      3.2.0             51m
# 99-master-chrony-configuration                                                                2.2.0             15h
# 99-master-container-registries                                                                3.1.0             15h
# 99-master-generated-registries                     723a8a4992f42530af95202e51e5a940d2a3d169   3.2.0             15h
# 99-master-ssh                                                                                 3.2.0             15h
# 99-worker-chrony-configuration                                                                2.2.0             15h
# 99-worker-container-registries                                                                3.1.0             15h
# 99-worker-generated-registries                     723a8a4992f42530af95202e51e5a940d2a3d169   3.2.0             15h
# 99-worker-ssh                                                                                 3.2.0             15h
# rendered-master-8c1e34a69aa4b919b6f2eec350570491   723a8a4992f42530af95202e51e5a940d2a3d169   3.2.0             15h
# rendered-worker-4afd90ddf39588aae385def4519e8da9   723a8a4992f42530af95202e51e5a940d2a3d169   3.2.0             51m
# rendered-worker-5abff4814eef2f9bc7535e5cbb10564c   723a8a4992f42530af95202e51e5a940d2a3d169   3.2.0             15h

# 那这个machine config里面是什么呢？我们看一看
# 原来是加了一个extension, 
# 经过查看源代码，这个sandboxed-containers extension就是对应了kata-containers rpm
oc get mc/50-enable-sandboxed-containers-extension -o yaml
# apiVersion: machineconfiguration.openshift.io/v1
# kind: MachineConfig
# metadata:
#   labels:
#     app: example-kataconfig
#     machineconfiguration.openshift.io/role: worker
#   name: 50-enable-sandboxed-containers-extension
# spec:
#   config:
#     ignition:
#       version: 3.2.0
#   extensions:
#   - sandboxed-containers

# 我们到worker-0上看看，发现确实是安装了一个新的kata-containers rpm
rpm-ostree status
# State: idle
# Deployments:
# ● pivot://quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6ddc94ab09a4807ea3d1f29a922fe15f0b4ee863529258c486a04e7fb7b95a4b
#               CustomOrigin: Managed by machine-config-operator
#                    Version: 48.84.202108161759-0 (2021-08-16T18:03:02Z)
#            LayeredPackages: kata-containers

#   pivot://quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6ddc94ab09a4807ea3d1f29a922fe15f0b4ee863529258c486a04e7fb7b95a4b
#               CustomOrigin: Managed by machine-config-operator
#                    Version: 48.84.202108161759-0 (2021-08-16T18:03:02Z)

# 我们看看这个kata-containers rpm里面都提供了什么文件
rpm -ql kata-containers
# /etc/crio/crio.conf.d/50-kata
# /usr/bin/containerd-shim-kata-v2
# /usr/bin/kata-collect-data.sh
# /usr/bin/kata-monitor
# /usr/bin/kata-runtime
# /usr/lib/.build-id
# /usr/lib/.build-id/0f
# /usr/lib/.build-id/0f/dc6751937c4b54a2e10ed431f7969bfd85d2d7
# /usr/lib/.build-id/5e
# /usr/lib/.build-id/5e/ad1e1eca5ab8111a23bf094caf6acbd3b9d7af
# /usr/lib/.build-id/67
# /usr/lib/.build-id/67/e5107c68c0e147f24f6e8f4e96104564b8f223
# /usr/lib/.build-id/be
# /usr/lib/.build-id/be/0add7df48b5f06a305e95497355666a1e04e39
# /usr/lib/systemd/system/kata-osbuilder-generate.service
# /usr/libexec/kata-containers
# /usr/libexec/kata-containers/VERSION
# /usr/libexec/kata-containers/agent
# /usr/libexec/kata-containers/agent/usr
# /usr/libexec/kata-containers/agent/usr/bin
# /usr/libexec/kata-containers/agent/usr/bin/kata-agent
# /usr/libexec/kata-containers/agent/usr/lib
# /usr/libexec/kata-containers/agent/usr/lib/systemd
# /usr/libexec/kata-containers/agent/usr/lib/systemd/system
# /usr/libexec/kata-containers/agent/usr/lib/systemd/system/kata-agent.service
# /usr/libexec/kata-containers/agent/usr/lib/systemd/system/kata-containers.target
# /usr/libexec/kata-containers/kata-netmon
# /usr/libexec/kata-containers/osbuilder
# /usr/libexec/kata-containers/osbuilder/dracut
# /usr/libexec/kata-containers/osbuilder/dracut/dracut.conf.d
# /usr/libexec/kata-containers/osbuilder/dracut/dracut.conf.d/05-base.conf
# /usr/libexec/kata-containers/osbuilder/dracut/dracut.conf.d/15-dracut-rhel.conf
# /usr/libexec/kata-containers/osbuilder/initrd-builder
# /usr/libexec/kata-containers/osbuilder/initrd-builder/README.md
# /usr/libexec/kata-containers/osbuilder/initrd-builder/initrd_builder.sh
# /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh
# /usr/libexec/kata-containers/osbuilder/nsdax
# /usr/libexec/kata-containers/osbuilder/rootfs-builder
# /usr/libexec/kata-containers/osbuilder/rootfs-builder/README.md
# /usr/libexec/kata-containers/osbuilder/rootfs-builder/rootfs.sh
# /usr/libexec/kata-containers/osbuilder/scripts
# /usr/libexec/kata-containers/osbuilder/scripts/lib.sh
# /usr/share/bash-completion/completions/kata-runtime
# /usr/share/doc/kata-containers
# /usr/share/doc/kata-containers/CONTRIBUTING.md
# /usr/share/doc/kata-containers/README.md
# /usr/share/kata-containers
# /usr/share/kata-containers/defaults
# /usr/share/kata-containers/defaults/configuration.toml
# /usr/share/licenses/kata-containers
# /usr/share/licenses/kata-containers/LICENSE
# /var/cache/kata-containers

# 我们看看kata-containers 使用的虚拟机镜像
ls -Rl /var/cache/kata-containers
# /var/cache/kata-containers:
# total 0
# lrwxrwxrwx. 1 root root 121 Aug 26 05:22 kata-containers-initrd.img -&gt; '/var/cache/kata-containers/osbuilder-images/4.18.0-305.12.1.el8_4.x86_64/&quot;rhcos&quot;-kata-4.18.0-305.12.1.el8_4.x86_64.initrd'
# drwxr-xr-x. 3 root root  42 Aug 26 05:22 osbuilder-images
# lrwxrwxrwx. 1 root root  50 Aug 26 05:22 vmlinuz.container -&gt; /lib/modules/4.18.0-305.12.1.el8_4.x86_64//vmlinuz

# /var/cache/kata-containers/osbuilder-images:
# total 0
# drwxr-xr-x. 2 root root 62 Aug 26 05:22 4.18.0-305.12.1.el8_4.x86_64

# /var/cache/kata-containers/osbuilder-images/4.18.0-305.12.1.el8_4.x86_64:
# total 19224
# -rw-r--r--. 1 root root 19682871 Aug 26 05:22 '&quot;rhcos&quot;-kata-4.18.0-305.12.1.el8_4.x86_64.initrd'

# 我们看看kata和crio的结合点，就是crios的配置文件里面
cat /etc/crio/crio.conf.d/50-kata
# [crio.runtime.runtimes.kata]
#   runtime_path = &quot;/usr/bin/containerd-shim-kata-v2&quot;
#   runtime_type = &quot;vm&quot;
#   runtime_root = &quot;/run/vc&quot;
#   privileged_without_host_devices = true

# 我们能看到，系统启动的时候，会根据当前操作系统，编译一个kata使用的虚拟机镜像。
# 后面如果项目上有需要，可以在这个步骤上，做定制，做一个客户需要的虚拟机镜像。
systemctl cat kata-osbuilder-generate.service
# # /usr/lib/systemd/system/kata-osbuilder-generate.service
# [Unit]
# Description=Generate Kata appliance image for host kernel

# [Service]
# Type=oneshot
# ExecStart=/usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh -c
# ExecReload=/usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh

# [Install]
# WantedBy=kubelet.service

# 我们来搞一个pod，测试一下。
cat &lt;&lt; EOF &gt; /data/install/kata.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mypod
  labels:
    app: mypod
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mypod
  template:
    metadata:
      labels:
        app: mypod
    spec:
      runtimeClassName: kata
      containers:
      - name: mypod
        image: quay.io/wangzheng422/qimgs:centos7-test
        command:
          - sleep
          - infinity
EOF
oc create -f /data/install/kata.yaml

# to restore
oc delete -f /data/install/kata.yaml

# 到worker-0上，可以看到qemu进程。
ps aufx ww | grep qemu
# root       99994  0.0  0.0  12816  1076 pts/0    S+   06:22   0:00                      \_ grep --color=auto qemu
# root       93561  1.3  0.9 2466300 326724 ?      Sl   06:19   0:03 /usr/libexec/qemu-kiwi -name sandbox-42f003b365352a71ab87e8a1f49b1c301b6c3c856ec5520b4986aa8b9e43151f -uuid 1cd86e5c-3f86-45e8-bce2-96b16dce635a -machine q35,accel=kvm,kernel_irqchip -cpu host,pmu=off -qmp unix:/run/vc/vm/42f003b365352a71ab87e8a1f49b1c301b6c3c856ec5520b4986aa8b9e43151f/qmp.sock,server=on,wait=off -m 2048M,slots=10,maxmem=33122M -device pci-bridge,bus=pcie.0,id=pci-bridge-0,chassis_nr=1,shpc=on,addr=2 -device virtio-serial-pci,disable-modern=false,id=serial0 -device virtconsole,chardev=charconsole0,id=console0 -chardev socket,id=charconsole0,path=/run/vc/vm/42f003b365352a71ab87e8a1f49b1c301b6c3c856ec5520b4986aa8b9e43151f/console.sock,server=on,wait=off -device virtio-scsi-pci,id=scsi0,disable-modern=false -object rng-random,id=rng0,filename=/dev/urandom -device virtio-rng-pci,rng=rng0 -device vhost-vsock-pci,disable-modern=false,vhostfd=3,id=vsock-976011602,guest-cid=976011602 -chardev socket,id=char-b4b86634faff36bb,path=/run/vc/vm/42f003b365352a71ab87e8a1f49b1c301b6c3c856ec5520b4986aa8b9e43151f/vhost-fs.sock -device vhost-user-fs-pci,chardev=char-b4b86634faff36bb,tag=kataShared -netdev tap,id=network-0,vhost=on,vhostfds=4,fds=5 -device driver=virtio-net-pci,netdev=network-0,mac=0a:58:0a:fe:01:1a,disable-modern=false,mq=on,vectors=4 -rtc base=utc,driftfix=slew,clock=host -global kvm-pit.lost_tick_policy=discard -vga none -no-user-config -nodefaults -nographic --no-reboot -daemonize -object memory-backend-file,id=dimm1,size=2048M,mem-path=/dev/shm,share=on -numa node,memdev=dimm1 -kernel /usr/lib/modules/4.18.0-305.12.1.el8_4.x86_64/vmlinuz -initrd /var/cache/kata-containers/osbuilder-images/4.18.0-305.12.1.el8_4.x86_64/&quot;rhcos&quot;-kata-4.18.0-305.12.1.el8_4.x86_64.initrd -append tsc=reliable no_timer_check rcupdate.rcu_expedited=1 i8042.direct=1 i8042.dumbkbd=1 i8042.nopnp=1 i8042.noaux=1 noreplace-smp reboot=k console=hvc0 console=hvc1 cryptomgr.notests net.ifnames=0 pci=lastbus=0 quiet panic=1 nr_cpus=24 scsi_mod.scan=none -pidfile /run/vc/vm/42f003b365352a71ab87e8a1f49b1c301b6c3c856ec5520b4986aa8b9e43151f/pid -smp 1,cores=1,threads=1,sockets=24,maxcpus=24

# 我们很好奇kata的详细配置，那么我们看看kata的配置文件在哪里
kata-runtime --show-default-config-paths
# /etc/kata-containers/configuration.toml
# /usr/share/kata-containers/defaults/configuration.toml

# 我们看看kata的配置文件内容
cat /usr/share/kata-containers/defaults/configuration.toml
</code></pre>
<p><a href="ocp4/4.8/./files/configuration.toml">result check here</a></p>
<pre><code class="language-bash"># 我们看看kata runtime感知到的配置内容
kata-runtime env
# [Meta]
#   Version = &quot;1.0.25&quot;

# [Runtime]
#   Debug = false
#   Trace = false
#   DisableGuestSeccomp = true
#   DisableNewNetNs = false
#   SandboxCgroupOnly = true
#   Path = &quot;/usr/bin/kata-runtime&quot;
#   [Runtime.Version]
#     OCI = &quot;1.0.1-dev&quot;
#     [Runtime.Version.Version]
#       Semver = &quot;2.1.0&quot;
#       Major = 2
#       Minor = 1
#       Patch = 0
#       Commit = &quot;fa7b9408555e863d0f36f7d0640134069b0c70c8&quot;
#   [Runtime.Config]
#     Path = &quot;/usr/share/kata-containers/defaults/configuration.toml&quot;

# [Hypervisor]
#   MachineType = &quot;q35&quot;
#   Version = &quot;QEMU emulator version 5.2.0 (qemu-kvm-5.2.0-16.module+el8.4.0+11536+725e25d9.2)\nCopyright (c) 2003-2020 Fabrice Bellard and the QEMU Project developers&quot;
#   Path = &quot;/usr/libexec/qemu-kiwi&quot;
#   BlockDeviceDriver = &quot;virtio-scsi&quot;
#   EntropySource = &quot;/dev/urandom&quot;
#   SharedFS = &quot;virtio-fs&quot;
#   VirtioFSDaemon = &quot;/usr/libexec/virtiofsd&quot;
#   Msize9p = 8192
#   MemorySlots = 10
#   PCIeRootPort = 0
#   HotplugVFIOOnRootBus = false
#   Debug = false

# [Image]
#   Path = &quot;&quot;

# [Kernel]
#   Path = &quot;/usr/lib/modules/4.18.0-305.12.1.el8_4.x86_64/vmlinuz&quot;
#   Parameters = &quot;scsi_mod.scan=none&quot;

# [Initrd]
#   Path = &quot;/var/cache/kata-containers/osbuilder-images/4.18.0-305.12.1.el8_4.x86_64/\&quot;rhcos\&quot;-kata-4.18.0-305.12.1.el8_4.x86_64.initrd&quot;

# [Agent]
#   Debug = false
#   Trace = false
#   TraceMode = &quot;&quot;
#   TraceType = &quot;&quot;

# [Host]
#   Kernel = &quot;4.18.0-305.12.1.el8_4.x86_64&quot;
#   Architecture = &quot;amd64&quot;
#   VMContainerCapable = true
#   SupportVSocks = true
#   [Host.Distro]
#     Name = &quot;Red Hat Enterprise Linux CoreOS&quot;
#     Version = &quot;4.8&quot;
#   [Host.CPU]
#     Vendor = &quot;GenuineIntel&quot;
#     Model = &quot;Intel(R) Xeon(R) CPU E5-2620 v2 @ 2.10GHz&quot;
#     CPUs = 24
#   [Host.Memory]
#     Total = 32868716
#     Free = 27704960
#     Available = 29880404

# [Netmon]
#   Path = &quot;/usr/libexec/kata-containers/kata-netmon&quot;
#   Debug = false
#   Enable = false
#   [Netmon.Version]
#     Semver = &quot;2.1.0&quot;
#     Major = 2
#     Minor = 1
#     Patch = 0
#     Commit = &quot;&lt;&lt;unknown&gt;&gt;&quot;

# 我们看看这个构建kata虚拟机镜像的脚本
cat /usr/libexec/kata-containers/osbuilder/kata-osbuilder.sh
</code></pre>
<p><a href="ocp4/4.8/./files/kata-osbuilder.sh">result check here</a></p>
<h1 id="try-to-debug"><a class="header" href="#try-to-debug">try to debug</a></h1>
<pre><code class="language-bash"># try to debug
# 为了能进入到kata虚拟机内部，我们需要修改一下kata的配置文件，激活debug console
mkdir -p /etc/kata-containers/
install -o root -g root -m 0640 /usr/share/kata-containers/defaults/configuration.toml /etc/kata-containers
sed -i -e 's/^# *\(debug_console_enabled\).*=.*$/\1 = true/g' /etc/kata-containers/configuration.toml

# 然后重启pod，我们就能直接连进去kata虚拟机了。
# ps -ef | grep qemu-kiwi | sed 's/.* sandbox-\([^ ]*\) .*/\1/p' | grep -v qemu-kiwi
KATA_PID=`ps -ef | grep qemu-kiwi | sed 's/.* sandbox-\([^ ]*\) .*/\1/g' | grep -v qemu-kiwi`
kata-runtime exec $KATA_PID
</code></pre>
<p>in the kata vm</p>
<pre><code># 虚拟机里面，是个超级简化的系统，命令奇缺
bash-4.4# cd /etc

# ls都没有，只能echo * 代替。
bash-4.4# echo *
chrony.conf cmdline.d conf.d group ld.so.cache ld.so.conf ld.so.conf.d machine-id modules-load.d passwd resolv.conf systemd udev virc

# 可以看到，操作系统和宿主机一样，因为启动的时候，用宿主机的内核构建出来的
bash-4.4# uname -a
Linux mypod-787d79b456-4f4xr 4.18.0-305.12.1.el8_4.x86_64 #1 SMP Mon Jul 26 08:06:24 EDT 2021 x86_64 x86_64 x86_64 GNU/Linux

# 看看激活了什么内核模块
bash-4.4# lsmod
Module                  Size  Used by
mcryptd                16384  0
virtio_blk             20480  0
virtio_console         36864  0
virtio_net             53248  0
net_failover           24576  1 virtio_net
sg                     40960  0
virtio_scsi            20480  0
virtiofs               28672  1
failover               16384  1 net_failover
vmw_vsock_virtio_transport    16384  2
vmw_vsock_virtio_transport_common    32768  1 vmw_vsock_virtio_transport
vsock                  45056  10 vmw_vsock_virtio_transport_common,vmw_vsock_virtio_transport
fuse                  151552  1 virtiofs

# 看看挂载了什么分区
bash-4.4# mount
rootfs on / type rootfs (rw,size=964048k,nr_inodes=241012)
sysfs on /sys type sysfs (rw,nosuid,nodev,noexec,relatime)
proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)
devtmpfs on /dev type devtmpfs (rw,nosuid,size=964064k,nr_inodes=241016,mode=755)
securityfs on /sys/kernel/security type securityfs (rw,nosuid,nodev,noexec,relatime)
selinuxfs on /sys/fs/selinux type selinuxfs (rw,relatime)
tmpfs on /dev/shm type tmpfs (rw,nosuid,nodev)
devpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000)
tmpfs on /run type tmpfs (rw,nosuid,nodev,mode=755)
tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)
cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)
pstore on /sys/fs/pstore type pstore (rw,nosuid,nodev,noexec,relatime)
bpf on /sys/fs/bpf type bpf (rw,nosuid,nodev,noexec,relatime,mode=700)
cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)
cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)
cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)
cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)
cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)
cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)
cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)
cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)
cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)
cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)
cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,rdma)
tmpfs on /tmp type tmpfs (rw,nosuid,nodev)
configfs on /sys/kernel/config type configfs (rw,relatime)
nsfs on /run/sandbox-ns/ipc type nsfs (rw)
nsfs on /run/sandbox-ns/uts type nsfs (rw)
kataShared on /run/kata-containers/shared/containers type virtiofs (rw,relatime)
shm on /run/kata-containers/sandbox/shm type tmpfs (rw,relatime)
tmpfs on /etc/resolv.conf type tmpfs (rw,nosuid,nodev,mode=755)
kataShared on /run/kata-containers/8330bf4c2a98360975ce16244af81c4a5dfa74d4ea3c8a520d9244f0c14e541b/rootfs type virtiofs (rw,relatime)
kataShared on /run/kata-containers/bc201bf92ec8dcad3435ff4191912a41efb64a1e0fb463ad4a651b4dea94a8a5/rootfs type virtiofs (rw,relatime)
b

# 看看都有什么进程
bash-4.4# ps efx ww
    PID TTY      STAT   TIME COMMAND
      2 ?        S      0:00 [kthreadd]
      3 ?        I&lt;     0:00  \_ [rcu_gp]
      4 ?        I&lt;     0:00  \_ [rcu_par_gp]
      6 ?        I&lt;     0:00  \_ [kworker/0:0H-events_highpri]
      7 ?        I      0:00  \_ [kworker/0:1-virtio_vsock]
      8 ?        I      0:00  \_ [kworker/u48:0-events_unbound]
      9 ?        I&lt;     0:00  \_ [mm_percpu_wq]
     10 ?        S      0:00  \_ [ksoftirqd/0]
     11 ?        I      0:00  \_ [rcu_sched]
     12 ?        S      0:00  \_ [migration/0]
     13 ?        S      0:00  \_ [watchdog/0]
     14 ?        S      0:00  \_ [cpuhp/0]
     16 ?        S      0:00  \_ [kdevtmpfs]
     17 ?        I&lt;     0:00  \_ [netns]
     18 ?        S      0:00  \_ [kauditd]
     19 ?        S      0:00  \_ [khungtaskd]
     20 ?        S      0:00  \_ [oom_reaper]
     21 ?        I&lt;     0:00  \_ [writeback]
     22 ?        S      0:00  \_ [kcompactd0]
     23 ?        SN     0:00  \_ [ksmd]
     24 ?        SN     0:00  \_ [khugepaged]
     25 ?        I&lt;     0:00  \_ [crypto]
     26 ?        I&lt;     0:00  \_ [kintegrityd]
     27 ?        I&lt;     0:00  \_ [kblockd]
     28 ?        I&lt;     0:00  \_ [blkcg_punt_bio]
     29 ?        I&lt;     0:00  \_ [tpm_dev_wq]
     30 ?        I&lt;     0:00  \_ [md]
     31 ?        I&lt;     0:00  \_ [edac-poller]
     32 ?        S      0:00  \_ [watchdogd]
     33 ?        I&lt;     0:00  \_ [kworker/0:1H]
     35 ?        I      0:00  \_ [kworker/u48:1]
     49 ?        S      0:00  \_ [kswapd0]
    132 ?        I&lt;     0:00  \_ [kthrotld]
    133 ?        I&lt;     0:00  \_ [acpi_thermal_pm]
    134 ?        S      0:00  \_ [hwrng]
    135 ?        I&lt;     0:00  \_ [kmpath_rdacd]
    136 ?        I&lt;     0:00  \_ [kaluad]
    137 ?        I&lt;     0:00  \_ [ipv6_addrconf]
    138 ?        I&lt;     0:00  \_ [kstrp]
    203 ?        I      0:00  \_ [kworker/0:3-mm_percpu_wq]
    206 ?        S      0:00  \_ [scsi_eh_0]
    207 ?        I&lt;     0:00  \_ [scsi_tmf_0]
    218 ?        S      0:00  \_ [khvcd]
      1 ?        Ss     0:00 /init HOME=/ TERM=linux
    193 ?        Ss     0:00 /usr/lib/systemd/systemd-journald PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin NOTIFY_SOCKET=/run/systemd/notify LISTEN_PID=193 LISTEN_FDS=3 LISTEN_FDNAMES=systemd-journald-dev-log.socket:systemd-journald.socket:systemd-journald.socket WATCHDOG_PID=193 WATCHDOG_USEC=180000000 INVOCATION_ID=00385279d7314bf5a02002d5f1e33050
    201 ?        Ss     0:00 /usr/lib/systemd/systemd-udevd PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin NOTIFY_SOCKET=/run/systemd/notify LISTEN_PID=201 LISTEN_FDS=2 LISTEN_FDNAMES=systemd-udevd-kernel.socket:systemd-udevd-control.socket WATCHDOG_PID=201 WATCHDOG_USEC=180000000 INVOCATION_ID=b3e4a3cd29b34c91a192bc9527da10cf JOURNAL_STREAM=9:10719
    225 ?        Ssl    0:02 /usr/bin/kata-agent PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin INVOCATION_ID=5683abfd11c542fe98c5f7ece1afa599 TERM=vt220
    231 ?        S      0:00  \_ /usr/bin/pod PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin TERM=xterm HOME=/root
    235 ?        S      0:00  \_ sleep infinity PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin TERM=xterm HOSTNAME=mypod-787d79b456-4f4xr NSS_SDB_USE_CACHE=no KUBERNETES_SERVICE_HOST=172.30.0.1 KUBERNETES_SERVICE_PORT=443 KUBERNETES_SERVICE_PORT_HTTPS=443 KUBERNETES_PORT=tcp://172.30.0.1:443 KUBERNETES_PORT_443_TCP=tcp://172.30.0.1:443 KUBERNETES_PORT_443_TCP_PROTO=tcp KUBERNETES_PORT_443_TCP_PORT=443 KUBERNETES_PORT_443_TCP_ADDR=172.30.0.1 HOME=/root
    236 pts/0    Ss     0:00  \_ [bash] PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin INVOCATION_ID=5683abfd11c542fe98c5f7ece1afa599 TERM=vt220 RUST_BACKTRACE=full
    268 pts/0    R+     0:00  |   \_ ps efx ww RUST_BACKTRACE=full INVOCATION_ID=5683abfd11c542fe98c5f7ece1afa599 PWD=/proc/net TERM=vt220 SHLVL=1 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin OLDPWD=/proc _=/usr/bin/ps
    247 pts/1    Ss+    0:00  \_ /bin/sh TERM=screen-256color HOSTNAME=mypod-787d79b456-4f4xr KUBERNETES_PORT_443_TCP_PORT=443 KUBERNETES_PORT=tcp://172.30.0.1:443 KUBERNETES_SERVICE_PORT=443 KUBERNETES_SERVICE_HOST=172.30.0.1 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin PWD=/ SHLVL=1 HOME=/root KUBERNETES_PORT_443_TCP_PROTO=tcp KUBERNETES_SERVICE_PORT_HTTPS=443 NSS_SDB_USE_CACHE=no KUBERNETES_PORT_443_TCP_ADDR=172.30.0.1 KUBERNETES_PORT_443_TCP=tcp://172.30.0.1:443 _=/bin/sh

# 看看有多少内存
bash-4.4# free -h
              total        used        free      shared  buff/cache   available
Mem:          1.9Gi        30Mi       1.8Gi        58Mi        72Mi       1.7Gi
Swap:            0B          0B          0B

# 看看内核启动参数
bash-4.4# cat cmdline
tsc=reliable no_timer_check rcupdate.rcu_expedited=1 i8042.direct=1 i8042.dumbkbd=1 i8042.nopnp=1 i8042.noaux=1 noreplace-smp reboot=k console=hvc0 console=hvc1 cryptomgr.notests net.ifnames=0 pci=lastbus=0 quiet panic=1 nr_cpus=24 scsi_mod.scan=none agent.debug_console agent.debug_console_vport=1026

# 没有ip命令，只能用内核接口，凑合看一下本机ip 地址
bash-4.4# cat /proc/net/fib_trie
Main:
  +-- 0.0.0.0/0 3 0 4
     +-- 0.0.0.0/4 2 0 2
        |-- 0.0.0.0
           /0 universe UNICAST
        +-- 10.254.0.0/23 2 0 1
           |-- 10.254.0.0
              /16 universe UNICAST
           +-- 10.254.1.0/28 2 0 2
              |-- 10.254.1.0
                 /32 link BROADCAST
                 /24 link UNICAST
              |-- 10.254.1.14
                 /32 host LOCAL
           |-- 10.254.1.255
              /32 link BROADCAST
     +-- 127.0.0.0/8 2 0 2
        +-- 127.0.0.0/31 1 0 0
           |-- 127.0.0.0
              /32 link BROADCAST
              /8 host LOCAL
           |-- 127.0.0.1
              /32 host LOCAL
        |-- 127.255.255.255
           /32 link BROADCAST
     |-- 172.30.0.0
        /16 universe UNICAST
     |-- 224.0.0.0
        /4 universe UNICAST
Local:
  +-- 0.0.0.0/0 3 0 4
     +-- 0.0.0.0/4 2 0 2
        |-- 0.0.0.0
           /0 universe UNICAST
        +-- 10.254.0.0/23 2 0 1
           |-- 10.254.0.0
              /16 universe UNICAST
           +-- 10.254.1.0/28 2 0 2
              |-- 10.254.1.0
                 /32 link BROADCAST
                 /24 link UNICAST
              |-- 10.254.1.14
                 /32 host LOCAL
           |-- 10.254.1.255
              /32 link BROADCAST
     +-- 127.0.0.0/8 2 0 2
        +-- 127.0.0.0/31 1 0 0
           |-- 127.0.0.0
              /32 link BROADCAST
              /8 host LOCAL
           |-- 127.0.0.1
              /32 host LOCAL
        |-- 127.255.255.255
           /32 link BROADCAST
     |-- 172.30.0.0
        /16 universe UNICAST
     |-- 224.0.0.0
        /4 universe UNICAST

# 看看systemctl的服务
bash-4.4# systemctl list-units
  UNIT                          LOAD   ACTIVE SUB     DESCRIPTION
  sys-devices-pci0000:00-0000:00:01.0-virtio0-virtio\x2dports-vport0p0.device loaded active plugged /sys/devices/pci0000:00/0000:00:01.0/virtio0/virtio-ports/vport0p0
  sys-devices-pci0000:00-0000:00:07.0-virtio5-net-eth0.device loaded active plugged /sys/devices/pci0000:00/0000:00:07.0/virtio5/net/eth0
  sys-devices-platform-serial8250-tty-ttyS0.device loaded active plugged /sys/devices/platform/serial8250/tty/ttyS0
  sys-devices-platform-serial8250-tty-ttyS1.device loaded active plugged /sys/devices/platform/serial8250/tty/ttyS1
  sys-devices-platform-serial8250-tty-ttyS2.device loaded active plugged /sys/devices/platform/serial8250/tty/ttyS2
  sys-devices-platform-serial8250-tty-ttyS3.device loaded active plugged /sys/devices/platform/serial8250/tty/ttyS3
  sys-devices-virtual-tty-hvc0.device loaded active plugged /sys/devices/virtual/tty/hvc0
  sys-devices-virtual-tty-hvc1.device loaded active plugged /sys/devices/virtual/tty/hvc1
  sys-devices-virtual-tty-hvc2.device loaded active plugged /sys/devices/virtual/tty/hvc2
  sys-devices-virtual-tty-hvc3.device loaded active plugged /sys/devices/virtual/tty/hvc3
  sys-devices-virtual-tty-hvc4.device loaded active plugged /sys/devices/virtual/tty/hvc4
  sys-devices-virtual-tty-hvc5.device loaded active plugged /sys/devices/virtual/tty/hvc5
  sys-devices-virtual-tty-hvc6.device loaded active plugged /sys/devices/virtual/tty/hvc6
  sys-devices-virtual-tty-hvc7.device loaded active plugged /sys/devices/virtual/tty/hvc7
  sys-module-configfs.device    loaded active plugged /sys/module/configfs
  sys-module-fuse.device        loaded active plugged /sys/module/fuse
  sys-subsystem-net-devices-eth0.device loaded active plugged /sys/subsystem/net/devices/eth0
  -.mount                       loaded active mounted Root Mount
  etc-resolv.conf.mount         loaded active mounted /etc/resolv.conf
  run-kata\x2dcontainers-3daea1739ff15b732a2a1e7cf76d64b49f128a5a55bb8807c5ddde96d378e5cd-rootfs.mount loaded active mounted /run/kata-containers/3daea1739ff15b732a2a1e7cf76d64b49f128a5a55bb8807c5ddde96d378e5cd/rootfs
  run-kata\x2dcontainers-e47a609923ce835a252c87d71fc3ba92adb974f00fdae194576b3d388b1bc770-rootfs.mount loaded active mounted /run/kata-containers/e47a609923ce835a252c87d71fc3ba92adb974f00fdae194576b3d388b1bc770/rootfs
  run-kata\x2dcontainers-sandbox-shm.mount loaded active mounted /run/kata-containers/sandbox/shm
-containers/shared/containersed-containers.mount loaded active mounted /run/kata--More--
  run-sandbox\x2dns-ipc.mount   loaded active mounted /run/sandbox-ns/ipc
  run-sandbox\x2dns-uts.mount   loaded active mounted /run/sandbox-ns/uts
  sys-kernel-config.mount       loaded active mounted Kernel Configuration File System
  tmp.mount                     loaded active mounted Temporary Directory (/tmp)
  systemd-ask-password-console.path loaded active waiting Dispatch Password Requests to Console Directory Watch
  init.scope                    loaded active running System and Service Manager
  kata-agent.service            loaded active running Kata Containers Agent
  kmod-static-nodes.service     loaded active exited  Create list of required static device nodes for the current kernel
  systemd-journald.service      loaded active running Journal Service
● systemd-modules-load.service  loaded failed failed  Load Kernel Modules
  systemd-sysctl.service        loaded active exited  Apply Kernel Variables
  systemd-tmpfiles-setup-dev.service loaded active exited  Create Static Device Nodes in /dev
  systemd-tmpfiles-setup.service loaded active exited  Create Volatile Files and Directories
  systemd-udev-trigger.service  loaded active exited  udev Coldplug all Devices
  systemd-udevd.service         loaded active running udev Kernel Device Manager
  -.slice                       loaded active active  Root Slice
  system.slice                  loaded active active  System Slice
  systemd-journald-dev-log.socket loaded active running Journal Socket (/dev/log)
  systemd-journald.socket       loaded active running Journal Socket
  systemd-udevd-control.socket  loaded active running udev Control Socket
  systemd-udevd-kernel.socket   loaded active running udev Kernel Socket
  basic.target                  loaded active active  Basic System
  kata-containers.target        loaded active active  Kata Containers Agent Target
  local-fs.target               loaded active active  Local File Systems
  multi-user.target             loaded active active  Multi-User System
  paths.target                  loaded active active  Paths
  slices.target                 loaded active active  Slices
  sockets.target                loaded active active  Sockets
  swap.target                   loaded active active  Swap
  sysinit.target                loaded active active  System Initialization
  timers.target                 loaded active active  Timers

# 有一个kata-containers的服务，我们很感兴趣，看看什么内容。
bash-4.4# systemctl cat kata-containers.target
# /usr/lib/systemd/system/kata-containers.target
#
# Copyright (c) 2018-2019 Intel Corporation
#
# SPDX-License-Identifier: Apache-2.0
#

[Unit]
Description=Kata Containers Agent Target
Requires=basic.target
Requires=tmp.mount
Wants=chronyd.service
Requires=kata-agent.service
Conflicts=rescue.service rescue.target
After=basic.target rescue.service rescue.target
AllowIsolate=yes

bash-4.4# systemctl cat kata-agent.service
# /usr/lib/systemd/system/kata-agent.service
#
# Copyright (c) 2018-2019 Intel Corporation
#
# SPDX-License-Identifier: Apache-2.0
#

[Unit]
Description=Kata Containers Agent
Documentation=https://github.com/kata-containers/kata-containers
Wants=kata-containers.target

[Service]
# Send agent output to tty to allow capture debug logs
# from a VM vsock port
StandardOutput=tty
Type=simple
ExecStart=/usr/bin/kata-agent
LimitNOFILE=1048576
# ExecStop is required for static agent tracing; in all other scenarios
# the runtime handles shutting down the VM.
ExecStop=/bin/sync ; /usr/bin/systemctl --force poweroff
FailureAction=poweroff
# Discourage OOM-killer from touching the agent
OOMScoreAdjust=-997

# 我们的容器都在哪里呢？找到了。
bash-4.4# pwd
/run/kata-containers/e47a609923ce835a252c87d71fc3ba92adb974f00fdae194576b3d388b1bc770/rootfs
bash-4.4# echo *
anaconda-post.log bin check.sh dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var

</code></pre>
<p>从helper登录到容器里面，看看什么情况。</p>
<pre><code>[root@helper ~]# oc rsh pod/mypod-787d79b456-4f4xr
sh-4.2# ls
anaconda-post.log  bin  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
sh-4.2# ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc fq_codel state UP group default qlen 1000
    link/ether 0a:58:0a:fe:01:0e brd ff:ff:ff:ff:ff:ff
    inet 10.254.1.14/24 brd 10.254.1.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::858:aff:fefe:10e/64 scope link
       valid_lft forever preferred_lft forever
    inet6 fe80::5c25:c3ff:fe29:f429/64 scope link
       valid_lft forever preferred_lft forever

sh-4.2# ps efx ww
    PID TTY      STAT   TIME COMMAND
      2 ?        Ss     0:00 /bin/sh TERM=screen-256color HOSTNAME=mypod-787d79b456-4f4xr KUBERNETES_PORT_443_TCP_PORT=443 KUBERNETES_PORT=tcp://172.30.0.1:443 KUBERNETES_SERVICE_PORT=443 KUBERNETES_SERVICE_HOST=172.30.0.1 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin PWD=/ SHLVL=1 HOME=/root KUBERNETES_PORT_443_TCP_PROTO=tcp KUBERNETES_SERVICE_PORT_HTTPS=443 NSS_SDB_USE_CACHE=no KUBERNETES_PORT_443_TCP_ADDR=172.30.0.1 KUBERNETES_PORT_443_TCP=tcp://172.30.0.1:443 _=/bin/sh
      9 ?        R+     0:00  \_ ps efx ww HOSTNAME=mypod-787d79b456-4f4xr KUBERNETES_PORT=tcp://172.30.0.1:443 KUBERNETES_PORT_443_TCP_PORT=443 TERM=screen-256color KUBERNETES_SERVICE_PORT=443 KUBERNETES_SERVICE_HOST=172.30.0.1 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin PWD=/ HOME=/root SHLVL=2 KUBERNETES_PORT_443_TCP_PROTO=tcp KUBERNETES_SERVICE_PORT_HTTPS=443 NSS_SDB_USE_CACHE=no KUBERNETES_PORT_443_TCP_ADDR=172.30.0.1 KUBERNETES_PORT_443_TCP=tcp://172.30.0.1:443 _=/usr/bin/ps
      1 ?        S      0:00 sleep infinity PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin TERM=xterm HOSTNAME=mypod-787d79b456-4f4xr NSS_SDB_USE_CACHE=no KUBERNETES_SERVICE_HOST=172.30.0.1 KUBERNETES_SERVICE_PORT=443 KUBERNETES_SERVICE_PORT_HTTPS=443 KUBERNETES_PORT=tcp://172.30.0.1:443 KUBERNETES_PORT_443_TCP=tcp://172.30.0.1:443 KUBERNETES_PORT_443_TCP_PROTO=tcp KUBERNETES_PORT_443_TCP_PORT=443 KUBERNETES_PORT_443_TCP_ADDR=172.30.0.1 HOME=/root       
</code></pre>
<h1 id="研究一下网络"><a class="header" href="#研究一下网络">研究一下网络</a></h1>
<p>kata的网络模型，我们很关心，官方有文档。</p>
<p>https://github.com/kata-containers/kata-containers/blob/main/docs/design/architecture.md</p>
<p><img src="ocp4/4.8/imgs/2021-08-28-19-32-57.png" alt="" /></p>
<pre><code># 我们在worker-0上，看看namespace情况
[root@worker-0 ~]# lsns --output NS,TYPE,NETNSID,PID,COMMAND | grep qemu
4026533791 net             5 20394 /usr/libexec/qemu-kiwi -name sandbox-0f60fb9af6dbf8c8e355b9e27a62debe8276aa76f4246857e46520fa677ce40e -uuid 0a101364-3814-42a4-91b9-c8a81fc377ef -machine q35,accel=kvm,kernel_irqchip -cpu host,pmu=off -qmp unix:/run/vc/vm/0f60fb9af6dbf8c8e355b9e27a62debe8276aa76f4246857e46520fa677ce40e/qmp.sock,server=on,wait=off -m 2048M,slots=10,maxmem=33122M -device pci-bridge,bus=pcie.0,id=pci-bridge-0,chassis_nr=1,shpc=on,addr=2 -device virtio-serial-pci,disable-modern=false,id=serial0 -device virtconsole,chardev=charconsole0,id=console0 -chardev socket,id=charconsole0,path=/run/vc/vm/0f60fb9af6dbf8c8e355b9e27a62debe8276aa76f4246857e46520fa677ce40e/console.sock,server=on,wait=off -device virtio-scsi-pci,id=scsi0,disable-modern=false -object rng-random,id=rng0,filename=/dev/urandom -device virtio-rng-pci,rng=rng0 -device vhost-vsock-pci,disable-modern=false,vhostfd=3,id=vsock-2809816003,guest-cid=2809816003 -chardev socket,id=char-3bb1f59f00a0b873,path=/run/vc/vm/0f60fb9af6dbf8c8e355b9e27a62debe8276aa76f4246857e46520fa677ce40e/vhost-fs.sock -device vhost-user-fs-pci,chardev=char-3bb1f59f00a0b873,tag=kataShared -netdev tap,id=network-0,vhost=on,vhostfds=4,fds=5 -device driver=virtio-net-pci,netdev=network-0,mac=0a:58:0a:81:00:12,disable-modern=false,mq=on,vectors=4 -rtc base=utc,driftfix=slew,clock=host -global kvm-pit.lost_tick_policy=discard -vga none -no-user-config -nodefaults -nographic --no-reboot -daemonize -object memory-backend-file,id=dimm1,size=2048M,mem-path=/dev/shm,share=on -numa node,memdev=dimm1 -kernel /usr/lib/modules/4.18.0-305.19.1.el8_4.x86_64/vmlinuz -initrd /var/cache/kata-containers/osbuilder-images/4.18.0-305.19.1.el8_4.x86_64/&quot;rhcos&quot;-kata-4.18.0-305.19.1.el8_4.x86_64.initrd -append tsc=reliable no_timer_check rcupdate.rcu_expedited=1 i8042.direct=1 i8042.dumbkbd=1 i8042.nopnp=1 i8042.noaux=1 noreplace-smp reboot=k console=hvc0 console=hvc1 cryptomgr.notests net.ifnames=0 pci=lastbus=0 quiet panic=1 nr_cpus=24 scsi_mod.scan=none agent.debug_console agent.debug_console_vport=1026 -pidfile /run/vc/vm/0f60fb9af6dbf8c8e355b9e27a62debe8276aa76f4246857e46520fa677ce40e/pid -smp 1,cores=1,threads=1,sockets=24,maxcpus=24

# 我们到kata的netns里面去看看忘了情况， eth0后面的@if22，说的是在对端，是22号接口和本接口做了peer。
[root@worker-0 ~]# nsenter -t 20394 -n ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
3: eth0@if22: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc noqueue state UP group default qlen 1000
    link/ether 0a:58:0a:81:00:12 brd ff:ff:ff:ff:ff:ff link-netns a4db0b05-2ff7-4a29-98da-1df2491622fb
    inet 10.129.0.18/23 brd 10.129.1.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::858:aff:fe81:12/64 scope link
       valid_lft forever preferred_lft forever
4: tap0_kata: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc mq state UNKNOWN group default qlen 1000
    link/ether 56:51:b2:40:7c:56 brd ff:ff:ff:ff:ff:ff
    inet6 fe80::5451:b2ff:fe40:7c56/64 scope link
       valid_lft forever preferred_lft forever

# 我们在worker-0上，能看到有28号接口，并且对应这kata里面的3好接口
[root@worker-0 ~]# ip link | grep 22 -A3
    link/ether 9e:88:4d:e5:55:80 brd ff:ff:ff:ff:ff:ff link-netns 7ccc8362-c042-4bf3-9ddc-fa4fef322134
18: 6f53bb03a970cf7@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc noqueue master ovs-system state UP mode DEFAULT group default
    link/ether 8e:a7:85:94:de:7b brd ff:ff:ff:ff:ff:ff link-netns 5f33c5e4-1788-4ab6-883b-78bf7ab5372e
22: 0f60fb9af6dbf8c@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc noqueue master ovs-system state UP mode DEFAULT group default
    link/ether 02:3c:63:91:ae:7f brd ff:ff:ff:ff:ff:ff link-netns 50226e1e-a0fd-48e3-b05c-7d5aa1d41acf

# 我们看看kata netns里面有没有nftables
[root@worker-0 ~]# nsenter -t 20394 -n nft list ruleset
table ip filter {
        chain INPUT {
                type filter hook input priority filter; policy accept;
        }

        chain FORWARD {
                type filter hook forward priority filter; policy accept;
                meta l4proto tcp tcp dport 22623 tcp flags &amp; (fin|syn|rst|ack) == syn counter packets 0 bytes 0 reject
                meta l4proto tcp tcp dport 22624 tcp flags &amp; (fin|syn|rst|ack) == syn counter packets 0 bytes 0 reject
                meta l4proto tcp ip daddr 169.254.169.254 tcp dport != 53 counter packets 0 bytes 0 reject
                meta l4proto udp ip daddr 169.254.169.254 udp dport 53 counter packets 0 bytes 0 reject
        }

        chain OUTPUT {
                type filter hook output priority filter; policy accept;
                meta l4proto tcp tcp dport 22623 tcp flags &amp; (fin|syn|rst|ack) == syn counter packets 0 bytes 0 reject
                meta l4proto tcp tcp dport 22624 tcp flags &amp; (fin|syn|rst|ack) == syn counter packets 0 bytes 0 reject
                meta l4proto tcp ip daddr 169.254.169.254 tcp dport != 53 counter packets 0 bytes 0 reject
                meta l4proto udp ip daddr 169.254.169.254 udp dport 53 counter packets 0 bytes 0 reject
        }
}
</code></pre>
<p>TC ( traffic control ) 的配置还是需要好好学习的，命令行比较复杂，可以参考以下的一些内容</p>
<ul>
<li><a href="https://people.netfilter.org/pablo/netdev0.1/papers/Linux-Traffic-Control-Classifier-Action-Subsystem-Architecture.pdf">很好的论文，解释了tc架构，以及stolen是怎么回事</a></li>
<li><a href="https://medium.com/swlh/traffic-mirroring-with-linux-tc-df4d36116119">直接讲解了tc mirror的配置和效果</a></li>
<li><a href="https://gist.github.com/mcastelino/7d85f4164ffdaf48242f9281bb1d0f9b">解释了 ingress 和 ffff: 之间的关系</a></li>
</ul>
<p>可以使用的 man 命令</p>
<ul>
<li>man tc-mirred</li>
<li>man tc-ctinfo</li>
<li>man tc-u32</li>
<li>man tc-actions</li>
</ul>
<p>注意 action 里面有一个stolen，这个是说，命中以后，后续tc动作就中断了，进入netfilter等内核后续流程。</p>
<pre><code># 我们看看文档里面的tc配置，意思就是在eth0和tap0_kata之间mirror流量

# 根据网上的文档，tc qdisc add dev eth0 handle ffff: ingress is equivalent to tc qdisc add dev eth0 ingress, and also equals to 'qdisc ingress ffff: dev enp0s31f6 parent ffff:fff1 ----------------'

[root@worker-0 ~]# nsenter -t 20394 -n tc -s -p qdisc show dev eth0
qdisc noqueue 0: root refcnt 2
 Sent 0 bytes 0 pkt (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
qdisc ingress ffff: parent ffff:fff1 ----------------
 Sent 192 bytes 2 pkt (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0

# 根据网上的文档，以下配置是 tc filter add dev eth0 parent ffff: protocol all u32 match u32 0 0 action mirred egress mirror dev tap0_kata 的结果

[root@worker-0 ~]# nsenter -t 20394 -n tc -s -p filter show dev eth0 root
filter parent ffff: protocol all pref 49152 u32 chain 0
filter parent ffff: protocol all pref 49152 u32 chain 0 fh 800: ht divisor 1
filter parent ffff: protocol all pref 49152 u32 chain 0 fh 800::800 order 2048 key ht 800 bkt 0 terminal flowid ??? not_in_hw  (rule hit 2 success 2)
  match 00000000/00000000 at 0 (success 2 )
        action order 1: mirred (Egress Redirect to device tap0_kata) stolen
        index 1 ref 1 bind 1 installed 2310 sec used 2310 sec firstused 2310 sec
        Action statistics:
        Sent 192 bytes 2 pkt (dropped 0, overlimits 0 requeues 0)
        backlog 0b 0p requeues 0

[root@worker-0 ~]# nsenter -t 20394 -n tc -s -p filter show dev eth0 ingress
filter parent ffff: protocol all pref 49152 u32 chain 0
filter parent ffff: protocol all pref 49152 u32 chain 0 fh 800: ht divisor 1
filter parent ffff: protocol all pref 49152 u32 chain 0 fh 800::800 order 2048 key ht 800 bkt 0 terminal flowid ??? not_in_hw  (rule hit 2 success 2)
  match 00000000/00000000 at 0 (success 2 )
        action order 1: mirred (Egress Redirect to device tap0_kata) stolen
        index 1 ref 1 bind 1 installed 1797 sec used 1797 sec firstused 1797 sec
        Action statistics:
        Sent 192 bytes 2 pkt (dropped 0, overlimits 0 requeues 0)
        backlog 0b 0p requeues 0

[root@worker-0 ~]# nsenter -t 20394 -n tc -s -p filter show dev eth0 egress
filter parent ffff: protocol all pref 49152 u32 chain 0
filter parent ffff: protocol all pref 49152 u32 chain 0 fh 800: ht divisor 1
filter parent ffff: protocol all pref 49152 u32 chain 0 fh 800::800 order 2048 key ht 800 bkt 0 terminal flowid ??? not_in_hw  (rule hit 2 success 2)
  match 00000000/00000000 at 0 (success 2 )
        action order 1: mirred (Egress Redirect to device tap0_kata) stolen
        index 1 ref 1 bind 1 installed 2330 sec used 2330 sec firstused 2330 sec
        Action statistics:
        Sent 192 bytes 2 pkt (dropped 0, overlimits 0 requeues 0)
        backlog 0b 0p requeues 0

# 根据网上的文档，以下配置是 tc filter add dev tap0_kata parent ffff: protocol all u32 match u32 0 0 action mirred egress mirror dev eth0 的结果

[root@worker-0 ~]# nsenter -t 20394 -n tc -s -p qdisc show dev tap0_kata
qdisc mq 0: root
 Sent 1296 bytes 16 pkt (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
qdisc fq_codel 0: parent :1 limit 10240p flows 1024 quantum 1414 target 5ms interval 100ms memory_limit 32Mb ecn drop_batch 64
 Sent 1296 bytes 16 pkt (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
  maxpacket 0 drop_overlimit 0 new_flow_count 0 ecn_mark 0
  new_flows_len 0 old_flows_len 0
qdisc ingress ffff: parent ffff:fff1 ----------------
 Sent 880 bytes 14 pkt (dropped 0, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0

[root@worker-0 ~]# nsenter -t 20394 -n tc -s -p filter show dev tap0_kata root
filter parent ffff: protocol all pref 49152 u32 chain 0
filter parent ffff: protocol all pref 49152 u32 chain 0 fh 800: ht divisor 1
filter parent ffff: protocol all pref 49152 u32 chain 0 fh 800::800 order 2048 key ht 800 bkt 0 terminal flowid ??? not_in_hw  (rule hit 15 success 15)
  match 00000000/00000000 at 0 (success 15 )
        action order 1: mirred (Egress Redirect to device eth0) stolen
        index 2 ref 1 bind 1 installed 2383 sec used 247 sec firstused 2380 sec
        Action statistics:
        Sent 936 bytes 15 pkt (dropped 0, overlimits 0 requeues 0)
        backlog 0b 0p requeues 0

[root@worker-0 ~]# nsenter -t 20394 -n tc -s -p filter show dev tap0_kata ingress
filter parent ffff: protocol all pref 49152 u32 chain 0
filter parent ffff: protocol all pref 49152 u32 chain 0 fh 800: ht divisor 1
filter parent ffff: protocol all pref 49152 u32 chain 0 fh 800::800 order 2048 key ht 800 bkt 0 terminal flowid ??? not_in_hw  (rule hit 14 success 14)
  match 00000000/00000000 at 0 (success 14 )
        action order 1: mirred (Egress Redirect to device eth0) stolen
        index 2 ref 1 bind 1 installed 1690 sec used 636 sec firstused 1687 sec
        Action statistics:
        Sent 880 bytes 14 pkt (dropped 0, overlimits 0 requeues 0)
        backlog 0b 0p requeues 0

[root@worker-0 ~]# nsenter -t 20394 -n tc -s -p filter show dev tap0_kata egress
filter parent ffff: protocol all pref 49152 u32 chain 0
filter parent ffff: protocol all pref 49152 u32 chain 0 fh 800: ht divisor 1
filter parent ffff: protocol all pref 49152 u32 chain 0 fh 800::800 order 2048 key ht 800 bkt 0 terminal flowid ??? not_in_hw  (rule hit 15 success 15)
  match 00000000/00000000 at 0 (success 15 )
        action order 1: mirred (Egress Redirect to device eth0) stolen
        index 2 ref 1 bind 1 installed 2400 sec used 264 sec firstused 2397 sec
        Action statistics:
        Sent 936 bytes 15 pkt (dropped 0, overlimits 0 requeues 0)
        backlog 0b 0p requeues 0

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="sriov-on-openshift4-with-unsupport-nic"><a class="header" href="#sriov-on-openshift4-with-unsupport-nic">sriov on openshift4 with unsupport NIC</a></h1>
<p>openshift4自带sriov支持，但是由于内核只认证了某些网卡，所以openshift4内置了一个白名单，sriov的功能只对这些网卡开放。那么我们做实验的时候，没有这些网卡，但是网卡本身支持sriov，怎么做实验呢？本文就讲述如何操作。</p>
<p>实验拓扑图</p>
<p><img src="ocp4/4.7/./dia/4.7.sriov.drawio.svg" alt="" /></p>
<p>视频讲解:</p>
<p><a href="https://www.bilibili.com/video/BV1ey4y1M7C2/"><kbd><img src="ocp4/4.7/imgs/2021-07-01-23-55-57.png" width="600"></kbd></a></p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1ey4y1M7C2/">bilibili</a></li>
<li><a href="https://www.ixigua.com/6979983491584918023">xigua</a></li>
<li><a href="https://youtu.be/HJZQYogTZFg">youtube</a></li>
</ul>
<p>there is nic whitelist build-in for openshift4's sriov, to disable it, using: </p>
<ul>
<li>https://docs.openshift.com/container-platform/4.6/networking/hardware_networks/configuring-sriov-operator.html#disable-enable-sr-iov-operator-admission-control-webhook_configuring-sriov-operator</li>
</ul>
<h1 id="openshift"><a class="header" href="#openshift">openshift</a></h1>
<pre><code class="language-bash"># sriov的实验不能在kvm里面做，因为sriov PF不能透传到kvm里面，那我们就搞一个物理机worker node

# check vendoer id and device id
# https://access.redhat.com/solutions/56081

# on worker-1
lspci -vv | grep -i Mellanox
# 04:00.0 Ethernet controller: Mellanox Technologies MT2892 Family [ConnectX-6 Dx]
#         Subsystem: Mellanox Technologies Device 0011
# 04:00.1 Ethernet controller: Mellanox Technologies MT2892 Family [ConnectX-6 Dx]
#         Subsystem: Mellanox Technologies Device 0011

lspci -nvv | grep &quot;04:00.0\|04:00.1&quot;
# pcilib: sysfs_read_vpd: read failed: Input/output error
# 04:00.0 0200: 15b3:101d
# 04:00.1 0200: 15b3:101d

cat /sys/class/net/*/device/sriov_numvfs
# 0
# 0
cat /sys/class/net/*/device/sriov_totalvfs
# 8
# 8

</code></pre>
<p>install NFD ( node feature discovery) operator</p>
<p><img src="ocp4/4.7/imgs/2021-06-30-15-16-13.png" alt="" /></p>
<p>install SRIOV operator</p>
<p><img src="ocp4/4.7/imgs/2021-06-30-14-21-21.png" alt="" /></p>
<pre><code class="language-bash">oc create namespace openshift-sriov-network-operator

oc create -f - &lt;&lt;EOF
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: sriov-network-operators
  namespace: openshift-sriov-network-operator
spec:
  targetNamespaces:
  - openshift-sriov-network-operator
EOF

# https://catalog.redhat.com/software/containers/openshift4/dpdk-base-rhel8/5e32be6cdd19c77896004a41
# registry.redhat.io/openshift4/dpdk-base-rhel8:latest

# oc get sriovnetworknodestates -n openshift-sriov-network-operator -o jsonpath='{.items[*].status}'  | jq

# 可以看到，worker-1上的网卡，已经辨别出了VF
oc get sriovnetworknodestates -n openshift-sriov-network-operator -o json  | jq &quot;.items[] | (.metadata.name, .status)&quot;
</code></pre>
<pre><code class="language-json">&quot;master-0&quot;
{
  &quot;interfaces&quot;: [
    {
      &quot;deviceID&quot;: &quot;1000&quot;,
      &quot;driver&quot;: &quot;virtio-pci&quot;,
      &quot;pciAddress&quot;: &quot;0000:00:03.0&quot;,
      &quot;vendor&quot;: &quot;1af4&quot;
    }
  ],
  &quot;syncStatus&quot;: &quot;Succeeded&quot;
}
&quot;master-1&quot;
{
  &quot;interfaces&quot;: [
    {
      &quot;deviceID&quot;: &quot;1000&quot;,
      &quot;driver&quot;: &quot;virtio-pci&quot;,
      &quot;pciAddress&quot;: &quot;0000:00:03.0&quot;,
      &quot;vendor&quot;: &quot;1af4&quot;
    }
  ],
  &quot;syncStatus&quot;: &quot;Succeeded&quot;
}
&quot;master-2&quot;
{
  &quot;interfaces&quot;: [
    {
      &quot;deviceID&quot;: &quot;1000&quot;,
      &quot;driver&quot;: &quot;virtio-pci&quot;,
      &quot;pciAddress&quot;: &quot;0000:00:03.0&quot;,
      &quot;vendor&quot;: &quot;1af4&quot;
    }
  ],
  &quot;syncStatus&quot;: &quot;Succeeded&quot;
}
&quot;worker-0&quot;
{
  &quot;interfaces&quot;: [
    {
      &quot;deviceID&quot;: &quot;1000&quot;,
      &quot;driver&quot;: &quot;virtio-pci&quot;,
      &quot;pciAddress&quot;: &quot;0000:00:03.0&quot;,
      &quot;vendor&quot;: &quot;1af4&quot;
    }
  ],
  &quot;syncStatus&quot;: &quot;Succeeded&quot;
}
&quot;worker-1&quot;
{
  &quot;interfaces&quot;: [
    {
      &quot;deviceID&quot;: &quot;165f&quot;,
      &quot;driver&quot;: &quot;tg3&quot;,
      &quot;linkSpeed&quot;: &quot;1000 Mb/s&quot;,
      &quot;linkType&quot;: &quot;ETH&quot;,
      &quot;mac&quot;: &quot;90:b1:1c:44:d6:0f&quot;,
      &quot;mtu&quot;: 1500,
      &quot;name&quot;: &quot;eno1&quot;,
      &quot;pciAddress&quot;: &quot;0000:01:00.0&quot;,
      &quot;vendor&quot;: &quot;14e4&quot;
    },
    {
      &quot;deviceID&quot;: &quot;165f&quot;,
      &quot;driver&quot;: &quot;tg3&quot;,
      &quot;linkSpeed&quot;: &quot;-1 Mb/s&quot;,
      &quot;linkType&quot;: &quot;ETH&quot;,
      &quot;mac&quot;: &quot;90:b1:1c:44:d6:10&quot;,
      &quot;mtu&quot;: 1500,
      &quot;name&quot;: &quot;eno2&quot;,
      &quot;pciAddress&quot;: &quot;0000:01:00.1&quot;,
      &quot;vendor&quot;: &quot;14e4&quot;
    },
    {
      &quot;deviceID&quot;: &quot;165f&quot;,
      &quot;driver&quot;: &quot;tg3&quot;,
      &quot;linkSpeed&quot;: &quot;-1 Mb/s&quot;,
      &quot;linkType&quot;: &quot;ETH&quot;,
      &quot;mac&quot;: &quot;90:b1:1c:44:d6:11&quot;,
      &quot;mtu&quot;: 1500,
      &quot;name&quot;: &quot;eno3&quot;,
      &quot;pciAddress&quot;: &quot;0000:02:00.0&quot;,
      &quot;vendor&quot;: &quot;14e4&quot;
    },
    {
      &quot;deviceID&quot;: &quot;165f&quot;,
      &quot;driver&quot;: &quot;tg3&quot;,
      &quot;linkSpeed&quot;: &quot;-1 Mb/s&quot;,
      &quot;linkType&quot;: &quot;ETH&quot;,
      &quot;mac&quot;: &quot;90:b1:1c:44:d6:12&quot;,
      &quot;mtu&quot;: 1500,
      &quot;name&quot;: &quot;eno4&quot;,
      &quot;pciAddress&quot;: &quot;0000:02:00.1&quot;,
      &quot;vendor&quot;: &quot;14e4&quot;
    },
    {
      &quot;deviceID&quot;: &quot;101d&quot;,
      &quot;driver&quot;: &quot;mlx5_core&quot;,
      &quot;linkSpeed&quot;: &quot;-1 Mb/s&quot;,
      &quot;linkType&quot;: &quot;ETH&quot;,
      &quot;mac&quot;: &quot;0c:42:a1:fa:18:52&quot;,
      &quot;mtu&quot;: 1500,
      &quot;name&quot;: &quot;enp4s0f0&quot;,
      &quot;pciAddress&quot;: &quot;0000:04:00.0&quot;,
      &quot;totalvfs&quot;: 8,
      &quot;vendor&quot;: &quot;15b3&quot;
    },
    {
      &quot;deviceID&quot;: &quot;101d&quot;,
      &quot;driver&quot;: &quot;mlx5_core&quot;,
      &quot;linkSpeed&quot;: &quot;-1 Mb/s&quot;,
      &quot;linkType&quot;: &quot;ETH&quot;,
      &quot;mac&quot;: &quot;0c:42:a1:fa:18:53&quot;,
      &quot;mtu&quot;: 1500,
      &quot;name&quot;: &quot;enp4s0f1&quot;,
      &quot;pciAddress&quot;: &quot;0000:04:00.1&quot;,
      &quot;totalvfs&quot;: 8,
      &quot;vendor&quot;: &quot;15b3&quot;
    }
  ],
  &quot;syncStatus&quot;: &quot;Succeeded&quot;
}
</code></pre>
<pre><code class="language-bash"># config worker-1 with hugepage

cat &lt;&lt; EOF &gt; /data/install/worker-performance.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: worker-performance
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,worker-performance]}
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/worker-performance: &quot;&quot;

EOF
oc create -f /data/install/worker-performance.yaml

# to restore
oc delete -f /data/install/worker-performance.yaml

oc label node worker-1 node-role.kubernetes.io/worker-performance=&quot;&quot;

cat &lt;&lt; EOF &gt; /data/install/worker-1-hugepage.yaml
apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: worker-1-hugepage
spec:
  cpu:
    isolated: &quot;5-23&quot;
    reserved: &quot;0-4&quot;
  hugepages:
    defaultHugepagesSize: 1G
    pages:
      - count: 4
        size: 1G
  nodeSelector:
    node-role.kubernetes.io/worker-performance: ''
EOF
oc create -f /data/install/worker-1-hugepage.yaml

# to restore
oc delete -f /data/install/worker-1-hugepage.yaml

# on worker-1
grep -i huge /proc/meminfo
# before
# AnonHugePages:    448512 kB
# ShmemHugePages:        0 kB
# HugePages_Total:       0
# HugePages_Free:        0
# HugePages_Rsvd:        0
# HugePages_Surp:        0
# Hugepagesize:       2048 kB
# Hugetlb:               0 kB

# after
# AnonHugePages:    376832 kB
# ShmemHugePages:        0 kB
# HugePages_Total:       4
# HugePages_Free:        4
# HugePages_Rsvd:        0
# HugePages_Surp:        0
# Hugepagesize:    1048576 kB
# Hugetlb:         4194304 kB


cat &lt;&lt; EOF &gt; /data/install/sriov-cx4.yaml
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policy-cx4-net-1
  namespace: openshift-sriov-network-operator
spec:
  resourceName: cx4nic1
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: &quot;true&quot;
  numVfs: 4
  nicSelector:
    vendor: &quot;15b3&quot;
    deviceID: &quot;101d&quot;
    # rootDevices:
    #   - &quot;0000:19:00.0&quot;
  deviceType: netdevice 
  isRdma: true
EOF
oc create -f /data/install/sriov-cx4.yaml
# Error from server (vendor/device 15b3/101d is not supported): error when creating &quot;/data/install/sriov-cx4.yaml&quot;: admission webhook &quot;operator-webhook.sriovnetwork.openshift.io&quot; denied the request: vendor/device 15b3/101d is not supported

# to restore
oc delete -f /data/install/sriov-cx4.yaml

oc get sriovoperatorconfig default -n openshift-sriov-network-operator -o yaml | yq e '.spec' -
# enableInjector: true
# enableOperatorWebhook: true
# logLevel: 2

oc patch sriovoperatorconfig default --type=merge \
  -n openshift-sriov-network-operator \
  --patch '{ &quot;spec&quot;: { &quot;enableOperatorWebhook&quot;: false } }'

oc get sriovoperatorconfig default -n openshift-sriov-network-operator -o yaml | yq e '.spec' -
# enableInjector: true
# enableOperatorWebhook: false
# logLevel: 2

oc create -f /data/install/sriov-cx4.yaml
# sriovnetworknodepolicy.sriovnetwork.openshift.io/policy-cx4-net-1 created

# you can see, VF num set to '4'
# oc get sriovnetworknodestates worker-1 -n openshift-sriov-network-operator -o json  | jq &quot;(.metadata.name, .status)&quot;
oc get sriovnetworknodestates worker-1 -n openshift-sriov-network-operator -o yaml | yq e &quot;del(.metadata.managedFields)&quot; -
</code></pre>
<pre><code class="language-yaml">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodeState
metadata:
  creationTimestamp: &quot;2021-06-30T16:00:09Z&quot;
  generation: 4
  name: worker-1
  namespace: openshift-sriov-network-operator
  ownerReferences:
    - apiVersion: sriovnetwork.openshift.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: SriovNetworkNodePolicy
      name: default
      uid: cef00fc5-7952-42ec-b863-980fdc1e6318
  resourceVersion: &quot;4425538&quot;
  selfLink: /apis/sriovnetwork.openshift.io/v1/namespaces/openshift-sriov-network-operator/sriovnetworknodestates/worker-1
  uid: fcf58d46-3127-4956-ac2f-df5ce2e2ac8c
spec:
  dpConfigVersion: &quot;4381421&quot;
  interfaces:
    - name: enp4s0f0
      numVfs: 4
      pciAddress: &quot;0000:04:00.0&quot;
      vfGroups:
        - deviceType: netdevice
          policyName: policy-cx4-net-1
          resourceName: cx4nic1
          vfRange: 0-3
    - name: enp4s0f1
      numVfs: 4
      pciAddress: &quot;0000:04:00.1&quot;
      vfGroups:
        - deviceType: netdevice
          policyName: policy-cx4-net-1
          resourceName: cx4nic1
          vfRange: 0-3
status:
  interfaces:
    - deviceID: 165f
      driver: tg3
      linkSpeed: 1000 Mb/s
      linkType: ETH
      mac: 90:b1:1c:44:d6:0f
      mtu: 1500
      name: eno1
      pciAddress: &quot;0000:01:00.0&quot;
      vendor: &quot;14e4&quot;
    - deviceID: 165f
      driver: tg3
      linkSpeed: -1 Mb/s
      linkType: ETH
      mac: 90:b1:1c:44:d6:10
      mtu: 1500
      name: eno2
      pciAddress: &quot;0000:01:00.1&quot;
      vendor: &quot;14e4&quot;
    - deviceID: 165f
      driver: tg3
      linkSpeed: -1 Mb/s
      linkType: ETH
      mac: 90:b1:1c:44:d6:11
      mtu: 1500
      name: eno3
      pciAddress: &quot;0000:02:00.0&quot;
      vendor: &quot;14e4&quot;
    - deviceID: 165f
      driver: tg3
      linkSpeed: -1 Mb/s
      linkType: ETH
      mac: 90:b1:1c:44:d6:12
      mtu: 1500
      name: eno4
      pciAddress: &quot;0000:02:00.1&quot;
      vendor: &quot;14e4&quot;
    - Vfs:
        - deviceID: 101e
          driver: mlx5_core
          mac: 36:da:1c:a9:47:9a
          mtu: 1500
          name: enp4s0f0v0
          pciAddress: &quot;0000:04:00.2&quot;
          vendor: 15b3
          vfID: 0
        - deviceID: 101e
          driver: mlx5_core
          mac: 62:ab:95:db:e6:cc
          mtu: 1500
          name: enp4s0f0v1
          pciAddress: &quot;0000:04:00.3&quot;
          vendor: 15b3
          vfID: 1
        - deviceID: 101e
          driver: mlx5_core
          pciAddress: &quot;0000:04:00.4&quot;
          vendor: 15b3
          vfID: 2
        - deviceID: 101e
          driver: mlx5_core
          mac: 5e:9f:cc:cc:e4:a1
          mtu: 1500
          name: enp4s0f0v3
          pciAddress: &quot;0000:04:00.5&quot;
          vendor: 15b3
          vfID: 3
      deviceID: 101d
      driver: mlx5_core
      eSwitchMode: legacy
      linkSpeed: -1 Mb/s
      linkType: ETH
      mac: 0c:42:a1:fa:18:52
      mtu: 1500
      name: enp4s0f0
      numVfs: 4
      pciAddress: &quot;0000:04:00.0&quot;
      totalvfs: 4
      vendor: 15b3
    - Vfs:
        - deviceID: 101e
          driver: mlx5_core
          mac: e6:75:48:6f:56:33
          mtu: 1500
          name: enp4s0f1v0
          pciAddress: &quot;0000:04:00.6&quot;
          vendor: 15b3
          vfID: 0
        - deviceID: 101e
          driver: mlx5_core
          mac: 5a:74:7a:e7:3d:2b
          mtu: 1500
          name: enp4s0f1v1
          pciAddress: &quot;0000:04:00.7&quot;
          vendor: 15b3
          vfID: 1
        - deviceID: 101e
          driver: mlx5_core
          mac: 62:f8:19:98:d5:5f
          mtu: 1500
          name: enp4s0f1v2
          pciAddress: &quot;0000:04:01.0&quot;
          vendor: 15b3
          vfID: 2
        - deviceID: 101e
          driver: mlx5_core
          mac: f2:14:1e:93:e9:39
          mtu: 1500
          name: enp4s0f1v3
          pciAddress: &quot;0000:04:01.1&quot;
          vendor: 15b3
          vfID: 3
      deviceID: 101d
      driver: mlx5_core
      eSwitchMode: legacy
      linkSpeed: -1 Mb/s
      linkType: ETH
      mac: 0c:42:a1:fa:18:53
      mtu: 1500
      name: enp4s0f1
      numVfs: 4
      pciAddress: &quot;0000:04:00.1&quot;
      totalvfs: 4
      vendor: 15b3
  syncStatus: Succeeded
</code></pre>
<pre><code class="language-bash">cat &lt;&lt; EOF &gt; /data/install/sriov-network.yaml
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: mlx-dpdk-network
  namespace: openshift-sriov-network-operator
spec:
  networkNamespace: demo
  ipam: &quot;{}&quot;
  resourceName: cx4nic1
EOF
oc create -f /data/install/sriov-network.yaml

# to restore
oc delete -f /data/install/sriov-network.yaml

# https://github.com/openshift/sriov-network-operator/issues/133

lspci -vv | grep -i Mellanox
# 04:00.0 Ethernet controller: Mellanox Technologies MT2892 Family [ConnectX-6 Dx]
#         Subsystem: Mellanox Technologies Device 0011
# 04:00.1 Ethernet controller: Mellanox Technologies MT2892 Family [ConnectX-6 Dx]
#         Subsystem: Mellanox Technologies Device 0011
# 04:00.2 Ethernet controller: Mellanox Technologies ConnectX Family mlx5Gen Virtual Function
#         Subsystem: Mellanox Technologies Device 0011
# 04:00.3 Ethernet controller: Mellanox Technologies ConnectX Family mlx5Gen Virtual Function
#         Subsystem: Mellanox Technologies Device 0011
# 04:00.4 Ethernet controller: Mellanox Technologies ConnectX Family mlx5Gen Virtual Function
#         Subsystem: Mellanox Technologies Device 0011
# 04:00.5 Ethernet controller: Mellanox Technologies ConnectX Family mlx5Gen Virtual Function
#         Subsystem: Mellanox Technologies Device 0011
# 04:00.6 Ethernet controller: Mellanox Technologies ConnectX Family mlx5Gen Virtual Function
#         Subsystem: Mellanox Technologies Device 0011
# 04:00.7 Ethernet controller: Mellanox Technologies ConnectX Family mlx5Gen Virtual Function
#         Subsystem: Mellanox Technologies Device 0011
# 04:01.0 Ethernet controller: Mellanox Technologies ConnectX Family mlx5Gen Virtual Function
#         Subsystem: Mellanox Technologies Device 0011
# 04:01.1 Ethernet controller: Mellanox Technologies ConnectX Family mlx5Gen Virtual Function
#         Subsystem: Mellanox Technologies Device 0011

lspci -nvv | grep &quot;04:00.0\|04:00.1&quot;
# pcilib: sysfs_read_vpd: read failed: Input/output error
# 04:00.0 0200: 15b3:101d
# 04:00.1 0200: 15b3:101d

lspci | grep -i Mellanox | awk '{print $1}' | xargs -I DEMO sh -c &quot;lspci -nvv | grep DEMO &quot;
# pcilib: sysfs_read_vpd: read failed: Input/output error
# 04:00.0 0200: 15b3:101d
# pcilib: sysfs_read_vpd: read failed: Input/output error
# 04:00.1 0200: 15b3:101d
# pcilib: sysfs_read_vpd: read failed: Input/output error
# 04:00.2 0200: 15b3:101e
# pcilib: sysfs_read_vpd: read failed: Input/output error
# 04:00.3 0200: 15b3:101e
# pcilib: sysfs_read_vpd: read failed: Input/output error
# 04:00.4 0200: 15b3:101e
# pcilib: sysfs_read_vpd: read failed: Input/output error
# 04:00.5 0200: 15b3:101e
# pcilib: sysfs_read_vpd: read failed: Input/output error
# 04:00.6 0200: 15b3:101e
# pcilib: sysfs_read_vpd: read failed: Input/output error
# 04:00.7 0200: 15b3:101e
# pcilib: sysfs_read_vpd: read failed: Input/output error
# 04:01.0 0200: 15b3:101e
# pcilib: sysfs_read_vpd: read failed: Input/output error
# 04:01.1 0200: 15b3:101e

# &lt;human readable name&gt;: &lt;vendor ID&gt; &lt;pf ID&gt; &lt;vf ID&gt;
cat &lt;&lt; EOF &gt; /data/install/sriov-unsupport.yaml
apiVersion: v1
data:
  CX6DX: 15b3 101d 101e
kind: ConfigMap
metadata:
  name: unsupported-nic-ids
  namespace: openshift-sriov-network-operator
EOF
oc create -f /data/install/sriov-unsupport.yaml

# try to deply a demo pod
cat &lt;&lt; EOF &gt; /data/install/dpdk-test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: dpdk-app
  annotations:
    k8s.v1.cni.cncf.io/networks: mlx-dpdk-network
spec:
  containers:
  - name: testpmd
    image: registry.redhat.io/openshift4/dpdk-base-rhel8:v4.6
    securityContext:
     capabilities:
        add: [&quot;IPC_LOCK&quot;] 
    volumeMounts:
    - mountPath: /dev/hugepages 
      name: hugepage
    resources:
      limits:
        openshift.io/cx4nic1: &quot;1&quot; 
        memory: &quot;1Gi&quot;
        cpu: &quot;4&quot; 
        hugepages-1Gi: &quot;4Gi&quot; 
      requests:
        openshift.io/cx4nic1: &quot;1&quot;
        memory: &quot;1Gi&quot;
        cpu: &quot;4&quot;
        hugepages-1Gi: &quot;4Gi&quot;
    command: [&quot;sleep&quot;, &quot;infinity&quot;]
  volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages
EOF
oc create -n demo -f /data/install/dpdk-test.yaml

# to restore
oc delete -n demo -f /data/install/dpdk-test.yaml

# in the pod
rpm -ql dpdk-tools
# /usr/sbin/dpdk-devbind
# /usr/share/dpdk/usertools
# /usr/share/dpdk/usertools/cpu_layout.py
# /usr/share/dpdk/usertools/dpdk-devbind.py
# /usr/share/dpdk/usertools/dpdk-pmdinfo.py
# /usr/share/dpdk/usertools/dpdk-telemetry-client.py

/usr/share/dpdk/usertools/dpdk-devbind.py --status-dev net
# lspci: Unable to load libkmod resources: error -12
# lspci: Unable to load libkmod resources: error -12
# lspci: Unable to load libkmod resources: error -12
# lspci: Unable to load libkmod resources: error -12
# lspci: Unable to load libkmod resources: error -12
# lspci: Unable to load libkmod resources: error -12
# lspci: Unable to load libkmod resources: error -12

# Network devices using kernel driver
# ===================================
# 0000:01:00.0 'NetXtreme BCM5720 2-port Gigabit Ethernet PCIe 165f' if= drv=tg3 unused= 
# 0000:01:00.1 'NetXtreme BCM5720 2-port Gigabit Ethernet PCIe 165f' if= drv=tg3 unused= 
# 0000:02:00.0 'NetXtreme BCM5720 2-port Gigabit Ethernet PCIe 165f' if= drv=tg3 unused= 
# 0000:02:00.1 'NetXtreme BCM5720 2-port Gigabit Ethernet PCIe 165f' if= drv=tg3 unused= 
# 0000:04:00.0 'MT2892 Family [ConnectX-6 Dx] 101d' if= drv=mlx5_core unused= 
# 0000:04:00.1 'MT2892 Family [ConnectX-6 Dx] 101d' if= drv=mlx5_core unused= 
# 0000:04:00.2 'ConnectX Family mlx5Gen Virtual Function 101e' if= drv=mlx5_core unused= 
# 0000:04:00.3 'ConnectX Family mlx5Gen Virtual Function 101e' if= drv=mlx5_core unused= 
# 0000:04:00.4 'ConnectX Family mlx5Gen Virtual Function 101e' if=net1 drv=mlx5_core unused= 
# 0000:04:00.5 'ConnectX Family mlx5Gen Virtual Function 101e' if= drv=mlx5_core unused= 
# 0000:04:00.6 'ConnectX Family mlx5Gen Virtual Function 101e' if= drv=mlx5_core unused= 
# 0000:04:00.7 'ConnectX Family mlx5Gen Virtual Function 101e' if= drv=mlx5_core unused= 
# 0000:04:01.0 'ConnectX Family mlx5Gen Virtual Function 101e' if= drv=mlx5_core unused= 
# 0000:04:01.1 'ConnectX Family mlx5Gen Virtual Function 101e' if= drv=mlx5_core unused= 

</code></pre>
<h1 id="kvm-doest-support-sriov-pf-passthrough"><a class="header" href="#kvm-doest-support-sriov-pf-passthrough">kvm does't support sriov PF passthrough</a></h1>
<p>it only support VF passthrough</p>
<ul>
<li>https://www.cnblogs.com/dion-90/articles/8522733.html</li>
<li>https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/sect-pci_devices-pci_passthrough</li>
</ul>
<pre><code class="language-bash"># on 101
ls /sys/class/net/

lspci -vv | grep -i Mellanox
# pcilib: sysfs_read_vpd: read failed: Input/output error
# 05:00.0 Ethernet controller: Mellanox Technologies MT27710 Family [ConnectX-4 Lx]
#         Subsystem: Mellanox Technologies Stand-up ConnectX-4 Lx EN, 25GbE dual-port SFP28, PCIe3.0 x8, MCX4121A-ACAT
# 05:00.1 Ethernet controller: Mellanox Technologies MT27710 Family [ConnectX-4 Lx]
#         Subsystem: Mellanox Technologies Stand-up ConnectX-4 Lx EN, 25GbE dual-port SFP28, PCIe3.0 x8, MCX4121A-ACAT
# 07:00.0 Ethernet controller: Mellanox Technologies MT27710 Family [ConnectX-4 Lx]
#         Subsystem: Mellanox Technologies Stand-up ConnectX-4 Lx EN, 25GbE dual-port SFP28, PCIe3.0 x8, MCX4121A-ACAT
# 07:00.1 Ethernet controller: Mellanox Technologies MT27710 Family [ConnectX-4 Lx]
#         Subsystem: Mellanox Technologies Stand-up ConnectX-4 Lx EN, 25GbE dual-port SFP28, PCIe3.0 x8, MCX4121A-ACAT

virsh nodedev-list | grep 000_05
# pci_0000_05_00_0
# pci_0000_05_00_1

virsh nodedev-dumpxml pci_0000_05_00_0
</code></pre>
<pre><code class="language-xml">&lt;device&gt;
  &lt;name&gt;pci_0000_05_00_0&lt;/name&gt;
  &lt;path&gt;/sys/devices/pci0000:00/0000:00:06.0/0000:05:00.0&lt;/path&gt;
  &lt;parent&gt;pci_0000_00_06_0&lt;/parent&gt;
  &lt;driver&gt;
    &lt;name&gt;mlx5_core&lt;/name&gt;
  &lt;/driver&gt;
  &lt;capability type='pci'&gt;
    &lt;domain&gt;0&lt;/domain&gt;
    &lt;bus&gt;5&lt;/bus&gt;
    &lt;slot&gt;0&lt;/slot&gt;
    &lt;function&gt;0&lt;/function&gt;
    &lt;product id='0x1015'&gt;MT27710 Family [ConnectX-4 Lx]&lt;/product&gt;
    &lt;vendor id='0x15b3'&gt;Mellanox Technologies&lt;/vendor&gt;
    &lt;capability type='virt_functions' maxCount='64'/&gt;
    &lt;iommuGroup number='17'&gt;
      &lt;address domain='0x0000' bus='0x05' slot='0x00' function='0x0'/&gt;
    &lt;/iommuGroup&gt;
    &lt;pci-express&gt;
      &lt;link validity='cap' port='0' speed='8' width='8'/&gt;
      &lt;link validity='sta' speed='5' width='4'/&gt;
    &lt;/pci-express&gt;
  &lt;/capability&gt;
&lt;/device&gt;
</code></pre>
<pre><code class="language-bash">virsh nodedev-dumpxml pci_0000_05_00_1
</code></pre>
<pre><code class="language-xml">&lt;device&gt;
  &lt;name&gt;pci_0000_05_00_1&lt;/name&gt;
  &lt;path&gt;/sys/devices/pci0000:00/0000:00:06.0/0000:05:00.1&lt;/path&gt;
  &lt;parent&gt;pci_0000_00_06_0&lt;/parent&gt;
  &lt;driver&gt;
    &lt;name&gt;mlx5_core&lt;/name&gt;
  &lt;/driver&gt;
  &lt;capability type='pci'&gt;
    &lt;domain&gt;0&lt;/domain&gt;
    &lt;bus&gt;5&lt;/bus&gt;
    &lt;slot&gt;0&lt;/slot&gt;
    &lt;function&gt;1&lt;/function&gt;
    &lt;product id='0x1015'&gt;MT27710 Family [ConnectX-4 Lx]&lt;/product&gt;
    &lt;vendor id='0x15b3'&gt;Mellanox Technologies&lt;/vendor&gt;
    &lt;capability type='virt_functions' maxCount='64'/&gt;
    &lt;iommuGroup number='18'&gt;
      &lt;address domain='0x0000' bus='0x05' slot='0x00' function='0x1'/&gt;
    &lt;/iommuGroup&gt;
    &lt;pci-express&gt;
      &lt;link validity='cap' port='0' speed='8' width='8'/&gt;
      &lt;link validity='sta' speed='5' width='4'/&gt;
    &lt;/pci-express&gt;
  &lt;/capability&gt;
&lt;/device&gt;
</code></pre>
<h2 id="on-103"><a class="header" href="#on-103">on 103</a></h2>
<pre><code class="language-bash">ls /sys/class/net/
# baremetal  eno1  eno2  eno3  eno4  enp4s0f0  enp4s0f1  lo  virbr0  virbr0-nic
echo 0 &gt; /sys/class/net/enp4s0f0/device/sriov_numvfs
echo 0 &gt; /sys/class/net/enp4s0f1/device/sriov_numvfs

lspci -vv | grep -i Mellanox
# 04:00.0 Ethernet controller: Mellanox Technologies MT2892 Family [ConnectX-6 Dx]
#         Subsystem: Mellanox Technologies Device 0011
# 04:00.1 Ethernet controller: Mellanox Technologies MT2892 Family [ConnectX-6 Dx]
#         Subsystem: Mellanox Technologies Device 0011

virsh nodedev-list | grep 000_04
# pci_0000_04_00_0
# pci_0000_04_00_1

virsh nodedev-dumpxml pci_0000_04_00_0
</code></pre>
<pre><code class="language-xml">&lt;device&gt;
  &lt;name&gt;pci_0000_04_00_0&lt;/name&gt;
  &lt;path&gt;/sys/devices/pci0000:00/0000:00:02.0/0000:04:00.0&lt;/path&gt;
  &lt;parent&gt;pci_0000_00_02_0&lt;/parent&gt;
  &lt;driver&gt;
    &lt;name&gt;mlx5_core&lt;/name&gt;
  &lt;/driver&gt;
  &lt;capability type='pci'&gt;
    &lt;class&gt;0x020000&lt;/class&gt;
    &lt;domain&gt;0&lt;/domain&gt;
    &lt;bus&gt;4&lt;/bus&gt;
    &lt;slot&gt;0&lt;/slot&gt;
    &lt;function&gt;0&lt;/function&gt;
    &lt;product id='0x101d'&gt;MT2892 Family [ConnectX-6 Dx]&lt;/product&gt;
    &lt;vendor id='0x15b3'&gt;Mellanox Technologies&lt;/vendor&gt;
    &lt;capability type='virt_functions' maxCount='8'/&gt;
    &lt;iommuGroup number='27'&gt;
      &lt;address domain='0x0000' bus='0x04' slot='0x00' function='0x0'/&gt;
    &lt;/iommuGroup&gt;
    &lt;numa node='0'/&gt;
    &lt;pci-express&gt;
      &lt;link validity='cap' port='0' speed='16' width='16'/&gt;
      &lt;link validity='sta' speed='8' width='8'/&gt;
    &lt;/pci-express&gt;
  &lt;/capability&gt;
&lt;/device&gt;
</code></pre>
<pre><code class="language-bash">virsh nodedev-dumpxml pci_0000_04_00_1
</code></pre>
<pre><code class="language-xml">&lt;device&gt;
  &lt;name&gt;pci_0000_04_00_1&lt;/name&gt;
  &lt;path&gt;/sys/devices/pci0000:00/0000:00:02.0/0000:04:00.1&lt;/path&gt;
  &lt;parent&gt;pci_0000_00_02_0&lt;/parent&gt;
  &lt;driver&gt;
    &lt;name&gt;mlx5_core&lt;/name&gt;
  &lt;/driver&gt;
  &lt;capability type='pci'&gt;
    &lt;class&gt;0x020000&lt;/class&gt;
    &lt;domain&gt;0&lt;/domain&gt;
    &lt;bus&gt;4&lt;/bus&gt;
    &lt;slot&gt;0&lt;/slot&gt;
    &lt;function&gt;1&lt;/function&gt;
    &lt;product id='0x101d'&gt;MT2892 Family [ConnectX-6 Dx]&lt;/product&gt;
    &lt;vendor id='0x15b3'&gt;Mellanox Technologies&lt;/vendor&gt;
    &lt;capability type='virt_functions' maxCount='8'/&gt;
    &lt;iommuGroup number='28'&gt;
      &lt;address domain='0x0000' bus='0x04' slot='0x00' function='0x1'/&gt;
    &lt;/iommuGroup&gt;
    &lt;numa node='0'/&gt;
    &lt;pci-express&gt;
      &lt;link validity='cap' port='0' speed='16' width='16'/&gt;
      &lt;link validity='sta' speed='8' width='8'/&gt;
    &lt;/pci-express&gt;
  &lt;/capability&gt;
&lt;/device&gt;
</code></pre>
<p>for ocp4-aHelper, change below kvm config </p>
<pre><code class="language-xml">&lt;hostdev mode='subsystem' type='pci' managed='yes'&gt;
      &lt;driver name='vfio'/&gt;
      &lt;source&gt;
        &lt;address domain='0x0000' bus='0x05' slot='0x00' function='0x0'/&gt;
      &lt;/source&gt;
      &lt;alias name='hostdev0'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x0a' function='0x0'/&gt;
    &lt;/hostdev&gt;
    &lt;hostdev mode='subsystem' type='pci' managed='yes'&gt;
      &lt;driver name='vfio'/&gt;
      &lt;source&gt;
        &lt;address domain='0x0000' bus='0x05' slot='0x00' function='0x1'/&gt;
      &lt;/source&gt;
      &lt;alias name='hostdev1'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x0b' function='0x0'/&gt;
    &lt;/hostdev&gt;
</code></pre>
<p>to </p>
<pre><code class="language-xml">    &lt;interface type='hostdev' managed='yes'&gt;
      &lt;driver name='vfio'/&gt;
      &lt;source&gt;
        &lt;address type='pci' domain='0x0000' bus='0x05' slot='0x00' function='0x0'/&gt;
      &lt;/source&gt;
    &lt;/interface&gt;
    &lt;interface type='hostdev' managed='yes'&gt;
      &lt;driver name='vfio'/&gt;
      &lt;source&gt;
        &lt;address type='pci' domain='0x0000' bus='0x05' slot='0x00' function='0x1'/&gt;
      &lt;/source&gt;
    &lt;/interface&gt;
</code></pre>
<pre><code class="language-bash">virsh edit ocp4-aHelper
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="keepalived-operator-in-openshift4"><a class="header" href="#keepalived-operator-in-openshift4">keepalived operator in openshift4</a></h1>
<h2 id="痛点"><a class="header" href="#痛点">痛点</a></h2>
<p>openshift4 标准安装，使用router(haproxy)来做ingress，向集群导入流量，这么做，默认只能工作在7层，虽然也有方法进行定制，让他工作在4层，但是不管从对外暴露的IP地址的可管理性，以及应用端口冲突处理方面来说，都非常不方便。</p>
<p>根本原因，其实是openshift4 私有化安装不支持 LoadBalancer 这个service type。 那么今天我们就找了 keepalived operator，来弥补这个缺陷。</p>
<p>视频讲解:</p>
<p><a href="https://www.bilibili.com/video/BV1Sw411Z79H/"><kbd><img src="ocp4/4.7/imgs/2021-06-10-16-43-56.png" width="600"></kbd></a></p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1Sw411Z79H/">bilibili</a></li>
<li><a href="https://www.ixigua.com/6972082855060963848">xigua</a></li>
<li><a href="https://youtu.be/_c9s8dQoWUw">youtube</a></li>
</ul>
<p>本文，参考openshift blog上的文章</p>
<ul>
<li>https://www.openshift.com/blog/self-hosted-load-balancer-for-openshift-an-operator-based-approach</li>
<li>https://github.com/redhat-cop/keepalived-operator</li>
</ul>
<h2 id="试验架构图"><a class="header" href="#试验架构图">试验架构图</a></h2>
<p><img src="ocp4/4.7/dia/4.7.keepalived.drawio.svg" alt="" /></p>
<p>可以看到，keepalived，会在节点上，根据service的定义，创建second IP，然后外部流量，就从这个IP地址，进入集群。这是一种k8s LoadBalancer 的实现方式，和ingress controller的方式对比，就是天然支持tcp模式的4层转发。</p>
<p>安装keepalived operator很简单</p>
<p><img src="ocp4/4.7/imgs/2021-06-07-23-11-26.png" alt="" /></p>
<p>在web界面操作完了，需要标记节点，已经调整一下权限</p>
<pre><code class="language-bash">oc label node master-2 node-role.kubernetes.io/loadbalancer=&quot;&quot;
oc label node master-1 node-role.kubernetes.io/loadbalancer=&quot;&quot;

oc adm policy add-scc-to-user privileged -z default -n keepalived-operator
</code></pre>
<p>接下来，我们来看看keepalived的部署有什么特殊的地方。</p>
<p>我们可以看到 keepalived pod 使用了 hostnetwork 和 privileged: true。 但是keepalived pod 没有挂载特殊的主机目录。 
<img src="ocp4/4.7/imgs/2021-06-09-22-04-22.png" alt="" /></p>
<h1 id="测试部署一个应用"><a class="header" href="#测试部署一个应用">测试部署一个应用</a></h1>
<pre><code class="language-bash">cat &lt;&lt; 'EOF' &gt; /data/install/network-patch.yaml
spec:
  externalIP:
    policy:
      allowedCIDRs:
      - ${ALLOWED_CIDR}
    autoAssignCIDRs:
      - &quot;${AUTOASSIGNED_CIDR}&quot;
EOF

# export VERSION=&quot;4.9.4&quot;
# export BINARY=&quot;yq_linux_amd64&quot;
# wget https://github.com/mikefarah/yq/releases/download/${VERSION}/${BINARY} -O /usr/local/bin/yq &amp;&amp; chmod +x /usr/local/bin/yq

# 24 256
# 25 128
# 26 64
# 27 32
# 28 16
cd /data/install
export ALLOWED_CIDR=&quot;172.21.6.33/27&quot;
export AUTOASSIGNED_CIDR=&quot;172.21.6.33/27&quot;
oc patch network cluster -p &quot;$(envsubst &lt; ./network-patch.yaml | yq eval -j -)&quot; --type=merge

oc get network cluster -o yaml
# spec:
#   clusterNetwork:
#   - cidr: 10.254.0.0/16
#     hostPrefix: 24
#   externalIP:
#     autoAssignCIDRs:
#     - 172.21.6.33/27
#     policy:
#       allowedCIDRs:
#       - 172.21.6.33/27
#   networkType: OpenShiftSDN
#   serviceNetwork:
#   - 172.30.0.0/16
# status:
#   clusterNetwork:
#   - cidr: 10.254.0.0/16
#     hostPrefix: 24
#   clusterNetworkMTU: 1450
#   networkType: OpenShiftSDN
#   serviceNetwork:
#   - 172.30.0.0/16

oc new-project demo

cat &lt;&lt; EOF &gt; /data/install/demo.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: test-0
  labels:
    env: test
spec:
  restartPolicy: OnFailure
  nodeSelector:
    kubernetes.io/hostname: 'master-0'
  containers:
  - name: php
    image: &quot;quay.io/wangzheng422/php:demo.02&quot;
---
apiVersion: v1
kind: Pod
metadata:
  name: test-1
  labels:
    env: test
spec:
  restartPolicy: OnFailure
  nodeSelector:
    kubernetes.io/hostname: 'master-2'
  containers:
  - name: php
    image: &quot;quay.io/wangzheng422/php:demo.02&quot;
---
kind: Service
apiVersion: v1
metadata:
  name: demo
  annotations:
    keepalived-operator.redhat-cop.io/keepalivedgroup: keepalived-operator/keepalivedgroup-workers
spec:
  type: LoadBalancer
  ports:
    - name: &quot;http&quot;
      protocol: TCP
      port: 80
      targetPort: 80
  selector:
    env: test
EOF
oc create -n demo -f /data/install/demo.yaml

# to restore
oc delete -n demo -f /data/install/demo.yaml

</code></pre>
<h2 id="分析一下应用的行为"><a class="header" href="#分析一下应用的行为">分析一下应用的行为</a></h2>
<p>看看service的配置，能看到已经分配了对外的IP
<img src="ocp4/4.7/imgs/2021-06-09-23-14-26.png" alt="" /></p>
<pre><code class="language-bash">oc get svc
# NAME   TYPE           CLUSTER-IP       EXTERNAL-IP               PORT(S)        AGE
# demo   LoadBalancer   172.30.203.237   172.21.6.50,172.21.6.50   80:31682/TCP   14m

curl http://172.21.6.50/
# Hello!&lt;br&gt;Welcome to RedHat Developer&lt;br&gt;Enjoy all of the ad-free articles&lt;br&gt;

</code></pre>
<p>master-2 上面，相关的iptables 配置</p>
<pre><code>    0     0 KUBE-FW-ZFZLPEKTCJ3DBGAL  tcp  --  *      *       0.0.0.0/0            172.21.6.50          /* demo/demo:http loadbalancer IP */ tcp dpt:80
</code></pre>
<p>可以看到，svc的防火墙策略，分流到了pod
<img src="ocp4/4.7/imgs/2021-06-09-23-11-47.png" alt="" /></p>
<h1 id="keepalived-pods-definition"><a class="header" href="#keepalived-pods-definition">keepalived pods definition</a></h1>
<p>we can see, it use hostnetwork and privileged: true</p>
<pre><code class="language-yaml">kind: Pod
apiVersion: v1
metadata:
  generateName: keepalivedgroup-workers-
  annotations:
    openshift.io/scc: privileged
  selfLink: /api/v1/namespaces/keepalived-operator/pods/keepalivedgroup-workers-fgzv8
  resourceVersion: '2700532'
  name: keepalivedgroup-workers-fgzv8
  uid: 1addc7c7-4e6d-49c7-ae5e-3a4e2963755b
  creationTimestamp: '2021-06-09T08:51:40Z'
  namespace: keepalived-operator
  ownerReferences:
    - apiVersion: apps/v1
      kind: DaemonSet
      name: keepalivedgroup-workers
      uid: dba36a9c-f2aa-4951-aa60-a3836275ae1b
      controller: true
      blockOwnerDeletion: true
  labels:
    controller-revision-hash: 7459c85f64
    keepalivedGroup: keepalivedgroup-workers
    pod-template-generation: '1'
spec:
  nodeSelector:
    node-role.kubernetes.io/loadbalancer: ''
  restartPolicy: Always
  initContainers:
    - resources: {}
      terminationMessagePath: /dev/termination-log
      name: config-setup
      command:
        - bash
        - '-c'
        - /usr/local/bin/notify.sh
      env:
        - name: file
          value: /etc/keepalived.d/src/keepalived.conf
        - name: dst_file
          value: /etc/keepalived.d/dst/keepalived.conf
        - name: reachip
        - name: create_config_only
          value: 'true'
      securityContext:
        runAsUser: 0
      imagePullPolicy: Always
      volumeMounts:
        - name: config
          readOnly: true
          mountPath: /etc/keepalived.d/src
        - name: config-dst
          mountPath: /etc/keepalived.d/dst
      terminationMessagePolicy: File
      image: 'quay.io/redhat-cop/keepalived-operator:latest'
  serviceAccountName: default
  imagePullSecrets:
    - name: default-dockercfg-2d5d5
  priority: 0
  schedulerName: default-scheduler
  hostNetwork: true
  enableServiceLinks: false
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchFields:
              - key: metadata.name
                operator: In
                values:
                  - master-1
  terminationGracePeriodSeconds: 30
  shareProcessNamespace: true
  preemptionPolicy: PreemptLowerPriority
  nodeName: master-1
  securityContext: {}
  containers:
    - resources: {}
      terminationMessagePath: /dev/termination-log
      name: keepalived
      command:
        - /bin/bash
      env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
      securityContext:
        privileged: true
      imagePullPolicy: Always
      volumeMounts:
        - name: lib-modules
          readOnly: true
          mountPath: /lib/modules
        - name: config-dst
          readOnly: true
          mountPath: /etc/keepalived.d
        - name: pid
          mountPath: /etc/keepalived.pid
        - name: stats
          mountPath: /tmp
      terminationMessagePolicy: File
      image: registry.redhat.io/openshift4/ose-keepalived-ipfailover
      args:
        - '-c'
        - &gt;
          exec /usr/sbin/keepalived --log-console --log-detail --dont-fork
          --config-id=${POD_NAME} --use-file=/etc/keepalived.d/keepalived.conf
          --pid=/etc/keepalived.pid/keepalived.pid
    - resources: {}
      terminationMessagePath: /dev/termination-log
      name: config-reloader
      command:
        - bash
        - '-c'
        - /usr/local/bin/notify.sh
      env:
        - name: pid
          value: /etc/keepalived.pid/keepalived.pid
        - name: file
          value: /etc/keepalived.d/src/keepalived.conf
        - name: dst_file
          value: /etc/keepalived.d/dst/keepalived.conf
        - name: reachip
        - name: create_config_only
          value: 'false'
      securityContext:
        runAsUser: 0
      imagePullPolicy: Always
      volumeMounts:
        - name: config
          readOnly: true
          mountPath: /etc/keepalived.d/src
        - name: config-dst
          mountPath: /etc/keepalived.d/dst
        - name: pid
          mountPath: /etc/keepalived.pid
      terminationMessagePolicy: File
      image: 'quay.io/redhat-cop/keepalived-operator:latest'
    - resources: {}
      terminationMessagePath: /dev/termination-log
      name: prometheus-exporter
      command:
        - /usr/local/bin/keepalived_exporter
      securityContext:
        privileged: true
      ports:
        - name: metrics
          hostPort: 9650
          containerPort: 9650
          protocol: TCP
      imagePullPolicy: Always
      volumeMounts:
        - name: lib-modules
          readOnly: true
          mountPath: /lib/modules
        - name: stats
          mountPath: /tmp
      terminationMessagePolicy: File
      image: 'quay.io/redhat-cop/keepalived-operator:latest'
      args:
        - '-web.listen-address'
        - ':9650'
        - '-web.telemetry-path'
        - /metrics
  automountServiceAccountToken: false
  serviceAccount: default
  volumes:
    - name: lib-modules
      hostPath:
        path: /lib/modules
        type: ''
    - name: config
      configMap:
        name: keepalivedgroup-workers
        defaultMode: 420
    - name: config-dst
      emptyDir: {}
    - name: pid
      emptyDir:
        medium: Memory
    - name: stats
      emptyDir: {}
  dnsPolicy: ClusterFirst
  tolerations:
    - operator: Exists
    - key: node.kubernetes.io/not-ready
      operator: Exists
      effect: NoExecute
    - key: node.kubernetes.io/unreachable
      operator: Exists
      effect: NoExecute
    - key: node.kubernetes.io/disk-pressure
      operator: Exists
      effect: NoSchedule
    - key: node.kubernetes.io/memory-pressure
      operator: Exists
      effect: NoSchedule
    - key: node.kubernetes.io/pid-pressure
      operator: Exists
      effect: NoSchedule
    - key: node.kubernetes.io/unschedulable
      operator: Exists
      effect: NoSchedule
    - key: node.kubernetes.io/network-unavailable
      operator: Exists
      effect: NoSchedule
status:
  containerStatuses:
    - restartCount: 0
      started: true
      ready: true
      name: config-reloader
      state:
        running:
          startedAt: '2021-06-09T08:52:34Z'
      imageID: &gt;-
        quay.io/redhat-cop/keepalived-operator@sha256:dab32df252b705b07840dc0488fce0577ed743aaa33bed47e293f115bdda9348
      image: 'quay.io/redhat-cop/keepalived-operator:latest'
      lastState: {}
      containerID: 'cri-o://2d9c37aea1c623f1ff4afb50233c1d67567d3315ea64d10476cd613e8ccc2d04'
    - restartCount: 0
      started: true
      ready: true
      name: keepalived
      state:
        running:
          startedAt: '2021-06-09T08:52:34Z'
      imageID: &gt;-
        registry.redhat.io/openshift4/ose-keepalived-ipfailover@sha256:385f014b07acc361d1bb41ffd9d3abc151ab64e01f42dacba80053a4dfcbd242
      image: 'registry.redhat.io/openshift4/ose-keepalived-ipfailover:latest'
      lastState: {}
      containerID: 'cri-o://02b384c94506b7dcbd18cbf8ceadef83b366c356de36b8e2646cc233f1c23902'
    - restartCount: 0
      started: true
      ready: true
      name: prometheus-exporter
      state:
        running:
          startedAt: '2021-06-09T08:52:34Z'
      imageID: &gt;-
        quay.io/redhat-cop/keepalived-operator@sha256:dab32df252b705b07840dc0488fce0577ed743aaa33bed47e293f115bdda9348
      image: 'quay.io/redhat-cop/keepalived-operator:latest'
      lastState: {}
      containerID: 'cri-o://daeb85bf94923d9562a0cc777664397269ed642bd0d86cf993f12a2ff6fff925'
  qosClass: BestEffort
  podIPs:
    - ip: 192.168.7.14
  podIP: 192.168.7.14
  hostIP: 192.168.7.14
  startTime: '2021-06-09T08:51:40Z'
  initContainerStatuses:
    - name: config-setup
      state:
        terminated:
          exitCode: 0
          reason: Completed
          startedAt: '2021-06-09T08:51:54Z'
          finishedAt: '2021-06-09T08:51:54Z'
          containerID: &gt;-
            cri-o://9ecc0e9a469a0518a7ca2fc5feef551d56c052dfe569dba391d0c0fc998b2f41
      lastState: {}
      ready: true
      restartCount: 0
      image: 'quay.io/redhat-cop/keepalived-operator:latest'
      imageID: &gt;-
        quay.io/redhat-cop/keepalived-operator@sha256:dab32df252b705b07840dc0488fce0577ed743aaa33bed47e293f115bdda9348
      containerID: 'cri-o://9ecc0e9a469a0518a7ca2fc5feef551d56c052dfe569dba391d0c0fc998b2f41'
  conditions:
    - type: Initialized
      status: 'True'
      lastProbeTime: null
      lastTransitionTime: '2021-06-09T08:51:55Z'
    - type: Ready
      status: 'True'
      lastProbeTime: null
      lastTransitionTime: '2021-06-09T08:52:35Z'
    - type: ContainersReady
      status: 'True'
      lastProbeTime: null
      lastTransitionTime: '2021-06-09T08:52:35Z'
    - type: PodScheduled
      status: 'True'
      lastProbeTime: null
      lastTransitionTime: '2021-06-09T08:51:40Z'
  phase: Running

</code></pre>
<h1 id="准备一个php的测试镜像"><a class="header" href="#准备一个php的测试镜像">准备一个php的测试镜像</a></h1>
<pre><code class="language-bash"># 准备一个php的测试镜像

cat &lt;&lt; 'EOF' &gt; index.php
&lt;?php
$localIP = getHostByName(getHostName());
ECHO &quot;Hello!&lt;br&gt;&quot;;
echo &quot;Welcome to RedHat Developer&lt;br&gt;&quot;;
EcHo &quot;Enjoy all of the ad-free articles&lt;br&gt;&quot;.$localIP;
?&gt;
EOF

cat &lt;&lt; EOF &gt; php.dockerfile
FROM php:apache
COPY . /var/www/html/
EOF

buildah bud -t quay.io/wangzheng422/php:demo.02 -f php.dockerfile .

buildah push quay.io/wangzheng422/php:demo.02

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="real-time-kernel-for-openshift4"><a class="header" href="#real-time-kernel-for-openshift4">Real-Time Kernel for Openshift4</a></h1>
<p>5G RAN vDU 对操作系统的实时性要求很高， 基本都要求基于实时操作系统搞， openshift4 是一个和操作系统紧密捆绑的paas平台， 内置了实时操作系统， 这个操作系统是使用了 rhel8 的内核， 并使用 ostree 打包的操作系统。</p>
<p>openshift4 可以在node 上启动实时操作系统，有2个办法，一个是通过performance-addon operator</p>
<ul>
<li>https://docs.openshift.com/container-platform/4.7/scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.html</li>
</ul>
<p>另外一个，是直接用machine config的办法搞</p>
<ul>
<li>https://docs.openshift.com/container-platform/4.7/post_installation_configuration/machine-configuration-tasks.html#nodes-nodes-rtkernel-arguments_post-install-machine-configuration-tasks</li>
</ul>
<p>本次试验部署架构图</p>
<p><img src="ocp4/4.7/dia/4.7.real-time.kernel.drawio.svg" alt="" /></p>
<p>视频讲解:</p>
<p><a href="https://www.bilibili.com/video/BV1av411V7dQ/"><kbd><img src="ocp4/4.7/imgs/2021-06-07-16-31-23.png" width="600"></kbd></a></p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1av411V7dQ/">bilibili</a></li>
<li><a href="https://youtu.be/Updyv2X1INY">youtube</a></li>
</ul>
<h1 id="操作系统上怎么做"><a class="header" href="#操作系统上怎么做">操作系统上怎么做</a></h1>
<p>用实时操作系统，就是为了性能，那么如果我们是一台物理机，不考虑容器平台，我们应该怎么配置，让这个实时操作系统性能最大化呢？</p>
<p>一般来说，有2个通用的配置</p>
<ul>
<li>对实时操作系统，并进行系统调优配置。</li>
<li>物理机bios进行配置，关闭超线程，关闭irq balance，关闭cpu c-state 等节电功能。</li>
</ul>
<p>对于第一个，实时操作系统的配置，参考这里</p>
<ul>
<li>install kernel-rt</li>
<li>install rt-test</li>
</ul>
<pre><code class="language-bash">cat /etc/tuned/realtime-variables.conf
# isolated_cores=1-30
# isolate_managed_irq=Y
tuned-adm profile realtime
reboot

swapoff -a
systemctl stop irqbalance
</code></pre>
<p>对于第二个，物理机上bios配置，要找服务器的厂商文档，查看官方的low latency配置文档。 <a href="https://www.dell.com/downloads/global/products/pedge/en/configuring_low_Latency_environments_on_dell_poweredge_servers.pdf">比如这里</a></p>
<table><thead><tr><th>System Setup Screen</th><th>Setting</th><th>Default</th><th>Recommended Alternative for Low- Latency Environments</th></tr></thead><tbody>
<tr><td>Processor Settings</td><td>Logical Processor</td><td>Enabled</td><td>Disabled</td></tr>
<tr><td>Processor Settings</td><td>Turbo Mode</td><td>Enabled</td><td>Disabled2</td></tr>
<tr><td>Processor Settings</td><td>C-States</td><td>Enabled</td><td>Disabled</td></tr>
<tr><td>Processor Settings</td><td>C1E</td><td>Enabled</td><td>Disabled</td></tr>
<tr><td>Power Management</td><td>Power Management</td><td>Active Power Controller</td><td>Maximum Performance</td></tr>
</tbody></table>
<h1 id="先使用performance-addon-operator这个是官方推荐的方法"><a class="header" href="#先使用performance-addon-operator这个是官方推荐的方法">先使用performance addon operator，这个是官方推荐的方法。</a></h1>
<p>performance addon operator 是openshift4里面的一个operator，他的作用是，让用户进行简单的yaml配置，然后operator帮助客户进行复杂的kernel parameter, kubelet, tuned配置。</p>
<pre><code class="language-bash"># on 104, create a new worker node
export KVM_DIRECTORY=/data/kvm

mkdir -p  ${KVM_DIRECTORY}
cd ${KVM_DIRECTORY}
scp root@172.21.6.11:/data/install/{*worker-0}.iso ${KVM_DIRECTORY}/

virt-install --name=ocp4-worker0 --vcpus=4 --ram=8192 \
--disk path=/data/kvm/ocp4-worker0.qcow2,bus=virtio,size=120 \
--os-variant rhel8.0 --network bridge=br0,model=virtio \
--graphics vnc,listen=127.0.0.1,port=59005 \
--boot menu=on --cdrom ${KVM_DIRECTORY}/rhcos_install-worker-0.iso 

# go back to helper
oc get csr
oc get csr -ojson | jq -r '.items[] | select(.status == {} ) | .metadata.name' | xargs oc adm certificate approve

# install performance addon operator following offical document
# https://docs.openshift.com/container-platform/4.7/scalability_and_performance/cnf-performance-addon-operator-for-low-latency-nodes.html

cat &lt;&lt; EOF &gt; /data/install/worker-rt.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: worker-rt
  labels:
    machineconfiguration.openshift.io/role: worker-rt
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,worker-rt]}
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/worker-rt: &quot;&quot;

EOF
oc create -f /data/install/worker-rt.yaml

oc label MachineConfigPool/worker-rt machineconfiguration.openshift.io/role=worker-rt

# to restore
oc delete -f /data/install/worker-rt.yaml

oc label node worker-0 node-role.kubernetes.io/worker-rt=&quot;&quot;

# 以下的配置，是保留了0-1核给系统，剩下的2-3核给应用，实际物理机上，一般是2-19给应用。
cat &lt;&lt; EOF &gt; /data/install/performance.yaml
apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
   name: example-performanceprofile
spec:
  additionalKernelArgs:
    - selinux=0
    - intel_iommu=on
  globallyDisableIrqLoadBalancing: true
  cpu:
      isolated: &quot;2-3&quot;
      reserved: &quot;0-1&quot;
  hugepages:
      defaultHugepagesSize: &quot;1G&quot;
      pages:
         - size: &quot;1G&quot;
           count: 2
           node: 0
  realTimeKernel:
      enabled: true
  numa:  
      topologyPolicy: &quot;single-numa-node&quot;
  nodeSelector:
      node-role.kubernetes.io/worker-rt: &quot;&quot;

EOF
oc create -f /data/install/performance.yaml

# restore
oc delete -f /data/install/performance.yaml

# check the result
ssh core@worker-0
uname -a
# Linux worker-0 4.18.0-240.22.1.rt7.77.el8_3.x86_64 #1 SMP PREEMPT_RT Fri Mar 26 18:44:48 EDT 2021 x86_64 x86_64 x86_64 GNU/Linux

</code></pre>
<h2 id="remove-worker-0"><a class="header" href="#remove-worker-0">remove worker-0</a></h2>
<pre><code class="language-bash">oc delete node worker-0

virsh destroy ocp4-worker0 

virsh undefine ocp4-worker0 

</code></pre>
<h1 id="try-with-machine-config-with-tunned-this-is-diy-if-you-like-"><a class="header" href="#try-with-machine-config-with-tunned-this-is-diy-if-you-like-">try with machine config with tunned, this is DIY if you like :)</a></h1>
<p>machine config的办法，特点是定制化程度很高，如果客户之前用rt-kernel的操作系统，调优过应用，那么用machine config的方法，能够直接把客户之前的调优参数于应用过来，就不用纠结各种调优的参数，在openshift4上面，应该怎么配置进去了。</p>
<p>you can use machine config dirctly, this can give you full customization capabilities. If you customer already fine-tune kernel parameter on rt-kernel, you can use their kernel parameter directly on openshift4 without try the parameters by yourself.</p>
<pre><code class="language-bash"># 打开节点的real time kernel
# cat &lt;&lt; EOF &gt; /data/install/99-worker-realtime.yaml
# apiVersion: machineconfiguration.openshift.io/v1
# kind: MachineConfig
# metadata:
#   labels:
#     machineconfiguration.openshift.io/role: &quot;worker-rt&quot;
#   name: 99-worker-realtime
# spec:
#   kernelType: realtime
# EOF
# oc create -f  /data/install/99-worker-realtime.yaml

# 配置kernel启动参数，每个参数一行
# http://abcdxyzk.github.io/blog/2015/02/11/kernel-base-param/
# no_timer_check clocksource=tsc tsc=perfect intel_pstate=disable selinux=0 enforcing=0 nmi_watchdog=0 softlockup_panic=0 isolcpus=2-19 nohz_full=2-19 idle=poll default_hugepagesz=1G hugepagesz=1G hugepages=32  skew_tick=1 rcu_nocbs=2-19 kthread_cpus=0-1 irqaffinity=0-1 rcu_nocb_poll iommu=pt intel_iommu=on
cat &lt;&lt; EOF &gt; /data/install/05-worker-kernelarg-realtime.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker-rt
  name: 05-worker-kernelarg-realtime
spec:
  config:
    ignition:
      version: 3.1.0
  kernelArguments:
    - no_timer_check  # 禁止运行内核中时钟IRQ源缺陷检测代码。主要用于解决某些AMD平台的CPU占用过高以及时钟过快的故障。
    - clocksource=tsc # clocksource={jiffies|acpi_pm|hpet|tsc} tsc TSC(Time Stamp Counter)的主体是位于CPU里面的一个64位TSC寄存器，与传统的以中断形式存在的周期性时钟不同，TSC是以计数器形式存在的单步递增性时钟，两者的区别在于，周期性时钟是通过周期性触发中断达到计时目的，如心跳一般。而单步递增时钟则不发送中断，取而代之的是由软件自己在需要的时候去主动读取TSC寄存器的值来获得时间。TSC的精度更高并且速度更快，但仅能在较新的CPU(Sandy Bridge之后)上使用。
    - tsc=perfect
    - intel_pstate=disable  # intel_pstate驱动支持现代Intel处理器的温控。 intel_pstate=disable选项可以强制使用传统遗留的CPU驱动acpi_cpufreq
    - selinux=0
    - enforcing=0
    - nmi_watchdog=0  # 配置nmi_watchdog(不可屏蔽中断看门狗) 0 表示关闭看门狗；
    - softlockup_panic=0  # 是否在检测到软死锁(soft-lockup)的时候让内核panic
    - isolcpus=2-19 # 将列表中的CPU从内核SMP平衡和调度算法中剔除。 提出后并不是绝对不能再使用该CPU的，操作系统仍然可以强制指定特定的进程使用哪个CPU(可以通过taskset来做到)。该参数的目的主要是用于实现特定cpu只运行特定进程的目的。
    - nohz_full=2-19  #在 16 核的系统中，设定 nohz_full=1-15 可以在 1 到 15 内核中启用动态无时钟内核性能，并将所有的计时移动至唯一未设定的内核中（0 内核）, [注意](1)&quot;boot CPU&quot;(通常都是&quot;0&quot;号CPU)会无条件的从列表中剔除。(2)这里列出的CPU编号必须也要同时列进&quot;rcu_nocbs=...&quot;参数中。
    - idle=poll # 对CPU进入休眠状态的额外设置。poll 从根本上禁用休眠功能(也就是禁止进入C-states状态)，可以略微提升一些CPU性能，但是却需要多消耗许多电力，得不偿失。不推荐使用。
    - default_hugepagesz=1G
    - hugepagesz=1G
    - hugepages=32
    - skew_tick=1 # Offset the periodic timer tick per cpu to mitigate xtime_lock contention on larger systems, and/or RCU lock contention on all systems with CONFIG_MAXSMP set. Note: increases power consumption, thus should only be enabled if running jitter sensitive (HPC/RT) workloads.
    - rcu_nocbs=2-19  # 指定哪些CPU是No-CB CPU
    - kthread_cpus=0-1
    - irqaffinity=0-1 # 通过内核参数irqaffinity==[cpu列表],设置linux中断的亲和性，设置后，默认由这些cpu核来处理非CPU绑定中断。避免linux中断影响cpu2、cpu3上的实时应用，将linux中断指定到cpu0、cpu1处理。
    - rcu_nocb_poll # 减少了需要从卸载cpu执行唤醒操作。避免了rcuo kthreads线程显式的唤醒。另一方面这会增加耗电量
    - iommu=pt
    - intel_iommu=on
  kernelType: realtime
EOF
oc create -f /data/install/05-worker-kernelarg-realtime.yaml

# 一般都需要 cpu/numa 绑核，这个在 kubelet 的配置里面做
cat &lt;&lt; EOF &gt; /data/install/cpumanager-kubeletconfig.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: cpumanager-enabled
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: cpumanager-enabled
  kubeletConfig:
     cpuManagerPolicy: static 
     cpuManagerReconcilePeriod: 5s 
     topologyManagerPolicy: single-numa-node 
     reservedSystemCPUs: &quot;0,1&quot; 
EOF
oc create -f  /data/install/cpumanager-kubeletconfig.yaml

# 以下如果在 bios 里面关掉了，就不用做了。
# if irqbalance disabled in bios, you can skip below step.
# cat &lt;&lt; EOF &gt; /data/install/99-custom-disable-irqbalance-worker.yaml
# apiVersion: machineconfiguration.openshift.io/v1
# kind: MachineConfig
# metadata:
#     labels:
#         machineconfiguration.openshift.io/role: worker-rt
#     name: 99-custom-disable-irqbalance-worker
# spec:
#     config:
#         ignition:
#             version: 2.2.0
#         systemd:
#             units:
#             - enabled: false
#               mask: true
#               name: irqbalance.service
# EOF
# oc create -f /data/install/99-custom-disable-irqbalance-worker.yaml


# 我们基于performace addon ， 改一下他的例子， 这次我们基于 realtime
cat &lt;&lt; EOF &gt; /data/install/tuned.yaml
apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: wzh-realtime
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
  - data: |
      [main]
      summary=wzh version for realtime, 5G RAN
      include=openshift-node,realtime

      # Different values will override the original values in parent profiles.

      [variables]
      # isolated_cores take a list of ranges; e.g. isolated_cores=2,4-7

      isolated_cores=2-19
      isolate_managed_irq=Y

      [service]
      service.stalld=start,enable

    name: wzh-realtime
  recommend:
  - machineConfigLabels:
      machineconfiguration.openshift.io/role: worker-rt
    priority: 20
    profile: wzh-realtime
EOF
oc create -f /data/install/tuned.yaml

# to restore
oc delete -f  /data/install/tuned.yaml

# https://zhuanlan.zhihu.com/p/336381111
# yum install rt-test
# 在测试现场，经过整个晚上的测试，可以看到系统的实时性非常好
# 目标结果，最大不应超过 6μs
cyclictest -m -p95 -d0 -a 2-17 -t 16
</code></pre>
<p><img src="ocp4/4.7/imgs/2021-05-25-10-18-23.png" alt="" /></p>
<h1 id="try-to-deploy-a-vdu-pod-using-yaml"><a class="header" href="#try-to-deploy-a-vdu-pod-using-yaml">try to deploy a vDU pod, using yaml</a></h1>
<pre><code class="language-yaml">---

apiVersion: &quot;k8s.cni.cncf.io/v1&quot;
kind: NetworkAttachmentDefinition
metadata:
  name: host-device-du
spec:
  config: '{
    &quot;cniVersion&quot;: &quot;0.3.0&quot;,
    &quot;type&quot;: &quot;host-device&quot;,
    &quot;device&quot;: &quot;ens81f1np1&quot;,
    &quot;ipam&quot;: {
      &quot;type&quot;: &quot;host-local&quot;,
      &quot;subnet&quot;: &quot;192.168.12.0/24&quot;,
      &quot;rangeStart&quot;: &quot;192.168.12.105&quot;,
      &quot;rangeEnd&quot;: &quot;192.168.12.105&quot;,
      &quot;routes&quot;: [{
        &quot;dst&quot;: &quot;0.0.0.0/0&quot;
      }],
      &quot;gateway&quot;: &quot;192.168.12.1&quot;
    }
  }'

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: du-deployment1
  labels:
    app: du-deployment1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: du-pod1
  template:
    metadata:
      labels: 
        app: du-pod1
      annotations:
        k8s.v1.cni.cncf.io/networks: '[
          { &quot;name&quot;: &quot;host-device-du&quot;,
            &quot;interface&quot;: &quot;net1&quot; }
          ]'
    spec:
      containers:
      - name: du-container1
        image: &quot;registry.ocp4.redhat.ren:5443/ocp4/centos:7.6.1810&quot;
        imagePullPolicy: IfNotPresent
        tty: true
        stdin: true
        env:
          - name: duNetProviderDriver
            value: &quot;host-netdevice&quot;
        command:
          - sleep
          - infinity
        securityContext:
            privileged: true
            capabilities:
                add:
                - CAP_SYS_ADMIN
        volumeMounts:
          - mountPath: /hugepages
            name: hugepage
          - name: lib-modules
            mountPath: /lib/modules
          - name: src
            mountPath: /usr/src
          - name: dev
            mountPath: /dev
          - name: cache-volume
            mountPath: /dev/shm
        resources:
          requests:
            cpu: 16
            memory: 48Gi
            hugepages-1Gi: 8Gi
          limits:
            cpu: 16
            memory: 48Gi
            hugepages-1Gi: 8Gi
      volumes:
        - name: hugepage
          emptyDir:
            medium: HugePages
        - name: lib-modules
          hostPath:
            path: /lib/modules
        - name: src
          hostPath:
            path: /usr/src
        - name: dev
          hostPath:
            path: &quot;/dev&quot;
        - name: cache-volume
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi
      nodeSelector:
        node-role.kubernetes.io/worker-rt: &quot;&quot;
</code></pre>
<h2 id="research"><a class="header" href="#research">research</a></h2>
<pre><code class="language-bash">
oc get Tuned -n openshift-cluster-node-tuning-operator
# NAME                                                    AGE
# default                                                 18d
# openshift-node-performance-example-performanceprofile   12d
# rendered                                                18d

oc get Tuned/default -o yaml -n openshift-cluster-node-tuning-operator
</code></pre>
<pre><code class="language-yaml">apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  creationTimestamp: &quot;2021-05-05T16:09:36Z&quot;
  generation: 1
  name: default
  namespace: openshift-cluster-node-tuning-operator
  resourceVersion: &quot;6067&quot;
  selfLink: /apis/tuned.openshift.io/v1/namespaces/openshift-cluster-node-tuning-operator/tuneds/default
  uid: 205c01c5-2609-4f2f-b676-ad746ea3c9f3
spec:
  profile:
  - data: |
      [main]
      summary=Optimize systems running OpenShift (parent profile)
      include=${f:virt_check:virtual-guest:throughput-performance}

      [selinux]
      avc_cache_threshold=8192

      [net]
      nf_conntrack_hashsize=131072

      [sysctl]
      net.ipv4.ip_forward=1
      kernel.pid_max=&gt;4194304
      net.netfilter.nf_conntrack_max=1048576
      net.ipv4.conf.all.arp_announce=2
      net.ipv4.neigh.default.gc_thresh1=8192
      net.ipv4.neigh.default.gc_thresh2=32768
      net.ipv4.neigh.default.gc_thresh3=65536
      net.ipv6.neigh.default.gc_thresh1=8192
      net.ipv6.neigh.default.gc_thresh2=32768
      net.ipv6.neigh.default.gc_thresh3=65536
      vm.max_map_count=262144

      [sysfs]
      /sys/module/nvme_core/parameters/io_timeout=4294967295
      /sys/module/nvme_core/parameters/max_retries=10
    name: openshift
  - data: |
      [main]
      summary=Optimize systems running OpenShift control plane
      include=openshift

      [sysctl]
      # ktune sysctl settings, maximizing i/o throughput
      #
      # Minimal preemption granularity for CPU-bound tasks:
      # (default: 1 msec#  (1 + ilog(ncpus)), units: nanoseconds)
      kernel.sched_min_granularity_ns=10000000
      # The total time the scheduler will consider a migrated process
      # &quot;cache hot&quot; and thus less likely to be re-migrated
      # (system default is 500000, i.e. 0.5 ms)
      kernel.sched_migration_cost_ns=5000000
      # SCHED_OTHER wake-up granularity.
      #
      # Preemption granularity when tasks wake up.  Lower the value to
      # improve wake-up latency and throughput for latency critical tasks.
      kernel.sched_wakeup_granularity_ns=4000000
    name: openshift-control-plane
  - data: |
      [main]
      summary=Optimize systems running OpenShift nodes
      include=openshift

      [sysctl]
      net.ipv4.tcp_fastopen=3
      fs.inotify.max_user_watches=65536
      fs.inotify.max_user_instances=8192
    name: openshift-node
  recommend:
  - match:
    - label: node-role.kubernetes.io/master
    - label: node-role.kubernetes.io/infra
    operand:
      debug: false
    priority: 30
    profile: openshift-control-plane
  - operand:
      debug: false
    priority: 40
    profile: openshift-node
status: {}
</code></pre>
<pre><code class="language-bas">oc get Tuned/openshift-node-performance-example-performanceprofile -o yaml -n openshift-cluster-node-tuning-operator
</code></pre>
<pre><code class="language-yaml">apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: openshift-node-performance-example-performanceprofile
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
  - data: &quot;[main]\nsummary=Openshift node optimized for deterministic performance at the cost of increased power consumption, focused on low latency network performance. Based on Tuned 2.11 and Cluster node tuning (oc 4.5)\ninclude=openshift-node,cpu-partitioning\n\n# Inheritance of base profiles legend:\n# cpu-partitioning -&gt; network-latency -&gt; latency-performance\n# https://github.com/redhat-performance/tuned/blob/master/profiles/latency-performance/tuned.conf\n# https://github.com/redhat-performance/tuned/blob/master/profiles/network-latency/tuned.conf\n# https://github.com/redhat-performance/tuned/blob/master/profiles/cpu-partitioning/tuned.conf\n\n# All values are mapped with a comment where a parent profile contains them.\n# Different values will override the original values in parent profiles.\n\n[variables]\n# isolated_cores take a list of ranges; e.g. isolated_cores=2,4-7\n\nisolated_cores=2-3 \n\n\nnot_isolated_cores_expanded=${f:cpulist_invert:${isolated_cores_expanded}}\n\n[cpu]\nforce_latency=cstate.id:1|3                   #  latency-performance  (override)\ngovernor=performance                          #  latency-performance \nenergy_perf_bias=performance                  #  latency-performance \nmin_perf_pct=100                              #  latency-performance \n\n[service]\nservice.stalld=start,enable\n\n[vm]\ntransparent_hugepages=never                   #  network-latency\n\n\n[irqbalance]\n# Override the value set by cpu-partitioning with an empty one\nbanned_cpus=\&quot;\&quot;\n\n\n[scheduler]\ngroup.ksoftirqd=0:f:11:*:ksoftirqd.*\ngroup.rcuc=0:f:11:*:rcuc.*\n\ndefault_irq_smp_affinity = ignore\n\n\n[sysctl]\nkernel.hung_task_timeout_secs = 600           # cpu-partitioning #realtime\nkernel.nmi_watchdog = 0                       # cpu-partitioning #realtime\nkernel.sched_rt_runtime_us = -1               # realtime \nkernel.timer_migration = 0                    # cpu-partitioning (= 1) #realtime (= 0)\nkernel.numa_balancing=0                       # network-latency\nnet.core.busy_read=50                         # network-latency\nnet.core.busy_poll=50                         # network-latency\nnet.ipv4.tcp_fastopen=3                       # network-latency\nvm.stat_interval = 10                         # cpu-partitioning  #realtime\n\n# ktune sysctl settings for rhel6 servers, maximizing i/o throughput\n#\n# Minimal preemption granularity for CPU-bound tasks:\n# (default: 1 msec#  (1 + ilog(ncpus)), units: nanoseconds)\nkernel.sched_min_granularity_ns=10000000      # latency-performance\n\n# If a workload mostly uses anonymous memory and it hits this limit, the entire\n# working set is buffered for I/O, and any more write buffering would require\n# swapping, so it's time to throttle writes until I/O can catch up.  Workloads\n# that mostly use file mappings may be able to use even higher values.\n#\n# The generator of dirty data starts writeback at this percentage (system default\n# is 20%)\nvm.dirty_ratio=10                             # latency-performance\n\n# Start background writeback (via writeback threads) at this percentage (system\n# default is 10%)\nvm.dirty_background_ratio=3                   # latency-performance\n\n# The swappiness parameter controls the tendency of the kernel to move\n# processes out of physical memory and onto the swap disk.\n# 0 tells the kernel to avoid swapping processes out of physical memory\n# for as long as possible\n# 100 tells the kernel to aggressively swap processes out of physical memory\n# and move them to swap cache\nvm.swappiness=10                              # latency-performance\n\n# The total time the scheduler will consider a migrated process\n# \&quot;cache hot\&quot; and thus less likely to be re-migrated\n# (system default is 500000, i.e. 0.5 ms)\nkernel.sched_migration_cost_ns=5000000        # latency-performance\n\n[selinux]\navc_cache_threshold=8192                      # Custom (atomic host)\n\n[net]\nnf_conntrack_hashsize=131072                  # Custom (atomic host)\n\n[bootloader]\n# set empty values to disable RHEL initrd setting in cpu-partitioning \ninitrd_remove_dir=     \ninitrd_dst_img=\ninitrd_add_dir=\n# overrides cpu-partitioning cmdline\ncmdline_cpu_part=+nohz=on rcu_nocbs=${isolated_cores} tuned.non_isolcpus=${not_isolated_cpumask} intel_pstate=disable nosoftlockup\n\ncmdline_realtime=+tsc=nowatchdog intel_iommu=on iommu=pt isolcpus=managed_irq,${isolated_cores} systemd.cpu_affinity=${not_isolated_cores_expanded}\n\ncmdline_hugepages=+ default_hugepagesz=1G  \ncmdline_additionalArg=+\n&quot;
    name: openshift-node-performance-example-performanceprofile
  recommend:
  - machineConfigLabels:
      machineconfiguration.openshift.io/role: worker-rt
    priority: 20
    profile: openshift-node-performance-example-performanceprofile
status: {}
</code></pre>
<pre><code class="language-yaml">apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: openshift-node-performance-example-performanceprofile
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
  - data: |
      [main]
      summary=Openshift node optimized for deterministic performance at the cost of increased power consumption, focused on low latency network performance. Based on Tuned 2.11 and Cluster node tuning (oc 4.5)
      include=openshift-node,cpu-partitioning

      # Inheritance of base profiles legend:
      # cpu-partitioning -&gt; network-latency -&gt; latency-performance
      # https://github.com/redhat-performance/tuned/blob/master/profiles/latency-performance/tuned.conf
      # https://github.com/redhat-performance/tuned/blob/master/profiles/network-latency/tuned.conf
      # https://github.com/redhat-performance/tuned/blob/master/profiles/cpu-partitioning/tuned.conf

      # All values are mapped with a comment where a parent profile contains them.
      # Different values will override the original values in parent profiles.

      [variables]
      # isolated_cores take a list of ranges; e.g. isolated_cores=2,4-7

      isolated_cores=2-3


      not_isolated_cores_expanded=

      [cpu]
      force_latency=cstate.id:1|3                   #  latency-performance  (override)
      governor=performance                          #  latency-performance
      energy_perf_bias=performance                  #  latency-performance
      min_perf_pct=100                              #  latency-performance

      [service]
      service.stalld=start,enable

      [vm]
      transparent_hugepages=never                   #  network-latency


      [irqbalance]
      # Override the value set by cpu-partitioning with an empty one
      banned_cpus=&quot;&quot;


      [scheduler]
      group.ksoftirqd=0:f:11:*:ksoftirqd.*
      group.rcuc=0:f:11:*:rcuc.*

      default_irq_smp_affinity = ignore


      [sysctl]
      kernel.hung_task_timeout_secs = 600           # cpu-partitioning #realtime
      kernel.nmi_watchdog = 0                       # cpu-partitioning #realtime
      kernel.sched_rt_runtime_us = -1               # realtime
      kernel.timer_migration = 0                    # cpu-partitioning (= 1) #realtime (= 0)
      kernel.numa_balancing=0                       # network-latency
      net.core.busy_read=50                         # network-latency
      net.core.busy_poll=50                         # network-latency
      net.ipv4.tcp_fastopen=3                       # network-latency
      vm.stat_interval = 10                         # cpu-partitioning  #realtime

      # ktune sysctl settings for rhel6 servers, maximizing i/o throughput
      #
      # Minimal preemption granularity for CPU-bound tasks:
      # (default: 1 msec#  (1 + ilog(ncpus)), units: nanoseconds)
      kernel.sched_min_granularity_ns=10000000      # latency-performance

      # If a workload mostly uses anonymous memory and it hits this limit, the entire
      # working set is buffered for I/O, and any more write buffering would require
      # swapping, so it's time to throttle writes until I/O can catch up.  Workloads
      # that mostly use file mappings may be able to use even higher values.
      #
      # The generator of dirty data starts writeback at this percentage (system default
      # is 20%)
      vm.dirty_ratio=10                             # latency-performance

      # Start background writeback (via writeback threads) at this percentage (system
      # default is 10%)
      vm.dirty_background_ratio=3                   # latency-performance

      # The swappiness parameter controls the tendency of the kernel to move
      # processes out of physical memory and onto the swap disk.
      # 0 tells the kernel to avoid swapping processes out of physical memory
      # for as long as possible
      # 100 tells the kernel to aggressively swap processes out of physical memory
      # and move them to swap cache
      vm.swappiness=10                              # latency-performance

      # The total time the scheduler will consider a migrated process
      # &quot;cache hot&quot; and thus less likely to be re-migrated
      # (system default is 500000, i.e. 0.5 ms)
      kernel.sched_migration_cost_ns=5000000        # latency-performance

      [selinux]
      avc_cache_threshold=8192                      # Custom (atomic host)

      [net]
      nf_conntrack_hashsize=131072                  # Custom (atomic host)

      [bootloader]
      # set empty values to disable RHEL initrd setting in cpu-partitioning
      initrd_remove_dir=
      initrd_dst_img=
      initrd_add_dir=
      # overrides cpu-partitioning cmdline
      cmdline_cpu_part=+nohz=on rcu_nocbs= tuned.non_isolcpus= intel_pstate=disable nosoftlockup

      cmdline_realtime=+tsc=nowatchdog intel_iommu=on iommu=pt isolcpus=managed_irq, systemd.cpu_affinity=

      cmdline_hugepages=+ default_hugepagesz=1G
      cmdline_additionalArg=+
    name: openshift-node-performance-example-performanceprofile
  recommend:
  - machineConfigLabels:
      machineconfiguration.openshift.io/role: worker-rt
    priority: 20
    profile: openshift-node-performance-example-performanceprofile
</code></pre>
<pre><code class="language-bash"># tuned 的配置，如果有些在bios里面做了，那么也可以忽略。我们基于performace addon ， 改一下他的例子.
cat &lt;&lt; EOF &gt; /data/install/tuned.yaml
apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: openshift-node-wzh-performance-profile
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
  - data: |
      [main]
      summary=Openshift node optimized for deterministic performance at the cost of increased power consumption, focused on low latency network performance. Based on Tuned 2.11 and Cluster node tuning (oc 4.5)
      include=openshift-node,cpu-partitioning

      # Inheritance of base profiles legend:
      # cpu-partitioning -&gt; network-latency -&gt; latency-performance
      # https://github.com/redhat-performance/tuned/blob/master/profiles/latency-performance/tuned.conf
      # https://github.com/redhat-performance/tuned/blob/master/profiles/network-latency/tuned.conf
      # https://github.com/redhat-performance/tuned/blob/master/profiles/cpu-partitioning/tuned.conf

      # All values are mapped with a comment where a parent profile contains them.
      # Different values will override the original values in parent profiles.

      [variables]
      # isolated_cores take a list of ranges; e.g. isolated_cores=2,4-7

      isolated_cores=2-19
      isolate_managed_irq=Y

      not_isolated_cores_expanded=

      [cpu]
      # force_latency=cstate.id:1|3                   #  latency-performance  (override)
      governor=performance                          #  latency-performance
      energy_perf_bias=performance                  #  latency-performance
      min_perf_pct=100                              #  latency-performance

      [service]
      service.stalld=start,enable

      [vm]
      transparent_hugepages=never                   #  network-latency


      [irqbalance]
      # Override the value set by cpu-partitioning with an empty one
      banned_cpus=&quot;&quot;


      [scheduler]
      group.ksoftirqd=0:f:11:*:ksoftirqd.*
      group.rcuc=0:f:11:*:rcuc.*

      default_irq_smp_affinity = ignore


      [sysctl]
      kernel.hung_task_timeout_secs = 600           # cpu-partitioning #realtime
      kernel.nmi_watchdog = 0                       # cpu-partitioning #realtime
      kernel.sched_rt_runtime_us = -1               # realtime
      kernel.timer_migration = 0                    # cpu-partitioning (= 1) #realtime (= 0)
      kernel.numa_balancing=0                       # network-latency
      net.core.busy_read=50                         # network-latency
      net.core.busy_poll=50                         # network-latency
      net.ipv4.tcp_fastopen=3                       # network-latency
      vm.stat_interval = 10                         # cpu-partitioning  #realtime

      # ktune sysctl settings for rhel6 servers, maximizing i/o throughput
      #
      # Minimal preemption granularity for CPU-bound tasks:
      # (default: 1 msec#  (1 + ilog(ncpus)), units: nanoseconds)
      kernel.sched_min_granularity_ns=10000000      # latency-performance

      # If a workload mostly uses anonymous memory and it hits this limit, the entire
      # working set is buffered for I/O, and any more write buffering would require
      # swapping, so it's time to throttle writes until I/O can catch up.  Workloads
      # that mostly use file mappings may be able to use even higher values.
      #
      # The generator of dirty data starts writeback at this percentage (system default
      # is 20%)
      vm.dirty_ratio=10                             # latency-performance

      # Start background writeback (via writeback threads) at this percentage (system
      # default is 10%)
      vm.dirty_background_ratio=3                   # latency-performance

      # The swappiness parameter controls the tendency of the kernel to move
      # processes out of physical memory and onto the swap disk.
      # 0 tells the kernel to avoid swapping processes out of physical memory
      # for as long as possible
      # 100 tells the kernel to aggressively swap processes out of physical memory
      # and move them to swap cache
      vm.swappiness=10                              # latency-performance

      # The total time the scheduler will consider a migrated process
      # &quot;cache hot&quot; and thus less likely to be re-migrated
      # (system default is 500000, i.e. 0.5 ms)
      kernel.sched_migration_cost_ns=5000000        # latency-performance

      [selinux]
      avc_cache_threshold=8192                      # Custom (atomic host)

      [net]
      nf_conntrack_hashsize=131072                  # Custom (atomic host)

      [bootloader]
      # set empty values to disable RHEL initrd setting in cpu-partitioning
      initrd_remove_dir=
      initrd_dst_img=
      initrd_add_dir=
      # overrides cpu-partitioning cmdline
      cmdline_cpu_part=+nohz=on rcu_nocbs= tuned.non_isolcpus= intel_pstate=disable nosoftlockup

      cmdline_realtime=+tsc=nowatchdog intel_iommu=on iommu=pt isolcpus=managed_irq, systemd.cpu_affinity=

      cmdline_hugepages=+ default_hugepagesz=1G
      cmdline_additionalArg=+
    name: openshift-node-wzh-performance-profile
  recommend:
  - machineConfigLabels:
      machineconfiguration.openshift.io/role: worker-rt
    priority: 20
    profile: openshift-node-wzh-performance-profile
EOF
oc create -f /data/install/tuned.yaml

# 用了performance example的profile 效果很好
cyclictest -m -p95 -d0 -a 2-17 -t 16
</code></pre>
<p><img src="ocp4/4.7/imgs/2021-05-26-21-01-57.png" alt="" /></p>
<h1 id="example-config"><a class="header" href="#example-config">example config</a></h1>
<pre><code class="language-bash">oc get mc
</code></pre>
<pre><code>NAME                                                  GENERATEDBYCONTROLLER                      IGNITIONVERSION   AGE
00-master                                             791d1cc2626d1e4e5da59f15c1a6166fd398aef8   3.1.0             62d
00-worker                                             791d1cc2626d1e4e5da59f15c1a6166fd398aef8   3.1.0             62d
01-master-container-runtime                           791d1cc2626d1e4e5da59f15c1a6166fd398aef8   3.1.0             62d
01-master-kubelet                                     791d1cc2626d1e4e5da59f15c1a6166fd398aef8   3.1.0             62d
01-worker-container-runtime                           791d1cc2626d1e4e5da59f15c1a6166fd398aef8   3.1.0             62d
01-worker-kubelet                                     791d1cc2626d1e4e5da59f15c1a6166fd398aef8   3.1.0             62d
05-worker-kernelarg-rtran                                                                        3.1.0             62d
50-nto-worker-rt                                                                                 3.1.0             58d
99-master-generated-registries                        791d1cc2626d1e4e5da59f15c1a6166fd398aef8   3.1.0             62d
99-master-ssh                                                                                    3.1.0             62d
99-worker-generated-registries                        791d1cc2626d1e4e5da59f15c1a6166fd398aef8   3.1.0             62d
99-worker-realtime                                                                                                 62d
99-worker-rt-generated-kubelet                        791d1cc2626d1e4e5da59f15c1a6166fd398aef8   3.1.0             58d
99-worker-ssh                                                                                    3.1.0             62d
load-sctp-module                                                                                 3.1.0             6d9h
rendered-master-0629f16bcba29a60e894f3d9e14e47b9      791d1cc2626d1e4e5da59f15c1a6166fd398aef8   3.1.0             62d
rendered-worker-7497d1b2e86631a4f390a6eba0aef74f      791d1cc2626d1e4e5da59f15c1a6166fd398aef8   3.1.0             62d
rendered-worker-rt-1e40da418635be6c6b81ebc33a1f0640   791d1cc2626d1e4e5da59f15c1a6166fd398aef8   3.1.0             62d
rendered-worker-rt-35d27df9ed0ff75a6a192700313a88f8   791d1cc2626d1e4e5da59f15c1a6166fd398aef8   3.1.0             58d
rendered-worker-rt-3e87a41fe1e455977a4a972f8d4258aa   791d1cc2626d1e4e5da59f15c1a6166fd398aef8   3.1.0             58d
rendered-worker-rt-4ba64193fdbace8fc101541335067ad4   791d1cc2626d1e4e5da59f15c1a6166fd398aef8   3.1.0             62d
rendered-worker-rt-7497d1b2e86631a4f390a6eba0aef74f   791d1cc2626d1e4e5da59f15c1a6166fd398aef8   3.1.0             62d
rendered-worker-rt-9cf8ebbc1c0cf88bb3a9716b6d66e60e   791d1cc2626d1e4e5da59f15c1a6166fd398aef8   3.1.0             58d
rendered-worker-rt-bb3c16a689e7797fb4c828cec877c9ed   791d1cc2626d1e4e5da59f15c1a6166fd398aef8   3.1.0             58d
rendered-worker-rt-ea53e6c4fc58b5f9f505ebed3cb32345   791d1cc2626d1e4e5da59f15c1a6166fd398aef8   3.1.0             58d
rendered-worker-rt-fd13902df04099f149d7653da3552f5d   791d1cc2626d1e4e5da59f15c1a6166fd398aef8   3.1.0             6d9h
</code></pre>
<pre><code class="language-bash">oc get mc/05-worker-kernelarg-rtran -o json | jq &quot;del(.metadata.managedFields, .metadata.uid, .metadata.selfLink, .metadata.resourceVersion, .metadata.generation, .metadata.creationTimestamp)&quot;
</code></pre>
<pre><code class="language-json">{
  &quot;apiVersion&quot;: &quot;machineconfiguration.openshift.io/v1&quot;,
  &quot;kind&quot;: &quot;MachineConfig&quot;,
  &quot;metadata&quot;: {
    &quot;labels&quot;: {
      &quot;machineconfiguration.openshift.io/role&quot;: &quot;worker-rt&quot;
    },
    &quot;name&quot;: &quot;05-worker-kernelarg-rtran&quot;
  },
  &quot;spec&quot;: {
    &quot;config&quot;: {
      &quot;ignition&quot;: {
        &quot;version&quot;: &quot;3.1.0&quot;
      }
    },
    &quot;kernelArguments&quot;: [
      &quot;no_timer_check&quot;,
      &quot;clocksource=tsc&quot;,
      &quot;tsc=perfect&quot;,
      &quot;selinux=0&quot;,
      &quot;enforcing=0&quot;,
      &quot;nmi_watchdog=0&quot;,
      &quot;softlockup_panic=0&quot;,
      &quot;isolcpus=2-19&quot;,
      &quot;nohz_full=2-19&quot;,
      &quot;idle=poll&quot;,
      &quot;default_hugepagesz=1G&quot;,
      &quot;hugepagesz=1G&quot;,
      &quot;hugepages=16&quot;,
      &quot;skew_tick=1&quot;,
      &quot;rcu_nocbs=2-19&quot;,
      &quot;kthread_cpus=0-1&quot;,
      &quot;irqaffinity=0-1&quot;,
      &quot;rcu_nocb_poll&quot;,
      &quot;iommu=pt&quot;,
      &quot;intel_iommu=on&quot;
    ]
  }
}
</code></pre>
<pre><code class="language-bash">oc get mc/50-nto-worker-rt -o json | jq &quot;del(.metadata.managedFields, .metadata.uid, .metadata.selfLink, .metadata.resourceVersion, .metadata.generation, .metadata.creationTimestamp)&quot;
</code></pre>
<pre><code class="language-json">{
  &quot;apiVersion&quot;: &quot;machineconfiguration.openshift.io/v1&quot;,
  &quot;kind&quot;: &quot;MachineConfig&quot;,
  &quot;metadata&quot;: {
    &quot;annotations&quot;: {
      &quot;tuned.openshift.io/generated-by-controller-version&quot;: &quot;v4.6.0-202104221811.p0-0-gfdb7aec-dirty&quot;
    },
    &quot;labels&quot;: {
      &quot;machineconfiguration.openshift.io/role&quot;: &quot;worker-rt&quot;
    },
    &quot;name&quot;: &quot;50-nto-worker-rt&quot;
  },
  &quot;spec&quot;: {
    &quot;config&quot;: {
      &quot;ignition&quot;: {
        &quot;config&quot;: {
          &quot;replace&quot;: {
            &quot;verification&quot;: {}
          }
        },
        &quot;proxy&quot;: {},
        &quot;security&quot;: {
          &quot;tls&quot;: {}
        },
        &quot;timeouts&quot;: {},
        &quot;version&quot;: &quot;3.1.0&quot;
      },
      &quot;passwd&quot;: {},
      &quot;storage&quot;: {},
      &quot;systemd&quot;: {}
    },
    &quot;extensions&quot;: null,
    &quot;fips&quot;: false,
    &quot;kernelArguments&quot;: [
      &quot;skew_tick=1&quot;,
      &quot;isolcpus=managed_irq,domain,2-19&quot;,
      &quot;intel_pstate=disable&quot;,
      &quot;nosoftlockup&quot;,
      &quot;tsc=nowatchdog&quot;
    ],
    &quot;kernelType&quot;: &quot;&quot;,
    &quot;osImageURL&quot;: &quot;&quot;
  }
}
</code></pre>
<pre><code class="language-bash">oc get mc/99-worker-realtime -o json | jq &quot;del(.metadata.managedFields, .metadata.uid, .metadata.selfLink, .metadata.resourceVersion, .metadata.generation, .metadata.creationTimestamp)&quot;
</code></pre>
<pre><code class="language-json">{
  &quot;apiVersion&quot;: &quot;machineconfiguration.openshift.io/v1&quot;,
  &quot;kind&quot;: &quot;MachineConfig&quot;,
  &quot;metadata&quot;: {
    &quot;labels&quot;: {
      &quot;machineconfiguration.openshift.io/role&quot;: &quot;worker-rt&quot;
    },
    &quot;name&quot;: &quot;99-worker-realtime&quot;
  },
  &quot;spec&quot;: {
    &quot;kernelType&quot;: &quot;realtime&quot;
  }
}
</code></pre>
<pre><code class="language-bash">oc get mc/load-sctp-module -o json | jq &quot;del(.metadata.managedFields, .metadata.uid, .metadata.selfLink, .metadata.resourceVersion, .metadata.generation, .metadata.creationTimestamp)&quot;
</code></pre>
<pre><code class="language-json">{
  &quot;apiVersion&quot;: &quot;machineconfiguration.openshift.io/v1&quot;,
  &quot;kind&quot;: &quot;MachineConfig&quot;,
  &quot;metadata&quot;: {
    &quot;labels&quot;: {
      &quot;machineconfiguration.openshift.io/role&quot;: &quot;worker-rt&quot;
    },
    &quot;name&quot;: &quot;load-sctp-module&quot;
  },
  &quot;spec&quot;: {
    &quot;config&quot;: {
      &quot;ignition&quot;: {
        &quot;version&quot;: &quot;3.1.0&quot;
      },
      &quot;storage&quot;: {
        &quot;files&quot;: [
          {
            &quot;contents&quot;: {
              &quot;source&quot;: &quot;data:,&quot;
            },
            &quot;mode&quot;: 420,
            &quot;overwrite&quot;: true,
            &quot;path&quot;: &quot;/etc/modprobe.d/sctp-blacklist.conf&quot;
          },
          {
            &quot;contents&quot;: {
              &quot;source&quot;: &quot;data:,sctp&quot;
            },
            &quot;mode&quot;: 420,
            &quot;overwrite&quot;: true,
            &quot;path&quot;: &quot;/etc/modules-load.d/sctp-load.conf&quot;
          }
        ]
      }
    }
  }
}

</code></pre>
<pre><code class="language-bash">oc get Tuned -n openshift-cluster-node-tuning-operator
NAME           AGE
default        62d
rendered       62d
wzh-realtime   58d

oc get Tuned/wzh-realtime -n openshift-cluster-node-tuning-operator -o json | jq &quot;del(.metadata.managedFields, .metadata.uid, .metadata.selfLink, .metadata.resourceVersion, .metadata.generation, .metadata.creationTimestamp)&quot; 
</code></pre>
<pre><code class="language-json">{
  &quot;apiVersion&quot;: &quot;tuned.openshift.io/v1&quot;,
  &quot;kind&quot;: &quot;Tuned&quot;,
  &quot;metadata&quot;: {
    &quot;name&quot;: &quot;wzh-realtime&quot;,
    &quot;namespace&quot;: &quot;openshift-cluster-node-tuning-operator&quot;
  },
  &quot;spec&quot;: {
    &quot;profile&quot;: [
      {
        &quot;data&quot;: &quot;[main]\nsummary=wzh version for realtime, 5G RAN\ninclude=openshift-node,realtime\n\n# Inheritance of base profiles legend:\n# cpu-partitioning -&gt; network-latency -&gt; latency-performance\n# https://github.com/redhat-performance/tuned/blob/master/profiles/latency-performance/tuned.conf\n# https://github.com/redhat-performance/tuned/blob/master/profiles/network-latency/tuned.conf\n# https://github.com/redhat-performance/tuned/blob/master/profiles/cpu-partitioning/tuned.conf\n\n# All values are mapped with a comment where a parent profile contains them.\n# Different values will override the original values in parent profiles.\n\n[variables]\n# isolated_cores take a list of ranges; e.g. isolated_cores=2,4-7\n\nisolated_cores=2-19\nisolate_managed_irq=Y\n&quot;,
        &quot;name&quot;: &quot;wzh-realtime&quot;
      }
    ],
    &quot;recommend&quot;: [
      {
        &quot;machineConfigLabels&quot;: {
          &quot;machineconfiguration.openshift.io/role&quot;: &quot;worker-rt&quot;
        },
        &quot;priority&quot;: 20,
        &quot;profile&quot;: &quot;wzh-realtime&quot;
      }
    ]
  }
}
</code></pre>
<pre><code class="language-bash">oc get Tuned/wzh-realtime -n openshift-cluster-node-tuning-operator -o json | jq &quot;.spec.profile[0].data&quot; | jq -r
</code></pre>
<pre><code>[main]
summary=wzh version for realtime, 5G RAN
include=openshift-node,realtime

# Inheritance of base profiles legend:
# cpu-partitioning -&gt; network-latency -&gt; latency-performance
# https://github.com/redhat-performance/tuned/blob/master/profiles/latency-performance/tuned.conf
# https://github.com/redhat-performance/tuned/blob/master/profiles/network-latency/tuned.conf
# https://github.com/redhat-performance/tuned/blob/master/profiles/cpu-partitioning/tuned.conf

# All values are mapped with a comment where a parent profile contains them.
# Different values will override the original values in parent profiles.

[variables]
# isolated_cores take a list of ranges; e.g. isolated_cores=2,4-7

isolated_cores=2-19
isolate_managed_irq=Y
</code></pre>
<pre><code class="language-bash">oc get deployment.apps/du-deployment1 -o json | jq &quot;del(.metadata.managedFields, .metadata.uid, .metadata.selfLink, .metadata.resourceVersion, .metadata.generation, .metadata.creationTimestamp)&quot;
</code></pre>
<pre><code class="language-json">{
  &quot;apiVersion&quot;: &quot;apps/v1&quot;,
  &quot;kind&quot;: &quot;Deployment&quot;,
  &quot;metadata&quot;: {
    &quot;annotations&quot;: {
      &quot;deployment.kubernetes.io/revision&quot;: &quot;1&quot;,
      &quot;kubectl.kubernetes.io/last-applied-configuration&quot;: &quot;{\&quot;apiVersion\&quot;:\&quot;apps/v1\&quot;,\&quot;kind\&quot;:\&quot;Deployment\&quot;,\&quot;metadata\&quot;:{\&quot;annotations\&quot;:{},\&quot;labels\&quot;:{\&quot;app\&quot;:\&quot;du-deployment1\&quot;},\&quot;name\&quot;:\&quot;du-deployment1\&quot;,\&quot;namespace\&quot;:\&quot;default\&quot;},\&quot;spec\&quot;:{\&quot;replicas\&quot;:1,\&quot;selector\&quot;:{\&quot;matchLabels\&quot;:{\&quot;app\&quot;:\&quot;du-pod1\&quot;}},\&quot;template\&quot;:{\&quot;metadata\&quot;:{\&quot;annotations\&quot;:{\&quot;k8s.v1.cni.cncf.io/networks\&quot;:\&quot;[ { \\\&quot;name\\\&quot;: \\\&quot;host-device-du\\\&quot;, \\\&quot;interface\\\&quot;: \\\&quot;veth11\\\&quot; } ]\&quot;},\&quot;labels\&quot;:{\&quot;app\&quot;:\&quot;du-pod1\&quot;}},\&quot;spec\&quot;:{\&quot;containers\&quot;:[{\&quot;command\&quot;:[\&quot;sleep\&quot;,\&quot;infinity\&quot;],\&quot;env\&quot;:[{\&quot;name\&quot;:\&quot;duNetProviderDriver\&quot;,\&quot;value\&quot;:\&quot;host-netdevice\&quot;}],\&quot;image\&quot;:\&quot;registry.ocp4.redhat.ren:5443/ocp4/du:v1-wzh\&quot;,\&quot;imagePullPolicy\&quot;:\&quot;IfNotPresent\&quot;,\&quot;name\&quot;:\&quot;du-container1\&quot;,\&quot;resources\&quot;:{\&quot;limits\&quot;:{\&quot;cpu\&quot;:16,\&quot;hugepages-1Gi\&quot;:\&quot;8Gi\&quot;,\&quot;memory\&quot;:\&quot;48Gi\&quot;},\&quot;requests\&quot;:{\&quot;cpu\&quot;:16,\&quot;hugepages-1Gi\&quot;:\&quot;8Gi\&quot;,\&quot;memory\&quot;:\&quot;48Gi\&quot;}},\&quot;securityContext\&quot;:{\&quot;capabilities\&quot;:{\&quot;add\&quot;:[\&quot;CAP_SYS_ADMIN\&quot;]},\&quot;privileged\&quot;:true},\&quot;stdin\&quot;:true,\&quot;tty\&quot;:true,\&quot;volumeMounts\&quot;:[{\&quot;mountPath\&quot;:\&quot;/hugepages\&quot;,\&quot;name\&quot;:\&quot;hugepage\&quot;},{\&quot;mountPath\&quot;:\&quot;/lib/modules\&quot;,\&quot;name\&quot;:\&quot;lib-modules\&quot;},{\&quot;mountPath\&quot;:\&quot;/usr/src\&quot;,\&quot;name\&quot;:\&quot;src\&quot;},{\&quot;mountPath\&quot;:\&quot;/dev\&quot;,\&quot;name\&quot;:\&quot;dev\&quot;},{\&quot;mountPath\&quot;:\&quot;/dev/shm\&quot;,\&quot;name\&quot;:\&quot;cache-volume\&quot;}]}],\&quot;nodeSelector\&quot;:{\&quot;node-role.kubernetes.io/worker-rt\&quot;:\&quot;\&quot;},\&quot;volumes\&quot;:[{\&quot;emptyDir\&quot;:{\&quot;medium\&quot;:\&quot;HugePages\&quot;},\&quot;name\&quot;:\&quot;hugepage\&quot;},{\&quot;hostPath\&quot;:{\&quot;path\&quot;:\&quot;/lib/modules\&quot;},\&quot;name\&quot;:\&quot;lib-modules\&quot;},{\&quot;hostPath\&quot;:{\&quot;path\&quot;:\&quot;/usr/src\&quot;},\&quot;name\&quot;:\&quot;src\&quot;},{\&quot;hostPath\&quot;:{\&quot;path\&quot;:\&quot;/dev\&quot;},\&quot;name\&quot;:\&quot;dev\&quot;},{\&quot;emptyDir\&quot;:{\&quot;medium\&quot;:\&quot;Memory\&quot;,\&quot;sizeLimit\&quot;:\&quot;16Gi\&quot;},\&quot;name\&quot;:\&quot;cache-volume\&quot;}]}}}}\n&quot;
    },
    &quot;labels&quot;: {
      &quot;app&quot;: &quot;du-deployment1&quot;
    },
    &quot;name&quot;: &quot;du-deployment1&quot;,
    &quot;namespace&quot;: &quot;default&quot;
  },
  &quot;spec&quot;: {
    &quot;progressDeadlineSeconds&quot;: 600,
    &quot;replicas&quot;: 1,
    &quot;revisionHistoryLimit&quot;: 10,
    &quot;selector&quot;: {
      &quot;matchLabels&quot;: {
        &quot;app&quot;: &quot;du-pod1&quot;
      }
    },
    &quot;strategy&quot;: {
      &quot;rollingUpdate&quot;: {
        &quot;maxSurge&quot;: &quot;25%&quot;,
        &quot;maxUnavailable&quot;: &quot;25%&quot;
      },
      &quot;type&quot;: &quot;RollingUpdate&quot;
    },
    &quot;template&quot;: {
      &quot;metadata&quot;: {
        &quot;annotations&quot;: {
          &quot;k8s.v1.cni.cncf.io/networks&quot;: &quot;[ { \&quot;name\&quot;: \&quot;host-device-du\&quot;, \&quot;interface\&quot;: \&quot;veth11\&quot; } ]&quot;
        },
        &quot;creationTimestamp&quot;: null,
        &quot;labels&quot;: {
          &quot;app&quot;: &quot;du-pod1&quot;
        }
      },
      &quot;spec&quot;: {
        &quot;containers&quot;: [
          {
            &quot;command&quot;: [
              &quot;sleep&quot;,
              &quot;infinity&quot;
            ],
            &quot;env&quot;: [
              {
                &quot;name&quot;: &quot;duNetProviderDriver&quot;,
                &quot;value&quot;: &quot;host-netdevice&quot;
              }
            ],
            &quot;image&quot;: &quot;registry.ocp4.redhat.ren:5443/ocp4/du:v1-wzh&quot;,
            &quot;imagePullPolicy&quot;: &quot;IfNotPresent&quot;,
            &quot;name&quot;: &quot;du-container1&quot;,
            &quot;resources&quot;: {
              &quot;limits&quot;: {
                &quot;cpu&quot;: &quot;16&quot;,
                &quot;hugepages-1Gi&quot;: &quot;8Gi&quot;,
                &quot;memory&quot;: &quot;48Gi&quot;
              },
              &quot;requests&quot;: {
                &quot;cpu&quot;: &quot;16&quot;,
                &quot;hugepages-1Gi&quot;: &quot;8Gi&quot;,
                &quot;memory&quot;: &quot;48Gi&quot;
              }
            },
            &quot;securityContext&quot;: {
              &quot;capabilities&quot;: {
                &quot;add&quot;: [
                  &quot;CAP_SYS_ADMIN&quot;
                ]
              },
              &quot;privileged&quot;: true
            },
            &quot;stdin&quot;: true,
            &quot;terminationMessagePath&quot;: &quot;/dev/termination-log&quot;,
            &quot;terminationMessagePolicy&quot;: &quot;File&quot;,
            &quot;tty&quot;: true,
            &quot;volumeMounts&quot;: [
              {
                &quot;mountPath&quot;: &quot;/hugepages&quot;,
                &quot;name&quot;: &quot;hugepage&quot;
              },
              {
                &quot;mountPath&quot;: &quot;/lib/modules&quot;,
                &quot;name&quot;: &quot;lib-modules&quot;
              },
              {
                &quot;mountPath&quot;: &quot;/usr/src&quot;,
                &quot;name&quot;: &quot;src&quot;
              },
              {
                &quot;mountPath&quot;: &quot;/dev&quot;,
                &quot;name&quot;: &quot;dev&quot;
              },
              {
                &quot;mountPath&quot;: &quot;/dev/shm&quot;,
                &quot;name&quot;: &quot;cache-volume&quot;
              }
            ]
          }
        ],
        &quot;dnsPolicy&quot;: &quot;ClusterFirst&quot;,
        &quot;nodeSelector&quot;: {
          &quot;node-role.kubernetes.io/worker-rt&quot;: &quot;&quot;
        },
        &quot;restartPolicy&quot;: &quot;Always&quot;,
        &quot;schedulerName&quot;: &quot;default-scheduler&quot;,
        &quot;securityContext&quot;: {},
        &quot;terminationGracePeriodSeconds&quot;: 30,
        &quot;volumes&quot;: [
          {
            &quot;emptyDir&quot;: {
              &quot;medium&quot;: &quot;HugePages&quot;
            },
            &quot;name&quot;: &quot;hugepage&quot;
          },
          {
            &quot;hostPath&quot;: {
              &quot;path&quot;: &quot;/lib/modules&quot;,
              &quot;type&quot;: &quot;&quot;
            },
            &quot;name&quot;: &quot;lib-modules&quot;
          },
          {
            &quot;hostPath&quot;: {
              &quot;path&quot;: &quot;/usr/src&quot;,
              &quot;type&quot;: &quot;&quot;
            },
            &quot;name&quot;: &quot;src&quot;
          },
          {
            &quot;hostPath&quot;: {
              &quot;path&quot;: &quot;/dev&quot;,
              &quot;type&quot;: &quot;&quot;
            },
            &quot;name&quot;: &quot;dev&quot;
          },
          {
            &quot;emptyDir&quot;: {
              &quot;medium&quot;: &quot;Memory&quot;,
              &quot;sizeLimit&quot;: &quot;16Gi&quot;
            },
            &quot;name&quot;: &quot;cache-volume&quot;
          }
        ]
      }
    }
  },
  &quot;status&quot;: {
    &quot;availableReplicas&quot;: 1,
    &quot;conditions&quot;: [
      {
        &quot;lastTransitionTime&quot;: &quot;2021-07-21T06:21:57Z&quot;,
        &quot;lastUpdateTime&quot;: &quot;2021-07-21T06:23:05Z&quot;,
        &quot;message&quot;: &quot;ReplicaSet \&quot;du-deployment1-d5dc9854d\&quot; has successfully progressed.&quot;,
        &quot;reason&quot;: &quot;NewReplicaSetAvailable&quot;,
        &quot;status&quot;: &quot;True&quot;,
        &quot;type&quot;: &quot;Progressing&quot;
      },
      {
        &quot;lastTransitionTime&quot;: &quot;2021-07-21T11:07:55Z&quot;,
        &quot;lastUpdateTime&quot;: &quot;2021-07-21T11:07:55Z&quot;,
        &quot;message&quot;: &quot;Deployment has minimum availability.&quot;,
        &quot;reason&quot;: &quot;MinimumReplicasAvailable&quot;,
        &quot;status&quot;: &quot;True&quot;,
        &quot;type&quot;: &quot;Available&quot;
      }
    ],
    &quot;observedGeneration&quot;: 7,
    &quot;readyReplicas&quot;: 1,
    &quot;replicas&quot;: 1,
    &quot;updatedReplicas&quot;: 1
  }
}

</code></pre>
<pre><code class="language-bash">oc get net-attach-def
# NAME             AGE
# host-device-du   6h32m
# macvlan-conf     23d

oc get net-attach-def/host-device-du -o json | jq &quot;del(.metadata.managedFields, .metadata.uid, .metadata.selfLink, .metadata.resourceVersion, .metadata.generation, .metadata.creationTimestamp)&quot;
</code></pre>
<pre><code class="language-json">{
  &quot;apiVersion&quot;: &quot;k8s.cni.cncf.io/v1&quot;,
  &quot;kind&quot;: &quot;NetworkAttachmentDefinition&quot;,
  &quot;metadata&quot;: {
    &quot;annotations&quot;: {
      &quot;kubectl.kubernetes.io/last-applied-configuration&quot;: &quot;{\&quot;apiVersion\&quot;:\&quot;k8s.cni.cncf.io/v1\&quot;,\&quot;kind\&quot;:\&quot;NetworkAttachmentDefinition\&quot;,\&quot;metadata\&quot;:{\&quot;annotations\&quot;:{},\&quot;name\&quot;:\&quot;host-device-du\&quot;,\&quot;namespace\&quot;:\&quot;default\&quot;},\&quot;spec\&quot;:{\&quot;config\&quot;:\&quot;{ \\\&quot;cniVersion\\\&quot;: \\\&quot;0.3.0\\\&quot;, \\\&quot;type\\\&quot;: \\\&quot;host-device\\\&quot;, \\\&quot;device\\\&quot;: \\\&quot;ens81f1np1\\\&quot;, \\\&quot;ipam\\\&quot;: { \\\&quot;type\\\&quot;: \\\&quot;host-local\\\&quot;, \\\&quot;subnet\\\&quot;: \\\&quot;192.168.12.0/24\\\&quot;, \\\&quot;rangeStart\\\&quot;: \\\&quot;192.168.12.105\\\&quot;, \\\&quot;rangeEnd\\\&quot;: \\\&quot;192.168.12.105\\\&quot;, \\\&quot;routes\\\&quot;: [{ \\\&quot;dst\\\&quot;: \\\&quot;0.0.0.0/0\\\&quot; }], \\\&quot;gateway\\\&quot;: \\\&quot;192.168.12.1\\\&quot; } }\&quot;}}\n&quot;
    },
    &quot;name&quot;: &quot;host-device-du&quot;,
    &quot;namespace&quot;: &quot;default&quot;
  },
  &quot;spec&quot;: {
    &quot;config&quot;: &quot;{ \&quot;cniVersion\&quot;: \&quot;0.3.0\&quot;, \&quot;type\&quot;: \&quot;host-device\&quot;, \&quot;device\&quot;: \&quot;ens18f1\&quot;, \&quot;ipam\&quot;: { \&quot;type\&quot;: \&quot;host-local\&quot;, \&quot;subnet\&quot;: \&quot;192.168.12.0/24\&quot;, \&quot;rangeStart\&quot;: \&quot;192.168.12.105\&quot;, \&quot;rangeEnd\&quot;: \&quot;192.168.12.105\&quot;, \&quot;routes\&quot;: [{ \&quot;dst\&quot;: \&quot;0.0.0.0/0\&quot; }], \&quot;gateway\&quot;: \&quot;192.168.12.1\&quot; } }&quot;
  }
}
</code></pre>
<pre><code class="language-bash">oc get net-attach-def/host-device-du -o json | jq &quot;del(.metadata.managedFields, .metadata.uid, .metadata.selfLink, .metadata.resourceVersion, .metadata.generation, .metadata.creationTimestamp)&quot; | jq .spec.config | jq &quot;fromjson&quot;
</code></pre>
<pre><code class="language-json">{
  &quot;cniVersion&quot;: &quot;0.3.0&quot;,
  &quot;type&quot;: &quot;host-device&quot;,
  &quot;device&quot;: &quot;ens18f1&quot;,
  &quot;ipam&quot;: {
    &quot;type&quot;: &quot;host-local&quot;,
    &quot;subnet&quot;: &quot;192.168.12.0/24&quot;,
    &quot;rangeStart&quot;: &quot;192.168.12.105&quot;,
    &quot;rangeEnd&quot;: &quot;192.168.12.105&quot;,
    &quot;routes&quot;: [
      {
        &quot;dst&quot;: &quot;0.0.0.0/0&quot;
      }
    ],
    &quot;gateway&quot;: &quot;192.168.12.1&quot;
  }
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="从容器向宿主机注入内核模块-kmod--driver"><a class="header" href="#从容器向宿主机注入内核模块-kmod--driver">从容器向宿主机注入内核模块 kmod / driver</a></h1>
<p>从容器向宿主机注入kmod/driver，最大的场景，就是在容器平台上给GPU和DPU装驱动，参考nvidia家的gpu驱动(nvidia gpu operator)，都是从容器向宿主机注入的方式做的。</p>
<p>还有一个大的使用场景，就是像RHACS/StackRox这种安全平台，向宿主机注入内核模块，进行系统监控。</p>
<p>视频讲解:</p>
<p><a href="https://www.bilibili.com/video/BV1gh411v7Zv/"><kbd><img src="ocp4/4.7/imgs/2021-05-11-11-43-25.png" width="600"></kbd></a></p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1gh411v7Zv/">bilibili</a></li>
<li><a href="https://www.ixigua.com/6960872682124640798">xigua</a></li>
<li><a href="https://youtu.be/BX1N8GVDw2Q">youtube</a></li>
</ul>
<h2 id="先用podman进行单机版本测试"><a class="header" href="#先用podman进行单机版本测试">先用podman进行单机版本测试</a></h2>
<pre><code class="language-bash"># on a centos8 to test the driver build
# https://blog.sourcerer.io/writing-a-simple-linux-kernel-module-d9dc3762c234

yum install -y epel-release
yum update -y
yum install -y byobu podman buildah

mkdir -p /data/kmod
cd /data/kmod

podman run -it --rm quay.io/generic/centos8 bash

# below will input/run in the container
dnf update -y

dnf install -y make gcc wget perl createrepo kernel-core-$(uname -r) kernel-devel-$(uname -r) pciutils python36-devel ethtool lsof elfutils-libelf-devel rpm-build kernel-rpm-macros python36 tk numactl-libs libmnl tcl binutils kmod procps git autoconf automake libtool hostname

mkdir -p ~/src/lkm_example
cd ~/src/lkm_example

cat &lt;&lt; 'EOF' &gt; lkm_example.c
#include &lt;linux/init.h&gt;
#include &lt;linux/module.h&gt;
#include &lt;linux/kernel.h&gt;
MODULE_LICENSE(&quot;GPL&quot;);
MODULE_AUTHOR(&quot;Wandering Star&quot;);
MODULE_DESCRIPTION(&quot;A simple example Linux module.&quot;);
MODULE_VERSION(&quot;0.01&quot;);
static int __init lkm_example_init(void) {
 printk(KERN_INFO &quot;Hello, World, Wandering Star!\n&quot;);
 return 0;
}
static void __exit lkm_example_exit(void) {
 printk(KERN_INFO &quot;Goodbye, World, Wandering Star!\n&quot;);
}
module_init(lkm_example_init);
module_exit(lkm_example_exit);

EOF

cat &lt;&lt; EOF &gt; Makefile
obj-m += lkm_example.o
all:
    make -C /lib/modules/$(uname -r)/build M=$(pwd) modules
clean:
    make -C/lib/modules/$(uname -r)/build M=$(pwd) clean
EOF
sed -i 's/^    /\t/g' Makefile

make
insmod lkm_example.ko
# insmod: ERROR: could not insert module lkm_example.ko: Operation not permitted

# poc again with priviledged
podman run -it --rm --privileged quay.io/generic/centos8 bash

# do the same above again
# yum install .............. 
# ........
# make

insmod lkm_example.ko

# go to host
dmesg | grep Wandering
# [ 5197.673179] Hello, World, Wandering Star!

lsmod | grep example
# lkm_example            16384  0

</code></pre>
<h2 id="try-the-demo-on-openshift4"><a class="header" href="#try-the-demo-on-openshift4">try the demo on openshift4</a></h2>
<p>first, we try to get rpm repo offline</p>
<ul>
<li>https://www.openshift.com/blog/how-to-use-entitled-image-builds-to-build-drivercontainers-with-ubi-on-openshift</li>
</ul>
<pre><code class="language-bash"># on a vultr host, centos7
mkdir -p /data/rhel8/entitle
cd /data/rhel8/entitle

# goto https://access.redhat.com/management/subscriptions
# search employee sku, find a system, go into, and download from subscription
# or goto: https://access.redhat.com/management/systems/4d1e4cc0-2c99-4431-99ce-2f589a24ea11/subscriptions
yum install -y unzip 
unzip *
unzip consumer_export.zip
find . -name *.pem -exec cp {} ./ \;

# podman run -ti --mount type=bind,source=/data/rhel8/entitle/$(ls *.pem | sed -n '2p'),target=/etc/pki/entitlement/entitlement.pem  --mount type=bind,source=/data/rhel8/entitle/$(ls *.pem | sed -n '2p'),target=/etc/pki/entitlement/entitlement-key.pem registry.access.redhat.com/ubi8:latest bash -c &quot;dnf search kernel-devel --showduplicates&quot;

mkdir -p /data/rhel8/dnf

podman run -it --rm -v /data/rhel8/dnf:/data/dnf:z \
    --mount type=bind,source=$(ls /data/rhel8/entitle/*.pem | sed -n '2p'),target=/etc/pki/entitlement/entitlement.pem  \
    --mount type=bind,source=$(ls /data/rhel8/entitle/*.pem | sed -n '2p'),target=/etc/pki/entitlement/entitlement-key.pem \
    registry.access.redhat.com/ubi8:8.3 bash

cd /data/dnf
# dnf -y --enablerepo=rhel-8-for-x86_64-baseos-rpms --releasever=8.3 install make gcc wget perl createrepo  pciutils python36-devel ethtool lsof elfutils-libelf-devel rpm-build kernel-rpm-macros python36 tk numactl-libs libmnl tcl binutils kmod procps git autoconf automake libtool hostname kernel-core-$(uname -r) kernel-devel-$(uname -r)

dnf -y --enablerepo=rhel-8-for-x86_64-baseos-rpms --releasever=8.3 install createrepo  

dnf -y download --resolve --alldeps --releasever=8.3 \
make gcc wget perl createrepo  pciutils python36-devel ethtool lsof elfutils-libelf-devel rpm-build kernel-rpm-macros python36 tk numactl-libs libmnl tcl binutils kmod procps git autoconf automake libtool hostname kernel-core-4.18.0-240.22.1.el8_3.x86_64 kernel-devel-4.18.0-240.22.1.el8_3.x86_64

dnf -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm
# dnf install -y https://kojipkgs.fedoraproject.org//packages/modulemd-tools/0.9/1.fc32/noarch/modulemd-tools-0.9-1.fc32.noarch.rpm
# https://copr.fedorainfracloud.org/coprs/frostyx/modulemd-tools/
dnf copr enable -y frostyx/modulemd-tools
dnf install -y modulemd-tools

createrepo ./
repo2module . \
    --module-name foo \
    --module-stream devel \
    --module-version 123 \
    --module-context f32
createrepo_mod .

# back to host
cd /data/rhel8
tar zcvf dnf.tgz dnf/

# upload dnf.tgz to helper /var/www/html/
# on helper
cd /var/www/html/
tar zvxf dnf.tgz


</code></pre>
<p>we will use an entrypoint file. the entrypoint script file is locate <a href="ocp4/4.7/./files/kmod.entrypoint.sh">here</a></p>
<pre><code class="language-bash"># on helper
mkdir -p /data/kmod
cd /data/kmod

cat &lt;&lt; EOF &gt; /data/kmod/Dockerfile
FROM registry.access.redhat.com/ubi8

WORKDIR /
COPY kmod.entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh

ENTRYPOINT [&quot;/entrypoint.sh&quot;]

EOF

buildah bud -t quay.io/wangzheng422/qimgs:kmod-demo.02 -f Dockerfile .
buildah push quay.io/wangzheng422/qimgs:kmod-demo.02

cd /data/install
cat &lt;&lt; EOF &gt; kmod-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: kmod-example
spec:
  nodeSelector:
    kubernetes.io/hostname: 'master-2'
  restartPolicy: Never
  containers:
  - securityContext:
      privileged: true
    image: quay.io/wangzheng422/qimgs:kmod-demo.02
    imagePullPolicy: Always
    name: kmod-example

EOF
oc create -n demo -f kmod-pod.yaml

# to restore
oc delete -n demo -f kmod-pod.yaml

# login to master-2
ssh core@master-2
lsmod | grep example
# lkm_example            16384  0

dmesg | grep Wandering
# [40933.691925] Hello, World, Wandering Star!
</code></pre>
<p><img src="ocp4/4.7/imgs/2021-05-10-21-08-07.png" alt="" /></p>
<h2 id="rhacsstackrox-使用案例"><a class="header" href="#rhacsstackrox-使用案例">RHACS/Stackrox 使用案例</a></h2>
<p>我们已经完成了内核模块的注入，但是为了更好的实现软件功能，我们一般需要把/sys, /dev这种目录挂载到容器中，以下就是RHACS/StackRox的挂载实例。</p>
<p><img src="ocp4/4.7/imgs/2021-05-10-21-15-21.png" alt="" /></p>
<h2 id="others"><a class="header" href="#others">others</a></h2>
<pre><code class="language-bash">
mkdir /etc/yum.repos.d.bak
mv /etc/yum.repos.d/* /etc/yum.repos.d.bak
cat &lt;&lt; EOF &gt; /etc/yum.repos.d/remote.repo
[remote]
name=RHEL-Mirror
baseurl=http://v.redhat.ren:8080/
enabled=1
gpgcheck=0

EOF


</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift4-上-gpuvgpu-共享"><a class="header" href="#openshift4-上-gpuvgpu-共享">openshift4 上 GPU/vGPU 共享</a></h1>
<p>openshift/k8s集群上，运行了越来越多的AI/ML应用，这些应用大部分需要GPU的支持，但是英伟达/k8s官方的device-plug中，GPU的调度，是按照一块GPU为单元来进行调度的，这就在k8s调度层面，带来一个问题，即GPU资源浪费的问题。</p>
<p>好在社区有很多类似的方案，比如<a href="https://github.com/AliyunContainerService/gpushare-scheduler-extender">aliyun的方案</a>，就相对简单，当然功能也简单。本文就试图在openshift4上，运行aliyun的gpu共享方案。</p>
<p>由于aliyun等类似的方案，大多基于nvidia-docker，而openshift4使用了crio，所以里面有一点定制化的部分。</p>
<p>由于时间所限，本文只是完成了方案的大致成功运行，完美的运行，需要更多的定制化，这个就有待项目中继续完善吧。</p>
<p>注意</p>
<ul>
<li>这是调度共享方案，不是共享隔离方案</li>
</ul>
<p>todo</p>
<ul>
<li>在真实的多GPU卡环境中验证。</li>
<li>增强scheduler extender安全性</li>
</ul>
<p>视频讲解</p>
<p><a href="https://www.bilibili.com/video/BV1pp4y1H7FS/"><kbd><img src="ocp4/4.6/imgs/2021-02-28-19-57-39.png" width="600"></kbd></a></p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1pp4y1H7FS/">bilibili</a></li>
<li><a href="https://www.ixigua.com/6934286379568333325">xigua</a></li>
<li><a href="https://youtu.be/ZTWpzrVtJ10">youtube</a></li>
</ul>
<h2 id="部署运行-scheduler-extender"><a class="header" href="#部署运行-scheduler-extender">部署运行 scheduler extender</a></h2>
<p>aliyun类似的方案，都是扩展k8s scheduler的功能，来增强k8s已有的功能，在最新版本的openshift4中，已经可以通过配置，把这个scheduler扩展功能激活。</p>
<pre><code class="language-bash">cd /data/install
cat &lt;&lt; EOF &gt; ./policy.cfg
    {
    &quot;kind&quot; : &quot;Policy&quot;,
    &quot;apiVersion&quot; : &quot;v1&quot;,
    &quot;predicates&quot; : [
            {&quot;name&quot; : &quot;MaxGCEPDVolumeCount&quot;},
            {&quot;name&quot; : &quot;GeneralPredicates&quot;},
            {&quot;name&quot; : &quot;MaxAzureDiskVolumeCount&quot;},
            {&quot;name&quot; : &quot;MaxCSIVolumeCountPred&quot;},
            {&quot;name&quot; : &quot;CheckVolumeBinding&quot;},
            {&quot;name&quot; : &quot;MaxEBSVolumeCount&quot;},
            {&quot;name&quot; : &quot;MatchInterPodAffinity&quot;},
            {&quot;name&quot; : &quot;CheckNodeUnschedulable&quot;},
            {&quot;name&quot; : &quot;NoDiskConflict&quot;},
            {&quot;name&quot; : &quot;NoVolumeZoneConflict&quot;},
            {&quot;name&quot; : &quot;PodToleratesNodeTaints&quot;}
            ],
    &quot;priorities&quot; : [
            {&quot;name&quot; : &quot;LeastRequestedPriority&quot;, &quot;weight&quot; : 1},
            {&quot;name&quot; : &quot;BalancedResourceAllocation&quot;, &quot;weight&quot; : 1},
            {&quot;name&quot; : &quot;ServiceSpreadingPriority&quot;, &quot;weight&quot; : 1},
            {&quot;name&quot; : &quot;NodePreferAvoidPodsPriority&quot;, &quot;weight&quot; : 1},
            {&quot;name&quot; : &quot;NodeAffinityPriority&quot;, &quot;weight&quot; : 1},
            {&quot;name&quot; : &quot;TaintTolerationPriority&quot;, &quot;weight&quot; : 1},
            {&quot;name&quot; : &quot;ImageLocalityPriority&quot;, &quot;weight&quot; : 1},
            {&quot;name&quot; : &quot;SelectorSpreadPriority&quot;, &quot;weight&quot; : 1},
            {&quot;name&quot; : &quot;InterPodAffinityPriority&quot;, &quot;weight&quot; : 1},
            {&quot;name&quot; : &quot;EqualPriority&quot;, &quot;weight&quot; : 1}
            ],
    &quot;extenders&quot;: [
            {
              &quot;urlPrefix&quot;: &quot;http://127.0.0.1:32766/gpushare-scheduler&quot;,
              &quot;filterVerb&quot;: &quot;filter&quot;,
              &quot;bindVerb&quot;:   &quot;bind&quot;,
              &quot;enableHttps&quot;: false,
              &quot;nodeCacheCapable&quot;: true,
              &quot;managedResources&quot;: [
                {
                  &quot;name&quot;: &quot;aliyun.com/gpu-mem&quot;,
                  &quot;ignoredByScheduler&quot;: false
                }
              ],
              &quot;ignorable&quot;: false
            }
          ]
    }
   
EOF
oc delete configmap -n openshift-config  scheduler-policy
oc create configmap -n openshift-config --from-file=policy.cfg scheduler-policy

oc patch Scheduler cluster --type='merge' -p '{&quot;spec&quot;:{&quot;policy&quot;:{&quot;name&quot;:&quot;scheduler-policy&quot;}}}' --type=merge

</code></pre>
<p>然后我们就可以部署 scheduler extender 了</p>
<pre><code class="language-bash">curl -O https://raw.githubusercontent.com/AliyunContainerService/gpushare-scheduler-extender/master/config/gpushare-schd-extender.yaml
# replace docker image
cd /data/install
sed -i 's/image:.*/image: quay.io\/wangzheng422\/qimgs:gpushare-scheduler-extender-2021-02-26-1339/' gpushare-schd-extender.yaml
oc delete -f gpushare-schd-extender.yaml
oc create -f gpushare-schd-extender.yaml

</code></pre>
<h2 id="operator-hub-中添加-catalog-source"><a class="header" href="#operator-hub-中添加-catalog-source">operator hub 中添加 catalog source</a></h2>
<p>我们定制了nvidia gpu-operator，所以我们要把我们新的operator加到operator hub中去。</p>
<pre><code class="language-bash">#
cat &lt;&lt; EOF &gt; /data/ocp4/my-catalog.yaml
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: wzh-operator-catalog
  namespace: openshift-marketplace
spec:
  displayName: WZH Operator Catalog
  image: 'quay.io/wangzheng422/qimgs:registry-wzh-index.2021-02-28-1446'
  publisher: WZH
  sourceType: grpc
EOF
oc create -f  /data/ocp4/my-catalog.yaml

oc delete -f /data/ocp4/my-catalog.yaml

</code></pre>
<p>到此，我们就能在 operator hub 中，查找到2个gpu-operator了
<img src="ocp4/4.6/imgs/2021-02-28-15-02-17.png" alt="" /></p>
<h2 id="安装-gpu-operator-并配置-clusterpolicies"><a class="header" href="#安装-gpu-operator-并配置-clusterpolicies">安装 gpu-operator 并配置 ClusterPolicies</a></h2>
<p>点击安装 nvidia &amp; wzh 那个。</p>
<p>安装成功以后，创建 project gpu-operator-resources</p>
<p>然后在 project gpu-operator-resources 中，给gpu-operator创建一个ClusterPolicies 配置，使用以下模版创建。不过里面涉及到准备一个离线安装源的操作，<a href="ocp4/4.6/./4.6.nvidia.gpu.disconnected.html">参考这里</a>完成。</p>
<pre><code class="language-yaml">
apiVersion: nvidia.com/v1
kind: ClusterPolicy
metadata:
  name: gpu-cluster-policy
spec:
  dcgmExporter:
    nodeSelector: {}
    imagePullSecrets: []
    resources: {}
    affinity: {}
    podSecurityContext: {}
    repository: nvcr.io/nvidia/k8s
    securityContext: {}
    version: 'sha256:85016e39f73749ef9769a083ceb849cae80c31c5a7f22485b3ba4aa590ec7b88'
    image: dcgm-exporter
    tolerations: []
  devicePlugin:
    nodeSelector: {}
    imagePullSecrets: []
    resources: {}
    affinity: {}
    podSecurityContext: {}
    repository: quay.io/wangzheng422
    securityContext: {}
    version: gpu-aliyun-device-plugin-2021-02-24-1346
    image: qimgs
    tolerations: []
    args:
      - 'gpushare-device-plugin-v2'
      - '-logtostderr'
      - '--v=5'
    env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            fieldPath: spec.nodeName
  driver:
    nodeSelector: {}
    imagePullSecrets: []
    resources: {}
    affinity: {}
    podSecurityContext: {}
    repository: nvcr.io/nvidia
    securityContext: {}
    repoConfig:
      configMapName: repo-config
      destinationDir: /etc/yum.repos.d
    version: 'sha256:324e9dc265dec320207206aa94226b0c8735fd93ce19b36a415478c95826d934'
    image: driver
    tolerations: []
  gfd:
    nodeSelector: {}
    imagePullSecrets: []
    resources: {}
    affinity: {}
    podSecurityContext: {}
    repository: nvcr.io/nvidia
    securityContext: {}
    version: 'sha256:8d068b7b2e3c0b00061bbff07f4207bd49be7d5bfbff51fdf247bc91e3f27a14'
    image: gpu-feature-discovery
    tolerations: []
    migStrategy: single
    sleepInterval: 60s
  operator:
    defaultRuntime: crio
    validator:
      image: cuda-sample
      imagePullSecrets: []
      repository: nvcr.io/nvidia/k8s
      version: 'sha256:2a30fe7e23067bc2c3f8f62a6867702a016af2b80b9f6ce861f3fea4dfd85bc2'
    deployGFD: true
  toolkit:
    nodeSelector: {}
    imagePullSecrets: []
    resources: {}
    affinity: {}
    podSecurityContext: {}
    repository: nvcr.io/nvidia/k8s
    securityContext: {}
    version: 'sha256:81295a9eca36cbe5d94b80732210b8dc7276c6ef08d5a60d12e50479b9e542cd'
    image: container-toolkit
    tolerations: []

</code></pre>
<p>至此，gpu-operator就安装完成了，我们可以看到，device-plugin的validate并没有运行，这是因为，我们定制了sheduler， nvidia.com/gpu 已经被 aliyun.com/gpu-mem 代替。 完美解决这个问题，就需要继续定制化了，但是系统已经能按照预期运行，我们就把定制化留到以后项目中去做好了。</p>
<p><img src="ocp4/4.6/imgs/2021-02-28-16-31-29.png" alt="" /></p>
<h2 id="测试一下-1"><a class="header" href="#测试一下-1">测试一下</a></h2>
<p>我们就来实际测试一下效果</p>
<pre><code class="language-bash">cat &lt;&lt; EOF &gt; /data/ocp4/gpu.test.yaml
---
kind: Deployment
apiVersion: apps/v1
metadata:
  annotations:
  name: demo1
  labels:
    app: demo1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo1
  template:
    metadata:
      labels:
        app: demo1
    spec:
      # nodeSelector:
      #   kubernetes.io/hostname: 'worker-0'
      restartPolicy: Always
      containers:
        - name: demo1
          image: &quot;docker.io/wangzheng422/imgs:tensorrt-ljj-2021-01-21-1151&quot;
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              valueFrom:
                fieldRef:
                  fieldPath: metadata.annotations['ALIYUN_COM_GPU_MEM_IDX']
          resources:
            limits:
              # GiB
              aliyun.com/gpu-mem: 3

EOF
oc create -n demo -f /data/ocp4/gpu.test.yaml


</code></pre>
<p>进入测试容器，看环境变量，我们就能看到 NVIDIA_VISIBLE_DEVICES 被自动设置了
<img src="ocp4/4.6/imgs/2021-02-28-16-24-58.png" alt="" /></p>
<p>我们进入scheduler extender看看日志， 可以看到scheduler试图给pod添加annotation
<img src="ocp4/4.6/imgs/2021-02-28-16-25-55.png" alt="" /></p>
<p>我们再进入device-plugin看看日志，可以看到device-plugin在对比内存，挑选gpu设备。
<img src="ocp4/4.6/imgs/2021-02-28-16-27-04.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="headless-service-with-router"><a class="header" href="#headless-service-with-router">headless service with router</a></h1>
<p>本文讲述，如果service是headless的情况下，k8s/ocp ingress如何处理，和普通headless有什么区别。</p>
<p>演示视频</p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1Q54y1S7Kg/">bilibili</a></li>
<li><a href="https://www.ixigua.com/6857041180861923843/">xigua</a></li>
<li><a href="https://youtu.be/u6UYc7Hdtqc">youtube</a></li>
</ul>
<p>结论是，不管是是不是headless service, openshift都会找到最终的pod ip，然后在ingress/router/haproxy里面，修改配置，让流量直接导向pod ip。</p>
<pre><code class="language-bash"># 这里是演示环境部署脚本
cat &lt;&lt; EOF &gt; headless.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: slb-001
spec:
  replicas: 1
  selector: 
    matchLabels: 
      pod: slb-001
  template: 
    metadata: 
      labels: 
        pod: slb-001
    spec:
      restartPolicy: Always
      
      containers:
      - name: slb-001-pg
        image: registry.redhat.ren:5443/docker.io/etherpad/etherpad:latest
        imagePullPolicy: IfNotPresent

---
apiVersion: v1
kind: Service
metadata:
  name: slb-001-service
spec:
  selector:
    pod: slb-001
  ports:
    - port: 9001
      protocol: TCP
      targetPort: 9001
---
apiVersion: v1
kind: Service
metadata:
  name: slb-002-service
spec:
  selector:
    pod: slb-001
  clusterIP: None
  ports:
    - port: 9001
      protocol: TCP
      targetPort: 9001

---
kind: Route
apiVersion: route.openshift.io/v1
metadata:
  name: slb-001
spec:
  to:
    kind: Service
    name: slb-001-service
  port:
    targetPort: 9001
---
kind: Route
apiVersion: route.openshift.io/v1
metadata:
  name: slb-002
spec:
  to:
    kind: Service
    name: slb-002-service
  port:
    targetPort: 9001
EOF
oc apply -n demo -f headless.yaml


</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="volumn-测试"><a class="header" href="#volumn-测试">volumn 测试</a></h1>
<p>this is for single node cluster:</p>
<p>https://docs.openshift.com/container-platform/4.3/storage/persistent_storage/persistent-storage-hostpath.html</p>
<h2 id="local-volumn"><a class="header" href="#local-volumn">local volumn</a></h2>
<p>https://docs.openshift.com/container-platform/4.3/storage/persistent_storage/persistent-storage-local.html</p>
<p>local volumn有个坑，他是挂载设备，不是节点上面的目录，所以想用的话，先要想办法把节点上面用lvm搞出很多个设备来，然后用local volumn挂载。。。这个太傻了。。。还是需要有商业版本的云原生的存储解决方案比较好。</p>
<pre><code class="language-bash"># on worker-0
mkdir -p /data/demo

# on helper
oc project demo
oc get sa
oc create serviceaccount -n demo demo-app
oc adm policy add-scc-to-user privileged -z demo-app


</code></pre>
<h3 id="local-volumn-block-share"><a class="header" href="#local-volumn-block-share">local volumn block share</a></h3>
<p>如果是local volume 在块设备的模式下，是可以被相同节点的pod共享的</p>
<p>video</p>
<ul>
<li>https://youtu.be/P33sxtR57u8</li>
<li>https://www.ixigua.com/i6841022539582407180/</li>
<li>https://www.bilibili.com/video/BV115411W7FV/</li>
</ul>
<pre><code class="language-bash"># on infra0 create a lv
lvcreate --type raid0 -L 40G --stripes 12 -n sharelv datavg

apiVersion: &quot;local.storage.openshift.io/v1&quot;
kind: &quot;LocalVolume&quot;
metadata:
  name: &quot;local-share-block-disks&quot;
  namespace: &quot;local-storage&quot; 
spec:
  nodeSelector: 
    nodeSelectorTerms:
    - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - infra0.hsc.redhat.ren
  storageClassDevices:
    - storageClassName: &quot;local-share-block-sc&quot;
      volumeMode: Block 
      devicePaths: 
        - /dev/datavg/sharelv

cat &lt;&lt; EOF &gt; storage.yaml
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: localpvc
spec:
  accessModes:
  - ReadWriteOnce
  volumeMode: Block 
  resources:
    requests:
      storage: 40Gi 
  storageClassName: local-share-block-sc
EOF
oc apply -n demo -f storage.yaml

cat &lt;&lt; EOF &gt; demo1.yaml
---
kind: Pod
apiVersion: v1
metadata:
  annotations:
  name: demo1
  namespace: demo
spec:
  nodeSelector:
    kubernetes.io/hostname: 'infra0.hsc.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo1
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [ &quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot; ]
      args: [ &quot;while true; do sleep 300000; done;&quot; ]
      imagePullPolicy: Always
      securityContext:
        privileged: true
      volumeDevices:
        - devicePath: /mnt/block
          name: demo
  serviceAccount: demo-app
  volumes:
    - name: demo 
      persistentVolumeClaim:
        claimName: localpvc 
---
kind: Pod
apiVersion: v1
metadata:
  annotations:
  name: demo2
  namespace: demo
spec:
  nodeSelector:
    kubernetes.io/hostname: 'infra0.hsc.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo1
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [ &quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot; ]
      args: [ &quot;while true; do sleep 300000; done;&quot; ]
      imagePullPolicy: Always
      securityContext:
        privileged: true
      volumeDevices:
        - devicePath: /mnt/block
          name: demo
  serviceAccount: demo-app
  volumes:
    - name: demo 
      persistentVolumeClaim:
        claimName: localpvc 
EOF
oc apply -f demo1.yaml

# 向块设备写入
oc exec -it -n demo demo1 -- bash -c &quot;echo 'test 1' &gt; /mnt/block&quot;
oc exec -it -n demo demo1 -- head -n 1 /mnt/block

oc exec -it -n demo demo2 -- head -n 2 /mnt/block

oc delete -f demo1.yaml

</code></pre>
<h3 id="local-volume-fs"><a class="header" href="#local-volume-fs">local volume fs</a></h3>
<pre><code class="language-bash">cat &lt;&lt; EOF &gt; demo1.yaml
---
apiVersion: &quot;local.storage.openshift.io/v1&quot;
kind: &quot;LocalVolume&quot;
metadata:
  name: &quot;local-disks&quot;
  namespace: &quot;local-storage&quot; 
spec:
  nodeSelector: 
    nodeSelectorTerms:
    - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - worker-0
  storageClassDevices:
    - storageClassName: &quot;local-sc&quot;
      volumeMode: Filesystem 
      fsType: xfs 
      devicePaths: 
        - /data/lv01
        - /data/lv02
EOF

oc apply -f demo1.yaml
oc delete -f demo1.yaml

oc get all -n local-storage
oc get pv

cat &lt;&lt; EOF &gt; demo1.yaml
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: local-pvc-name 
spec:
  accessModes:
  - ReadWriteOnce
  volumeMode: Filesystem 
  resources:
    requests:
      storage: 100Gi 
  storageClassName: local-sc 
---
kind: Pod
apiVersion: v1
metadata:
  annotations:
  name: demo1
  namespace: demo
spec:
  nodeSelector:
    kubernetes.io/hostname: 'worker-0'
  restartPolicy: Always
  containers:
    - name: demo1
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [ &quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot; ]
      args: [ &quot;while true; do sleep 300000; done;&quot; ]
      imagePullPolicy: Always
      securityContext:
        privileged: true
      volumeMounts:
        - mountPath: /data
          name: demo 
          readOnly: false
  serviceAccount: demo-app
  volumes:
    - name: demo 
      persistentVolumeClaim:
        claimName: localpvc 
EOF
oc apply -f demo1.yaml
</code></pre>
<h2 id="demo-for-hostpath"><a class="header" href="#demo-for-hostpath">demo for hostpath</a></h2>
<p>https://docs.openshift.com/container-platform/4.3/storage/persistent_storage/persistent-storage-local.html</p>
<p>video</p>
<ul>
<li>https://www.bilibili.com/video/BV1MV411Z7ZK/</li>
<li>https://youtu.be/Dzq-xZW3O5E</li>
</ul>
<pre><code class="language-bash">
oc project demo
oc get sa
oc create serviceaccount -n demo demo-app
oc adm policy add-scc-to-user privileged -z demo-app

cat &lt;&lt; EOF &gt; demo1.yaml
---
kind: Deployment
apiVersion: apps/v1
metadata:
  annotations:
  name: demo1
  namespace: demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo1
  template:
    metadata:
      labels:
        app: demo1  
    spec:
      nodeSelector:
        kubernetes.io/hostname: 'infra1.hsc.redhat.ren'
      restartPolicy: Always
      containers:
        - name: demo1
          image: &gt;- 
            registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
          env:
            - name: key
              value: value
          command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot; ]
          args: [ &quot;trap : TERM INT; sleep infinity &amp; wait&quot; ]
          imagePullPolicy: Always
          securityContext:
            privileged: true
          volumeMounts:
            - mountPath: /data
              name: demo 
              readOnly: false
      serviceAccount: demo-app
      volumes:
        - name: demo 
          hostPath:
            path: /data
            type: Directory
EOF
oc apply -f demo1.yaml

oc delete -f demo1.yaml
</code></pre>
<h2 id="demo-for-emptydir"><a class="header" href="#demo-for-emptydir">demo for emptydir</a></h2>
<p>https://kubernetes.io/docs/concepts/storage/volumes/</p>
<pre><code class="language-bash">cat &lt;&lt; EOF &gt; demo1.yaml
---
kind: Deployment
apiVersion: apps/v1
metadata:
  annotations:
  name: demo1
  namespace: demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo1
  template:
    metadata:
      labels:
        app: demo1  
    spec:
      nodeSelector:
        kubernetes.io/hostname: 'infra1.hsc.redhat.ren'
      restartPolicy: Always
      containers:
        - name: demo1
          image: &gt;- 
            registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
          env:
            - name: key
              value: value
          command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot; ]
          args: [ &quot;trap : TERM INT; sleep infinity &amp; wait&quot; ]
          imagePullPolicy: Always
          volumeMounts:
            - mountPath: /data
              name: demo 
              readOnly: false
      volumes:
        - name: demo 
          emptyDir: {}
EOF
oc apply -f demo1.yaml

oc delete -f demo1.yaml

</code></pre>
<h2 id="secret"><a class="header" href="#secret">secret</a></h2>
<p>https://docs.openshift.com/container-platform/4.3/nodes/pods/nodes-pods-secrets.html</p>
<pre><code class="language-bash">cat &lt;&lt; EOF &gt; demo1.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: test-secret
  namespace: demo
data:
  username: dmFsdWUtMQ0K     
  password: dmFsdWUtMQ0KDQo= 
stringData:
  hostname: myapp.mydomain.com 
  secret.properties: |-     
    property1=valueA
    property2=valueB
---
kind: Deployment
apiVersion: apps/v1
metadata:
  annotations:
  name: demo1
  namespace: demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo1
  template:
    metadata:
      labels:
        app: demo1  
    spec:
      nodeSelector:
        kubernetes.io/hostname: 'infra1.hsc.redhat.ren'
      restartPolicy: Always
      containers:
        - name: demo1
          image: &gt;- 
            registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
          env:
            - name: key
              value: value
          command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot; ]
          args: [ &quot;trap : TERM INT; sleep infinity &amp; wait&quot; ]
          imagePullPolicy: Always
          volumeMounts:
            - mountPath: /data
              name: demo 
              readOnly: true
      volumes:
        - name: demo 
          secret:
            secretName: test-secret
EOF
oc apply -f demo1.yaml

oc delete -f demo1.yaml

</code></pre>
<h2 id="configmap"><a class="header" href="#configmap">configmap</a></h2>
<p>https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/</p>
<pre><code class="language-bash">cat &lt;&lt; EOF &gt; demo1.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: special-config
  namespace: demo
data:
  SPECIAL_LEVEL: very
  SPECIAL_TYPE: charm
---
kind: Deployment
apiVersion: apps/v1
metadata:
  annotations:
  name: demo1
  namespace: demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo1
  template:
    metadata:
      labels:
        app: demo1  
    spec:
      nodeSelector:
        kubernetes.io/hostname: 'infra1.hsc.redhat.ren'
      restartPolicy: Always
      containers:
        - name: demo1
          image: &gt;- 
            registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
          env:
            - name: key
              value: value
          command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot; ]
          args: [ &quot;trap : TERM INT; sleep infinity &amp; wait&quot; ]
          imagePullPolicy: Always
          volumeMounts:
            - mountPath: /data
              name: demo 
              readOnly: true
      volumes:
        - name: demo 
          configMap:
            name: special-config
EOF
oc apply -f demo1.yaml

oc delete -f demo1.yaml

</code></pre>
<h2 id="nfs-manual"><a class="header" href="#nfs-manual">nfs manual</a></h2>
<p>https://docs.openshift.com/container-platform/4.3/storage/persistent_storage/persistent-storage-nfs.html</p>
<p>video</p>
<ul>
<li>https://www.bilibili.com/video/BV1Ng4y1z7Dj/</li>
<li>https://youtu.be/DIM9fLGJZLU</li>
</ul>
<pre><code class="language-bash"># on helper
mkdir -p /data/export/lv01
mkdir -p /data/export/lv02

chown -R nfsnobody:nfsnobody /data/export/lv01
chown -R nfsnobody:nfsnobody /data/export/lv02

chmod 777 /data/export/lv01
chmod 777 /data/export/lv02

cat &lt;&lt; EOF &gt; /etc/exports
/data/export    *(rw,sync,root_squash)
/data/export/lv01    *(rw,sync,root_squash)
/data/export/lv02    *(rw,sync,root_squash)

EOF

systemctl restart nfs-server

exportfs  -s

cat &lt;&lt; EOF &gt; demo.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0001 
  labels:
    storage-purpose: demo
spec:
  capacity:
    storage: 5Gi 
  accessModes:
  - ReadWriteOnce 
  nfs: 
    path: /data/export/lv01 
    server: 117.177.241.16
  persistentVolumeReclaimPolicy: Retain 
EOF

oc create -n demo -f demo.yaml

oc get pv

cat &lt;&lt; EOF &gt; demo.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-claim1
spec:
  storageClassName: &quot;&quot;
  accessModes:
    - ReadWriteOnce 
  resources:
    requests:
      storage: 5Gi
  selector: 
    matchLabels:
      storage-purpose: demo
EOF

oc create -n demo -f demo.yaml

cat &lt;&lt; EOF &gt; demo.yaml
---
kind: Deployment
apiVersion: apps/v1
metadata:
  annotations:
  name: demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo
  template:
    metadata:
      labels:
        app: demo  
    spec:
      nodeSelector:
        kubernetes.io/hostname: 'infra1.hsc.redhat.ren'
      restartPolicy: Always
      containers:
        - name: demo1
          image: &gt;- 
            registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
          env:
            - name: key
              value: value
          command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot; ]
          args: [ &quot;trap : TERM INT; sleep infinity &amp; wait&quot; ]
          imagePullPolicy: Always
          volumeMounts:
            - mountPath: /data
              name: demo 
      volumes:
        - name: demo 
          persistentVolumeClaim:
            claimName: nfs-claim1
EOF
oc apply -n demo -f demo.yaml

</code></pre>
<h2 id="nfs-auto"><a class="header" href="#nfs-auto">nfs auto</a></h2>
<p>https://github.com/kubernetes-incubator/external-storage/blob/master/nfs-client/deploy/test-claim.yaml</p>
<p>video</p>
<ul>
<li>https://www.bilibili.com/video/BV1vt4y1272R/</li>
<li>https://youtu.be/aSfiv-G67Gg</li>
</ul>
<pre><code class="language-bash">cat &lt;&lt; EOF &gt; demo.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc-demo
  annotations:
    volume.beta.kubernetes.io/storage-class: nfs-storage-provisioner
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
EOF

oc create -n demo -f demo.yaml

cat &lt;&lt; EOF &gt; demo.yaml
---
kind: Deployment
apiVersion: apps/v1
metadata:
  annotations:
  name: demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo
  template:
    metadata:
      labels:
        app: demo  
    spec:
      nodeSelector:
        kubernetes.io/hostname: 'infra1.hsc.redhat.ren'
      restartPolicy: Always
      containers:
        - name: demo1
          image: &gt;- 
            registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
          env:
            - name: key
              value: value
          command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot; ]
          args: [ &quot;trap : TERM INT; sleep infinity &amp; wait&quot; ]
          imagePullPolicy: Always
          volumeMounts:
            - mountPath: /data
              name: demo 
      volumes:
        - name: demo 
          persistentVolumeClaim:
            claimName: pvc-demo
EOF
oc apply -n demo -f demo.yaml

oc delete -n demo -f demo.yaml

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-43-enable-supportpodpidslimit"><a class="header" href="#openshift-43-enable-supportpodpidslimit">openshift 4.3 enable SupportPodPidsLimit</a></h1>
<p>默认 /sys/fs/cgroup/pids/pids.max 是1024, 有些业务是要求突破这个值。如果不放松限制，会有 &quot;read init-p: connection reset by peer&quot; 这种错误，无法rsh进pod. 而且客户的java程序可能会出现线程创建失败的问题。</p>
<p>解决问题的思路，不要按照文档，开启集群的PodPidsLimit功能，而是用mc放开crio.conf里面的pid限制。</p>
<p>https://www.redhat.com/en/blog/red-hat-openshift-container-platform-4-now-defaults-cri-o-underlying-container-engine</p>
<p>https://docs.openshift.com/container-platform/4.3/nodes/clusters/nodes-cluster-enabling-features.html</p>
<p>https://blog.spider.im/post/pid-limit-in-k8s/</p>
<p>这个pids系统限制的是线程+进程数，可以理解成pstree -pl看到的数量</p>
<p>https://docs.openshift.com/container-platform/4.3/scalability_and_performance/recommended-host-practices.html</p>
<p>https://github.com/openshift/machine-config-operator/blob/master/pkg/apis/machineconfiguration.openshift.io/v1/types.go</p>
<p>https://github.com/openshift/machine-config-operator/blob/master/vendor/k8s.io/kubelet/config/v1beta1/types.go</p>
<p>https://github.com/cri-o/cri-o/issues/1921</p>
<h2 id="正确"><a class="header" href="#正确">正确</a></h2>
<p>直接覆盖 /etc/crio/crio.conf</p>
<pre><code class="language-bash">
# check current pids limit
crictl ps | awk '{print $1}' | xargs -I DEMO crictl exec DEMO cat /sys/fs/cgroup/pids/pids.max

oc label mcp worker custom-kubelet-pod-pids-limit=true

cat &lt;&lt; EOF &gt; crio.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: ContainerRuntimeConfig
metadata:
 name: set-log-and-pid
spec:
 machineConfigPoolSelector:
   matchLabels:
     custom-kubelet-pod-pids-limit: 'true'
 containerRuntimeConfig:
   pidsLimit: 10240
EOF
oc apply -f crio.yaml

oc delete -f crio.yaml

</code></pre>
<h2 id="错误"><a class="header" href="#错误">错误</a></h2>
<pre><code class="language-bash">
# PodPidsLimit
oc label mcp worker custom-kubelet-pod-pids-limit=true

cat &lt;&lt; EOF &gt; PodPidsLimit.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: pod-pids-limit
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet-pod-pids-limit: 'true'
  kubeletConfig:
    PodPidsLimit: 4096
EOF
oc apply -f PodPidsLimit.yaml

oc delete -f PodPidsLimit.yaml

cat &lt;&lt; EOF &gt; PodPidsLimit.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: pod-pids-limit
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet-pod-pids-limit: 'true'
  kubeletConfig:
    PodPidsLimit: 10240
EOF
oc apply -f PodPidsLimit.yaml

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-使用-rh-sso-做-oauth-认证"><a class="header" href="#openshift-使用-rh-sso-做-oauth-认证">openshift 使用 rh sso 做 oauth 认证</a></h1>
<p>https://access.redhat.com/documentation/en-us/red_hat_single_sign-on/7.3/html/red_hat_single_sign-on_for_openshift/index</p>
<p>官方文档写的很好，但是是基于 ocp 3.11 的，所以里面有几个配置点需要调整:</p>
<ul>
<li>通过catalog部署的时候，一定要设置admin的用户名和密码。</li>
<li>issuer url: https://sso-sso-app-demo.apps.ocpef0a.sandbox1717.opentlc.com/auth/realms/OpenShift </li>
<li>Valid Redirect URIs: https://oauth-openshift.apps.ocpef0a.sandbox1717.opentlc.com/*</li>
<li>ca.crt 这个文件可以在web界面上上传，但是传什么文件呢，是 openshift-ingress-operator 的 router-ca 里面的 tls.crt</li>
<li>界面老是刷不出来 openid 的登录方法： 这种情况，需要一路回退到系统界面，然后在跳转回来，再刷新才行。在登录界面一直刷新是没用的，应该是前端页面的小bug。</li>
<li>用户在rh sso里面单点认证以后，如果在openshift退出，想换一个用户登录，是不行的。 这种情况，需要登录到rh sso，把之前的用户session做登出操作，然后openshift上面才能换一个用户登录。</li>
<li>添加一个oauth Identity Providers 容易，但是没有删除界面。 这种情况，只能去直接改Identity Providers 的 yaml文件，删掉相关配置。</li>
</ul>
<h2 id="详细步骤"><a class="header" href="#详细步骤">详细步骤</a></h2>
<p>这里是配置过程的录屏：</p>
<ul>
<li>https://www.ixigua.com/i6800709743808610827/</li>
<li>https://youtu.be/Ak9qdgIbOic</li>
</ul>
<p>创建项目 sso-app-demo
<img src="ocp4/4.3/imgs/2020-03-04-19-24-18.png" alt="" /></p>
<p>从 catalog 里面选择 sso 创建， 注意设定sso管理员密码， 省的之后麻烦。
https://access.redhat.com/documentation/en-us/red_hat_single_sign-on/7.3/html-single/red_hat_single_sign-on_for_openshift/index#deploying_the_red_hat_single_sign_on_image_using_the_application_template
<img src="ocp4/4.3/imgs/2020-03-04-19-25-18.png" alt="" /></p>
<p>然后登录rh sso ，按照官方文档进行配置
https://access.redhat.com/documentation/en-us/red_hat_single_sign-on/7.3/html-single/red_hat_single_sign-on_for_openshift/index#OSE-SSO-AUTH-TUTE</p>
<h2 id="备用命令"><a class="header" href="#备用命令">备用命令</a></h2>
<pre><code class="language-bash">
# oc -n openshift import-image redhat-sso73-openshift:1.0

# oc new-project sso-app-demo
# oc policy add-role-to-user view system:serviceaccount:$(oc project -q):default

# oc policy remove-role-from-user view system:serviceaccount:$(oc project -q):default

# get issuer url
curl -k https://sso-sso-app-demo.apps.ocpef0a.sandbox1717.opentlc.com/auth/realms/OpenShift/.well-known/openid-configuration | python -m json.tool | grep issuer

# curl -k https://sso-sso-app-demo.apps.ocpef0a.sandbox1717.opentlc.com/auth/realms/OpenShift/.well-known/openid-configuration | jq | less

# # on mac create a ca
# cd ~/Downloads/tmp/tmp/
# openssl req \
#    -newkey rsa:2048 -nodes -keyout redhat.ren.key \
#    -x509 -days 3650 -out redhat.ren.crt -subj \
#    &quot;/C=CN/ST=GD/L=SZ/O=Global Security/OU=IT Department/CN=*.redhat.ren&quot;
# # upload crt to ocp
# oc create configmap ca-config-map --from-file=ca.crt=./redhat.ren.crt -n openshift-config

# oc delete configmap ca-config-map -n openshift-config

oc get secrets router-ca -n openshift-ingress-operator -o jsonpath='{.data.tls\.crt}' | base64 -d &gt; router.ca.crt

# oc get secrets router-ca -n openshift-ingress-operator -o jsonpath='{.data.tls\.key}' | base64 -d

# oc get OAuthClient

# if you want to debug, https://bugzilla.redhat.com/show_bug.cgi?id=1744599
oc patch authentication.operator cluster --type=merge -p &quot;{\&quot;spec\&quot;:{\&quot;operatorLogLevel\&quot;: \&quot;TraceAll\&quot;}}&quot;
oc patch authentication.operator cluster --type=merge -p &quot;{\&quot;spec\&quot;:{\&quot;operatorLogLevel\&quot;: \&quot;\&quot;}}&quot;

# update imate stream for offline
oc patch -n openshift is mysql -p &quot;{\&quot;spec\&quot;:{\&quot;tags\&quot;:[{\&quot;name\&quot;: \&quot;5.7\&quot;,\&quot;from\&quot;:{\&quot;name\&quot;:\&quot;registry.redhat.ren:5443/registry.redhat.io/rhscl/mysql-57-rhel7:latest\&quot;}}]}}&quot;
oc patch -n openshift is mysql -p &quot;{\&quot;spec\&quot;:{\&quot;tags\&quot;:[{\&quot;name\&quot;: \&quot;8.0\&quot;,\&quot;from\&quot;:{\&quot;name\&quot;:\&quot;registry.redhat.ren:5443/registry.redhat.io/rhscl/mysql-80-rhel7:latest\&quot;}}]}}&quot;
oc patch -n openshift is redhat-sso73-openshift -p &quot;{\&quot;spec\&quot;:{\&quot;tags\&quot;:[{\&quot;name\&quot;: \&quot;1.0\&quot;,\&quot;from\&quot;:{\&quot;name\&quot;:\&quot;registry.redhat.ren:5443/registry.redhat.io/redhat-sso-7/sso73-openshift:1.0\&quot;}}]}}&quot;
oc patch -n openshift is redhat-sso73-openshift -p &quot;{\&quot;spec\&quot;:{\&quot;tags\&quot;:[{\&quot;name\&quot;: \&quot;latest\&quot;,\&quot;from\&quot;:{\&quot;name\&quot;:\&quot;registry.redhat.ren:5443/registry.redhat.io/redhat-sso-7/sso73-openshift:1.0\&quot;}}]}}&quot;

oc create is ipa-server -n openshift
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ocp-scc"><a class="header" href="#ocp-scc">ocp scc</a></h1>
<h2 id="seccomp"><a class="header" href="#seccomp">SecComp</a></h2>
<p>https://docs.openshift.com/container-platform/4.3/authentication/managing-security-context-constraints.html</p>
<p>https://docs.docker.com/engine/security/seccomp/</p>
<p>https://docs.openshift.com/container-platform/4.3/nodes/nodes/nodes-nodes-managing.html</p>
<p>https://docs.openshift.com/container-platform/3.11/admin_guide/seccomp.html</p>
<p>https://gardener.cloud/050-tutorials/content/howto/secure-seccomp/</p>
<p>video</p>
<ul>
<li>https://www.bilibili.com/video/BV1Sa4y1x7UP/</li>
<li>https://youtu.be/gwu53N4dIws</li>
</ul>
<p>实验证明，容器内部更改date，不影响主机。</p>
<pre><code class="language-bash"># 在kube-system中创建特权用户
oc project kube-system
oc create serviceaccount -n kube-system demo-app
oc adm policy add-scc-to-user privileged -z demo-app

# 创建seccomp需要的security profile，我们这里创建一个只屏蔽clock set的profile
cat &lt;&lt; EOF &gt; demo.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: seccomp-profile
  namespace: kube-system
data:
  my-profile.json: |
    {
      &quot;defaultAction&quot;: &quot;SCMP_ACT_ALLOW&quot;,
      &quot;syscalls&quot;: [
        {
          &quot;name&quot;: &quot;clock_settime&quot;,
          &quot;action&quot;: &quot;SCMP_ACT_ERRNO&quot;
        }
      ]
    }
EOF
oc apply -f demo.yaml

# 创建一个daemon set，把我们自定义的security profile复制到各个节点的containerd运行时配置目录中，这样containerd就可以根据需要，使用这个security profile
cat &lt;&lt; EOF &gt; demo.yaml
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: seccomp
  namespace: kube-system
  labels:
    security: seccomp
spec:
  selector:
    matchLabels:
      security: seccomp
  template:
    metadata:
      labels:
        security: seccomp
    spec:
      initContainers:
      - name: installer
        image: docker.io/library/alpine:latest
        command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;cp -r -L /seccomp/*.json /host/seccomp/&quot;]
        securityContext:
            privileged: true
        volumeMounts:
        - name: profiles
          mountPath: /seccomp
        - name: hostseccomp
          mountPath: /host/seccomp
          readOnly: false
      containers:
      - name: pause
        image: gcr.io/google_containers/pause-amd64:3.0
      terminationGracePeriodSeconds: 5
      serviceAccount: demo-app
      volumes:
      - name: hostseccomp
        hostPath:
          path: /var/lib/kubelet/seccomp
      - name: profiles
        configMap:
          name: seccomp-profile
EOF
oc apply -f demo.yaml

# 在我们的demo project中，创建特权用户
oc project demo
oc create serviceaccount -n demo demo-app
oc adm policy add-scc-to-user privileged -z demo-app

# 在demo project中，创建应用，指定使用我们刚创建的security profile，为了展现效果，我们特别的指明，需要clock setting这个权限，后面可以看到，security profile屏蔽了这个请求。
cat &lt;&lt; EOF &gt; demo.yaml
---
apiVersion: v1
kind: Pod
metadata:
  annotations:
    seccomp.security.alpha.kubernetes.io/pod: &quot;localhost/my-profile.json&quot;
  name: demo
spec:
  nodeSelector:
    kubernetes.io/hostname: 'worker-0.ocp4.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo1
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot; ]
      args: [ &quot;trap : TERM INT; sleep infinity &amp; wait&quot; ]
      imagePullPolicy: Always
      securityContext:
        capabilities:
            add: [&quot;CAP_SYS_TIME&quot;]
  serviceAccount: demo-app
EOF
oc apply -n demo -f demo.yaml

# 进入这个pod，运行命令能看到命令失败
# this will failed, even you add the capabilities.
date -s &quot;1 second&quot;
# date: cannot set date: Operation not permitted
# Tue Mar 24 02:10:49 UTC 2020

# 为了对比，我们更改刚才的security profile，所有的都放开
# try to allow
cat &lt;&lt; EOF &gt; demo.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: seccomp-profile
  namespace: kube-system
data:
  my-profile.json: |
    {
      &quot;defaultAction&quot;: &quot;SCMP_ACT_ALLOW&quot;
    }
EOF
oc apply -f demo.yaml

# 通过打标签的方法，让daemon set重启，这样就把我们更新的security profile更新到各个节点上去
# restart damonset and restart pod.
# oc annotate -n kube-system ds seccomp last-update=&quot;`date`&quot;
oc patch -n kube-system ds seccomp -p &quot;{\&quot;spec\&quot;:{\&quot;template\&quot;:{\&quot;metadata\&quot;:{\&quot;annotations\&quot;:{\&quot;date\&quot;:\&quot;`date +'%s'`\&quot;}}}}}&quot;
oc get pod -n kube-system
# 进入demo pod，再次运行命令，可以看到命令运行成功。 
# this command will ok.
date -s &quot;1 second&quot;

# 最好，为了防止安全风险，我们将security profile重置成拒绝所有，并重启daemon set，更新到所有节点上。
# finally, restore 
cat &lt;&lt; EOF &gt; demo.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: seccomp-profile
  namespace: kube-system
data:
  my-profile.json: |
    {
      &quot;defaultAction&quot;: &quot;SCMP_ACT_ERRNO&quot;
    }
EOF
oc apply -f demo.yaml
# restart damonset and restart pod.
oc patch -n kube-system ds seccomp -p &quot;{\&quot;spec\&quot;:{\&quot;template\&quot;:{\&quot;metadata\&quot;:{\&quot;annotations\&quot;:{\&quot;date\&quot;:\&quot;`date +'%s'`\&quot;}}}}}&quot;

</code></pre>
<h2 id="capabilities"><a class="header" href="#capabilities">capabilities</a></h2>
<p>video</p>
<ul>
<li>https://youtu.be/yLdJghw-7xs</li>
<li>https://www.bilibili.com/video/BV1x64y1T7BZ/</li>
</ul>
<pre><code class="language-bash"># 创建pod,限制selinux，没有clock setting的capability.

cat &lt;&lt; EOF &gt; demo.yaml
---
kind: Deployment
apiVersion: apps/v1
metadata:
  annotations:
  name: demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo
  template:
    metadata:
      labels:
        app: demo  
    spec:
      nodeSelector:
        kubernetes.io/hostname: 'infra1.hsc.redhat.ren'
      restartPolicy: Always
      containers:
        - name: demo1
          image: &gt;- 
            registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
          env:
            - name: key
              value: value
          command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot; ]
          args: [ &quot;trap : TERM INT; sleep infinity &amp; wait&quot; ]
          imagePullPolicy: Always
          securityContext:
            capabilities:
                drop: [&quot;CAP_SYS_TIME&quot;]
      serviceAccount: demo-app

EOF
oc apply -n demo -f demo.yaml

# 进入pod，运行以下命令，不成功
date -s &quot;1 second&quot;

# 更新这个pod，赋予clock setting的capability
cat &lt;&lt; EOF &gt; demo.yaml
---
kind: Deployment
apiVersion: apps/v1
metadata:
  annotations:
  name: demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo
  template:
    metadata:
      labels:
        app: demo  
    spec:
      nodeSelector:
        kubernetes.io/hostname: 'infra1.hsc.redhat.ren'
      restartPolicy: Always
      containers:
        - name: demo1
          image: &gt;- 
            registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
          env:
            - name: key
              value: value
          command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot; ]
          args: [ &quot;trap : TERM INT; sleep infinity &amp; wait&quot; ]
          imagePullPolicy: Always
          securityContext:
            capabilities:
                add: [&quot;CAP_SYS_TIME&quot;]
      serviceAccount: demo-app

EOF
oc apply -n demo -f demo.yaml

# 进入pod，运行命令，可以看到命令运行成功
date -s &quot;1 second&quot;

# 删除演示应用
oc delete -n demo -f demo.yaml


</code></pre>
<h2 id="mcs"><a class="header" href="#mcs">MCS</a></h2>
<p>https://access.redhat.com/documentation/en-us/openshift_container_platform/3.3/html/installation_and_configuration/configuring-persistent-storage#selinuxoptions</p>
<p>http://www.178linux.com/98614</p>
<p>https://access.redhat.com/sites/default/files/video/files/mls_-_wide_8.pdf</p>
<p>https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/selinux_users_and_administrators_guide/mls</p>
<p>https://www.cnblogs.com/charlieroro/p/10830721.html</p>
<p>video</p>
<ul>
<li>https://youtu.be/XoQ11ZXEL7Y</li>
<li>https://www.bilibili.com/video/BV115411t777/</li>
</ul>
<pre><code class="language-bash"># on worker-0
# yum install selinux-policy-mls
# vi /etc/selinux/config
# # SELINUXTYPE=mls
# # SELINUXTYPE=mls
# getenforce
# fixfiles -F onboot
# cat /.autorelabel | less
# semanage login -l
# # semanage login --modify --range s0-s15:c0.c1023 root
# chcon -R -t default_t /data/mcs

# 创建测试用的目录，设置特殊的权限。
mkdir /data/mcs
chcon -R -l s0:c100 /data/mcs
chcon -R -t container_file_t /data/mcs
chown -R 1000:2000 /data/mcs
chmod -R 775 /data/mcs

# semanage fcontext -l | grep default_t

oc get project demo -o yaml 
# metadata:
#   annotations:
#     openshift.io/description: &quot;&quot;
#     openshift.io/display-name: &quot;&quot;
#     openshift.io/requester: kube:admin
#     openshift.io/sa.scc.mcs: s0:c23,c22
#     openshift.io/sa.scc.supplemental-groups: 1000550000/10000
#     openshift.io/sa.scc.uid-range: 1000550000/10000

# 创建pod，指定selinux的权限s0:c99。
cat &lt;&lt; EOF &gt; demo.yaml
---
apiVersion: v1
kind: Pod
metadata:
  annotations:
  name: demo
spec:
  nodeSelector:
    kubernetes.io/hostname: 'infra1.hsc.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo1
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot; ]
      args: [ &quot;trap : TERM INT; sleep infinity &amp; wait&quot; ]
      imagePullPolicy: Always
      securityContext:
        runAsUser: 1000
        runAsGroup: 2000 
        seLinuxOptions:
          level: 's0:c99'
      volumeMounts:
        - mountPath: /data
          name: demo 
          readOnly: false
  serviceAccount: demo-app
  volumes:
    - name: demo 
      hostPath:
        path: /data/mcs
        type: Directory
EOF
oc apply -n demo -f demo.yaml

# 进入pod检查权限，由于s0:c99 和目录的s0:c100 权限不符，以下操作失败
# below will fail
cd /data

# 修改目录权限，符合pod中的selinux声明
# after change the host path selinux flag
chcon -R -l s0:c99 /data/mcs
# system_u:object_r:default_t:s0:c99
# system_u:system_r:container_t:s0:c99
# seinfo -tcontainer_t
# seinfo -rsystem_r

# 进入pod操作，以下操作能够成功。
# then, below will ok
cd /data
ls
touch test


</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ocp-43-recover-from-node-not-ready"><a class="header" href="#ocp-43-recover-from-node-not-ready">ocp 4.3 recover from node not ready</a></h1>
<p>https://access.redhat.com/solutions/4923031</p>
<pre><code class="language-bash">cat &lt;&lt; &quot;EOF&quot; &gt; recover_kubeconfig.sh
#!/bin/bash

set -eou pipefail

# context
intapi=$(oc get infrastructures.config.openshift.io cluster -o &quot;jsonpath={.status.apiServerInternalURI}&quot;)
context=&quot;$(oc config current-context)&quot;
# cluster
cluster=&quot;$(oc config view -o &quot;jsonpath={.contexts[?(@.name==\&quot;$context\&quot;)].context.cluster}&quot;)&quot;
server=&quot;$(oc config view -o &quot;jsonpath={.clusters[?(@.name==\&quot;$cluster\&quot;)].cluster.server}&quot;)&quot;
# token
ca_crt_data=&quot;$(oc get secret -n openshift-machine-config-operator node-bootstrapper-token -o &quot;jsonpath={.data.ca\.crt}&quot; | base64 --decode)&quot;
namespace=&quot;$(oc get secret -n openshift-machine-config-operator node-bootstrapper-token  -o &quot;jsonpath={.data.namespace}&quot; | base64 --decode)&quot;
token=&quot;$(oc get secret -n openshift-machine-config-operator node-bootstrapper-token -o &quot;jsonpath={.data.token}&quot; | base64 --decode)&quot;

export KUBECONFIG=&quot;$(mktemp)&quot;
oc config set-credentials &quot;kubelet&quot; --token=&quot;$token&quot; &gt;/dev/null
ca_crt=&quot;$(mktemp)&quot;; echo &quot;$ca_crt_data&quot; &gt; $ca_crt
oc config set-cluster $cluster --server=&quot;$intapi&quot; --certificate-authority=&quot;$ca_crt&quot; --embed-certs &gt;/dev/null
oc config set-context kubelet --cluster=&quot;$cluster&quot; --user=&quot;kubelet&quot; &gt;/dev/null
oc config use-context kubelet &gt;/dev/null
cat &quot;$KUBECONFIG&quot;
EOF

chmod 755 recover_kubeconfig.sh
./recover_kubeconfig.sh &gt; kubeconfig-bootstrap

# scp kubeconfig-bootstrap to each affected nodes
scp kubeconfig-bootstrap core@node.ip.address:~/

# on each affected nodes
systemctl stop kubelet
mkdir -p /root/backup-certs
cp -a /var/lib/kubelet/pki /var/lib/kubelet/kubeconfig /root/backup-certs
rm -rf /var/lib/kubelet/pki /var/lib/kubelet/kubeconfig
cp /home/core/kubeconfig-bootstrap /etc/kubernetes/kubeconfig
systemctl start kubelet

# on helper
oc get node
oc get csr
oc get csr -ojson | jq -r '.items[] | select(.status == {} ) | .metadata.name' | xargs oc adm certificate approve

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-43-qos"><a class="header" href="#openshift-43-qos">openshift 4.3 QoS</a></h1>
<p>https://docs.openshift.com/container-platform/4.3/nodes/pods/nodes-pods-configuring.html</p>
<p>https://docs.openshift.com/container-platform/3.11/admin_guide/managing_pods.html#admin-guide-manage-pods-limit-bandwidth</p>
<p>video</p>
<ul>
<li>https://youtu.be/ghObMDoLcAQ</li>
<li>https://www.bilibili.com/video/BV16Z4y1W75P/</li>
</ul>
<pre><code class="language-bash">
# 创建一个服务端Pod，用iperf3作为服务端，服务端限制带宽1Mb/s。再创建一个客户端Pod，有iperf3作为客户端。
cat &lt;&lt; EOF &gt; demo.yaml
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod
  annotations:
    kubernetes.io/ingress-bandwidth: 1M
    kubernetes.io/egress-bandwidth: 1M
spec:
  nodeSelector:
    kubernetes.io/hostname: 'infra1.hsc.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo1
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;iperf3&quot;, &quot;-s&quot;, &quot;-p&quot; ]
      args: [ &quot;6666&quot; ]
      imagePullPolicy: Always
---
kind: Deployment
apiVersion: apps/v1
metadata:
  annotations:
  name: iperf
spec:
  replicas: 1
  selector:
    matchLabels:
      app: iperf
  template:
    metadata:
      labels:
        app: iperf  
    spec:
      nodeSelector:
        kubernetes.io/hostname: 'infra0.hsc.redhat.ren'
      restartPolicy: Always
      containers:
        - name: iperf
          image: &gt;- 
            registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
          env:
            - name: key
              value: value
          command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot; ]
          args: [ &quot;trap : TERM INT; sleep infinity &amp; wait&quot; ]
          imagePullPolicy: Always
EOF
oc apply -n demo -f demo.yaml

# 查找服务端pod ip
oc get pod -o wide

# 进入客户端，进行测速
oc exec -it iperf-5b95866ff5-c9p9m -- iperf3 -t 20 -b 2M -p 6666 -c 10.254.5.52

# 查看服务端pod的日志，可以看到流量信息

# 更改服务端带宽为2M
oc delete pod -n demo demo-pod

cat &lt;&lt; EOF &gt; demo1.yaml
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod
  annotations:
    kubernetes.io/ingress-bandwidth: 2M
    kubernetes.io/egress-bandwidth: 2M
spec:
  nodeSelector:
    kubernetes.io/hostname: 'infra1.hsc.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo1
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;iperf3&quot;, &quot;-s&quot;, &quot;-p&quot; ]
      args: [ &quot;6666&quot; ]
      imagePullPolicy: Always

EOF
oc apply -n demo -f demo1.yaml

# 查找服务端pod ip
oc get pod -o wide

# 进入客户端，进行测速
oc exec -it iperf-5b95866ff5-c9p9m -- iperf3 -t 20 -b 2M -p 6666 -c 10.254.5.53

# 查看服务端pod的日志，可以看到流量信息

oc delete -n demo -f demo.yaml

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-43-qos-1"><a class="header" href="#openshift-43-qos-1">openshift 4.3 QoS</a></h1>
<p>本文测试，openshift (ovs) pod 在大流量下， 限流功能的表现。</p>
<p>video</p>
<ul>
<li>https://youtu.be/IaWdkPsRinw</li>
<li>https://www.bilibili.com/video/BV1cV411d7LV/</li>
</ul>
<p>参考资料：</p>
<p>https://docs.openshift.com/container-platform/4.3/nodes/pods/nodes-pods-configuring.html</p>
<p>https://docs.openshift.com/container-platform/3.11/admin_guide/managing_pods.html#admin-guide-manage-pods-limit-bandwidth</p>
<pre><code class="language-bash">
# 查看infra0, infra1上面的端口速度，可以看到是10GE的网口
ethtool em1
# Settings for em1:
#         Supported ports: [ FIBRE ]
#         Supported link modes:   1000baseT/Full
#                                 10000baseT/Full
#         Supported pause frame use: Symmetric Receive-only
#         Supports auto-negotiation: No
#         Supported FEC modes: Not reported
#         Advertised link modes:  10000baseT/Full
#         Advertised pause frame use: No
#         Advertised auto-negotiation: No
#         Advertised FEC modes: Not reported
#         Speed: 10000Mb/s
#         Duplex: Full
#         Port: FIBRE
#         PHYAD: 1
#         Transceiver: internal
#         Auto-negotiation: off
#         Supports Wake-on: g
#         Wake-on: d
#         Current message level: 0x00000000 (0)

#         Link detected: yes

# 创建2个服务端Pod，用iperf3作为服务端，服务端不限速。再创建一个客户端Pod，有iperf3作为客户端。
cat &lt;&lt; EOF &gt; demo.yaml
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod1
  namespace: demo
spec:
  nodeSelector:
    kubernetes.io/hostname: 'infra1.hsc.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo1
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;iperf3&quot;, &quot;-s&quot;, &quot;-p&quot; ]
      args: [ &quot;6666&quot; ]
      imagePullPolicy: Always
      resources:
        requests:
          cpu: 4.0
          memory: 8Gi
        limits:
          cpu: 60.0
          memory: 100Gi
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod2
  namespace: default
spec:
  nodeSelector:
    kubernetes.io/hostname: 'infra1.hsc.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo1
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;iperf3&quot;, &quot;-s&quot;, &quot;-p&quot; ]
      args: [ &quot;6666&quot; ]
      imagePullPolicy: Always
      resources:
        requests:
          cpu: 4.0
          memory: 8Gi
        limits:
          cpu: 60.0
          memory: 100Gi
---
kind: Pod
apiVersion: v1
metadata:
  name: iperf
  namespace: zte
spec:
  nodeSelector:
    kubernetes.io/hostname: 'infra0.hsc.redhat.ren'
  restartPolicy: Always
  containers:
    - name: iperf
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot; ]
      args: [ &quot;trap : TERM INT; sleep infinity &amp; wait&quot; ]
      imagePullPolicy: Always
      resources:
        requests:
          cpu: 4.0
          memory: 8Gi
        limits:
          cpu: 60.0
          memory: 100Gi
EOF
oc apply -f demo.yaml

# 查找服务端pod ip
oc get pod -A -o wide | grep demo-pod
oc get pod -n zte -o wide

pod_demo1_ip=$(oc get pod -n demo demo-pod1 -o json | jq -r '.status.podIPs[0].ip')

pod_demo2_ip=$(oc get pod -n default demo-pod2 -o json | jq -r '.status.podIPs[0].ip')

echo $pod_demo1_ip
echo $pod_demo2_ip

# 进入客户端，对两个服务端pod进行测速
/bin/rm -f nohup.out
nohup oc exec -n zte -it iperf -- iperf3 -T demo1 -i 10 -t 30 -b 3G -P 6 -p 6666 -c $pod_demo1_ip 2&gt;&amp;1 &amp;

nohup oc exec -n zte -it iperf -- iperf3 -T demo2 -i 10 -t 30 -b 6G -P 6 -p 6666 -c $pod_demo2_ip 2&gt;&amp;1 &amp;

tail -f nohup.out

# 调整流量重新测试，对两个服务端pod进行测速
/bin/rm -f nohup.out
nohup oc exec -n zte -it iperf -- iperf3 -T demo1 -i 10 -t 30 -b 6G -P 6 -p 6666 -c $pod_demo1_ip 2&gt;&amp;1 &amp;

nohup oc exec -n zte -it iperf -- iperf3 -T demo2 -i 10 -t 30 -b 6G -P 6 -p 6666 -c $pod_demo2_ip 2&gt;&amp;1 &amp;

tail -f nohup.out

# 调整流量重新测试，对两个服务端pod进行测速
/bin/rm -f nohup.out
nohup oc exec -n zte -it iperf -- iperf3 -T demo1 -i 10 -t 30 -b 8G -P 6 -p 6666 -c $pod_demo1_ip 2&gt;&amp;1 &amp;

nohup oc exec -n zte -it iperf -- iperf3 -T demo2 -i 10 -t 30 -b 6G -P 6 -p 6666 -c $pod_demo2_ip 2&gt;&amp;1 &amp;

tail -f nohup.out

# 查看服务端pod的日志，可以看到流量信息

# 更改服务端带宽为6G
oc delete pod -n demo demo-pod1

cat &lt;&lt; EOF &gt; demo1.yaml
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod1
  namespace: demo
  annotations:
    kubernetes.io/ingress-bandwidth: 6G
    kubernetes.io/egress-bandwidth: 6G
spec:
  nodeSelector:
    kubernetes.io/hostname: 'infra1.hsc.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo1
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;iperf3&quot;, &quot;-s&quot;, &quot;-p&quot; ]
      args: [ &quot;6666&quot; ]
      imagePullPolicy: Always

EOF
oc apply -n demo -f demo1.yaml

# 查找服务端pod ip
oc get pod -A -o wide | grep demo-pod
oc get pod -n zte -o wide

pod_demo1_ip=$(oc get pod -n demo demo-pod1 -o json | jq -r '.status.podIPs[0].ip')

pod_demo2_ip=$(oc get pod -n default demo-pod2 -o json | jq -r '.status.podIPs[0].ip')

echo $pod_demo1_ip
echo $pod_demo2_ip

# 调整流量重新测试，对两个服务端pod进行测速
/bin/rm -f nohup.out
nohup oc exec -n zte -it iperf -- iperf3 -T demo1 -i 10 -t 30 -b 8G -P 6 -p 6666 -c $pod_demo1_ip 2&gt;&amp;1 &amp;

nohup oc exec -n zte -it iperf -- iperf3 -T demo2 -i 10 -t 30 -b 6G -P 6 -p 6666 -c $pod_demo2_ip 2&gt;&amp;1 &amp;

tail -f nohup.out

# 查看服务端pod的日志，可以看到流量信息

# 更改服务端带宽为3G
oc delete pod -n demo demo-pod1

cat &lt;&lt; EOF &gt; demo1.yaml
---
kind: Pod
apiVersion: v1
metadata:
  name: demo-pod1
  namespace: demo
  annotations:
    kubernetes.io/ingress-bandwidth: 3G
    kubernetes.io/egress-bandwidth: 3G
spec:
  nodeSelector:
    kubernetes.io/hostname: 'infra1.hsc.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo1
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;iperf3&quot;, &quot;-s&quot;, &quot;-p&quot; ]
      args: [ &quot;6666&quot; ]
      imagePullPolicy: Always

EOF
oc apply -n demo -f demo1.yaml

# 查找服务端pod ip
oc get pod -A -o wide | grep demo-pod
oc get pod -n zte -o wide

pod_demo1_ip=$(oc get pod -n demo demo-pod1 -o json | jq -r '.status.podIPs[0].ip')

pod_demo2_ip=$(oc get pod -n default demo-pod2 -o json | jq -r '.status.podIPs[0].ip')

echo $pod_demo1_ip
echo $pod_demo2_ip

# 调整流量重新测试，对两个服务端pod进行测速
/bin/rm -f nohup.out
nohup oc exec -n zte -it iperf -- iperf3 -T demo1 -i 10 -t 30 -b 8G -P 6 -p 6666 -c $pod_demo1_ip 2&gt;&amp;1 &amp;

nohup oc exec -n zte -it iperf -- iperf3 -T demo2 -i 10 -t 30 -b 6G -P 6 -p 6666 -c $pod_demo2_ip 2&gt;&amp;1 &amp;

tail -f nohup.out

# 查看服务端pod的日志，可以看到流量信息

oc delete -f demo.yaml

</code></pre>
<h2 id="package-size"><a class="header" href="#package-size">package size</a></h2>
<pre><code class="language-bash">
oc exec -n zte -it iperf -- iperf3 -T demo1 -V -b 10G -M 1500 -p 6666 -c $pod_demo1_ip
# demo1:  Test Complete. Summary Results:
# demo1:  [ ID] Interval           Transfer     Bandwidth       Retr
# demo1:  [  4]   0.00-10.00  sec  3.66 GBytes  3.15 Gbits/sec  221             sender
# demo1:  [  4]   0.00-10.00  sec  3.66 GBytes  3.14 Gbits/sec                  receiver
# demo1:  CPU Utilization: local/sender 38.5% (1.8%u/36.6%s), remote/receiver 9.6% (0.4%u/9.2%s)

oc exec -n zte -it iperf -- iperf3 -T demo1 -V -b 10G -M 1000 -p 6666 -c $pod_demo1_ip
# demo1:  Test Complete. Summary Results:
# demo1:  [ ID] Interval           Transfer     Bandwidth       Retr
# demo1:  [  4]   0.00-10.00  sec  2.68 GBytes  2.30 Gbits/sec  304             sender
# demo1:  [  4]   0.00-10.00  sec  2.68 GBytes  2.30 Gbits/sec                  receiver
# demo1:  CPU Utilization: local/sender 22.8% (1.0%u/21.7%s), remote/receiver 2.4% (0.2%u/2.2%s)

oc exec -n zte -it iperf -- iperf3 -T demo1 -V -b 10G -M 500 -p 6666 -c $pod_demo1_ip
# demo1:  Test Complete. Summary Results:
# demo1:  [ ID] Interval           Transfer     Bandwidth       Retr
# demo1:  [  4]   0.00-10.00  sec  1.32 GBytes  1.14 Gbits/sec  195             sender
# demo1:  [  4]   0.00-10.00  sec  1.32 GBytes  1.13 Gbits/sec                  receiver
# demo1:  CPU Utilization: local/sender 13.6% (0.9%u/12.7%s), remote/receiver 4.2% (0.3%u/4.0%s)

oc exec -n zte -it iperf -- iperf3 -T demo1 -V -b 10G -M 100 -p 6666 -c $pod_demo1_ip
# demo1:  Test Complete. Summary Results:
# demo1:  [ ID] Interval           Transfer     Bandwidth       Retr
# demo1:  [  4]   0.00-10.00  sec   224 MBytes   188 Mbits/sec  590             sender
# demo1:  [  4]   0.00-10.00  sec   223 MBytes   187 Mbits/sec                  receiver
# demo1:  CPU Utilization: local/sender 3.5% (0.2%u/3.3%s), remote/receiver 10.2% (0.1%u/10.1%s)


oc exec -n zte -it iperf -- iperf3 -T demo1 -V -b 10G -M 1500 -P 10 -p 6666 -c $pod_demo1_ip
# demo1:  [SUM]   0.00-10.00  sec  9.21 GBytes  7.91 Gbits/sec  4804             sender
# demo1:  [SUM]   0.00-10.00  sec  9.20 GBytes  7.90 Gbits/sec                  receiver
# demo1:  CPU Utilization: local/sender 65.3% (2.5%u/62.8%s), remote/receiver 28.5% (0.4%u/28.1%s)

oc exec -n zte -it iperf -- iperf3 -T demo1 -V -b 10G -M 1000 -P 10 -p 6666 -c $pod_demo1_ip
# demo1:  [SUM]   0.00-10.00  sec  8.62 GBytes  7.40 Gbits/sec  4354             sender
# demo1:  [SUM]   0.00-10.00  sec  8.61 GBytes  7.40 Gbits/sec                  receiver
# demo1:  CPU Utilization: local/sender 73.7% (2.4%u/71.3%s), remote/receiver 19.7% (0.9%u/18.8%s)

oc exec -n zte -it iperf -- iperf3 -T demo1 -V -b 10G -M 500 -P 10 -p 6666 -c $pod_demo1_ip
# demo1:  [SUM]   0.00-10.00  sec  4.72 GBytes  4.05 Gbits/sec  7142             sender
# demo1:  [SUM]   0.00-10.00  sec  4.71 GBytes  4.05 Gbits/sec                  receiver
# demo1:  CPU Utilization: local/sender 49.4% (2.0%u/47.3%s), remote/receiver 17.6% (0.6%u/17.1%s)

oc exec -n zte -it iperf -- iperf3 -T demo1 -V -b 10G -M 100 -P 10 -p 6666 -c $pod_demo1_ip
# demo1:  [SUM]   0.00-10.00  sec   895 MBytes   750 Mbits/sec  10362             sender
# demo1:  [SUM]   0.00-10.00  sec   889 MBytes   745 Mbits/sec                  receiver
# demo1:  CPU Utilization: local/sender 14.4% (0.6%u/13.7%s), remote/receiver 22.6% (0.3%u/22.3%s)



iperf3 -T demo1 -V -b 10G -M 1500 -p 6666 -c 117.177.241.24
# demo1:  Test Complete. Summary Results:
# demo1:  [ ID] Interval           Transfer     Bandwidth       Retr
# demo1:  [  4]   0.00-10.00  sec  10.5 GBytes  8.98 Gbits/sec    0             sender
# demo1:  [  4]   0.00-10.00  sec  10.4 GBytes  8.98 Gbits/sec                  receiver
# demo1:  CPU Utilization: local/sender 52.8% (2.7%u/50.2%s), remote/receiver 30.6% (1.0%u/29.5%s)

iperf3 -T demo1 -V -b 10G -M 1000 -p 6666 -c 117.177.241.24
# demo1:  Test Complete. Summary Results:
# demo1:  [ ID] Interval           Transfer     Bandwidth       Retr
# demo1:  [  4]   0.00-10.00  sec  9.28 GBytes  7.97 Gbits/sec    0             sender
# demo1:  [  4]   0.00-10.00  sec  9.27 GBytes  7.96 Gbits/sec                  receiver
# demo1:  CPU Utilization: local/sender 54.4% (3.2%u/51.2%s), remote/receiver 19.2% (0.1%u/19.1%s)

iperf3 -T demo1 -V -b 10G -M 500 -p 6666 -c 117.177.241.24
# demo1:  Test Complete. Summary Results:
# demo1:  [ ID] Interval           Transfer     Bandwidth       Retr
# demo1:  [  4]   0.00-10.00  sec  6.14 GBytes  5.28 Gbits/sec  5857             sender
# demo1:  [  4]   0.00-10.00  sec  6.14 GBytes  5.27 Gbits/sec                  receiver
# demo1:  CPU Utilization: local/sender 30.6% (2.1%u/28.5%s), remote/receiver 12.6% (0.1%u/12.5%s)

iperf3 -T demo1 -V -b 10G -M 100 -p 6666 -c 117.177.241.24
# demo1:  Test Complete. Summary Results:
# demo1:  [ ID] Interval           Transfer     Bandwidth       Retr
# demo1:  [  4]   0.00-10.00  sec  1.41 GBytes  1.21 Gbits/sec  3499             sender
# demo1:  [  4]   0.00-10.00  sec  1.40 GBytes  1.21 Gbits/sec                  receiver
# demo1:  CPU Utilization: local/sender 8.2% (0.9%u/7.4%s), remote/receiver 23.8% (0.1%u/23.7%s)


</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="如何添加-http_proxy-来下载镜像"><a class="header" href="#如何添加-http_proxy-来下载镜像">如何添加 http_proxy 来下载镜像</a></h1>
<p>我们的部署环境，如果有一个image proxy，那么可以配置集群，使用这个proxy去下载镜像。</p>
<p>关键是crio的环境变量，所以给这个目录添加一个环境变量进去，/etc/systemd/system/crio.service.d/</p>
<pre><code class="language-bash">cat &lt;&lt; EOF &gt; crio-env.conf
[Service]
Environment=HTTP_PROXY=http://v.redhat.ren:8080
Environment=HTTPS_PROXY=http://v.redhat.ren:8080
Environment=NO_PROXY=redhat.ren,10.254.0.0/16,172.30.0.0/16
EOF

config_source=$(cat ./crio-env.conf | python3 -c &quot;import sys, urllib.parse; print(urllib.parse.quote(''.join(sys.stdin.readlines())))&quot;  )

cat &lt;&lt;EOF &gt; 50-crio-env-conf.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 50-crio-env-conf
spec:
  config:
    ignition:
      version: 2.2.0
    storage:
      files:
      - contents:
          source: data:text/plain,${config_source}
          verification: {}
        filesystem: root
        mode: 0420
        path: /etc/systemd/system/crio.service.d/20-wzh-env.conf
      - contents:
          source: data:text/plain,${config_source}
          verification: {}
        filesystem: root
        mode: 0420
        path: /etc/systemd/system/kubelet.service.d/20-wzh-env.conf
      - contents:
          source: data:text/plain,${config_source}
          verification: {}
        filesystem: root
        mode: 0420
        path: /etc/systemd/system/machine-config-daemon-host.service.d/20-wzh-env.conf
      - contents:
          source: data:text/plain,${config_source}
          verification: {}
        filesystem: root
        mode: 0420
        path: /etc/systemd/system/pivot.service.d/20-wzh-env.conf
EOF
oc apply -f 50-crio-env-conf.yaml -n openshift-config
</code></pre>
<p>等待集群重启以后，测试一下</p>
<pre><code class="language-bash">cat &lt;&lt; EOF &gt; test-local-dc.yaml
kind: DeploymentConfig
apiVersion: apps.openshift.io/v1
metadata:
  name: busybox
  labels:
    run: busybox
spec:
  replicas: 1
  template:
    metadata:
      labels:
        run: busybox
    spec:
      containers:
        - name: busybox
          image: 'docker.io/busybox:1.28.0-glibc'
          command:
            - sleep
            - '36000'

EOF
oc apply -f test-local-dc.yaml
</code></pre>
<p><img src="ocp4/4.3/imgs/2020-02-22-23-11-39.png" alt="" />
虽然实验环境网络问题，没下载成功，但是看到下载是在走proxy了。</p>
<h2 id="以下是弯路"><a class="header" href="#以下是弯路">以下是弯路</a></h2>
<p>这样就可以通过内网的proxy server去pull image了。</p>
<p>调优 /etc/crio/crio.conf 的方法不可以，因为查过源代码以后，发现下面链接说的操作，源代码里面也就支持3个选项，其他选项都不支持。
https://www.redhat.com/en/blog/red-hat-openshift-container-platform-4-now-defaults-cri-o-underlying-container-engine</p>
<p>然后从源代码里面，高兴的发现，/etc/systemd/system/crio.service.d/10-default-env.conf 是可以通过proxy的配置生效的。 
https://github.com/openshift/machine-config-operator/blob/master/templates/common/_base/files/etc-systemd-system-crio.service.d-10-default-env.conf.yaml</p>
<p>配置一个proxy https://access.redhat.com/solutions/3442811</p>
<pre><code class="language-yaml">apiVersion: config.openshift.io/v1
kind: Proxy
metadata:
  name: cluster
spec:
  httpProxy: http://v.redhat.ren:8080 
  httpsProxy: http://v.redhat.ren:8080 
  noProxy: example.com 
</code></pre>
<pre><code class="language-bash">cat &lt;&lt; EOF &gt; proxy.yaml
apiVersion: config.openshift.io/v1
kind: Proxy
metadata:
  name: cluster
spec:
  httpProxy: http://v.redhat.ren:8080 
  httpsProxy: http://v.redhat.ren:8080 
  readinessEndpoints:
  - http://www.google.com 
  noProxy: example.com 
  trustedCA:
    name: ca.for.proxy
EOF
oc apply -f proxy.yaml

cat &lt;&lt; EOF &gt; proxy.yaml
apiVersion: config.openshift.io/v1
kind: Proxy
metadata:
  name: cluster
spec: {}
EOF
oc apply -f proxy.yaml

cat /etc/systemd/system/crio.service.d/10-default-env.conf

cat &lt;&lt; EOF &gt; ca.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ca.for.proxy
  namespace: openshift-config 
data:
  ca-bundle.crt: | 
    -----BEGIN PRIVATE KEY-----
    MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCpRqAtwkQsmdA5
    qDyAV7ABoRmZdDh7aaH9OY+gHRVtMDYbEH1e3u4oIJ5CoAK4EiZ/AZA2Pb5xFO+5
    63YwMFEucg0TcCAs20yFbhkRXac1UxsGmx3zUSfex6/A6yxwyx14/HBoli6Trqpr
    oPxUFDFoHHe6zIqgQkdjdYttL/vwrVg2yH2Z3IS1qQ/uN8UpyL/yY48VRimQsGjX
    9FmRusONsUdRYh29gbOI76hJ7ooCNGvgbXq/6L6OGu6by+g6MgqHtBWMjnObWkWV
    ln1lRRfmhwlGO0136lURt58diJSIWPXOpSO4Ulc2JMH9D+pgAD59JU4pm1PvGotc
    e+WIxvJ9AgMBAAECggEACpulcBirgwwEk4hqejSEkCWTYB17aKh/AUp5KLSJ4jTS
    PzHyWV6pGBSrNkumv/hLN0xWyD9oTtfcCg+qcWylub5l+WDec1Eu43G52m+/CcVy
    fSB9aQEd+YUUC4fxWgQwjaNsO/Gla5XXkjUdevtk+TxHeIpW6aIdrSrxmN8X78Yj
    F0FIPYSAM4Lh2ZdykFS9igbteRN27WGlypKF6D7efDfbh4TLuVtSMRyehjewyy3U
    DAYkkMm1SD/TH4HJQU8eU3Gp3ZZmP4uSTESfBc/6lrSy/ooXqtc/x8dv0SQtky0I
    FQu/bTdrSjz3gOKZVfaLsG4LMiMo7M4SekyU2EGulQKBgQDUobsMXV0WrwVF4JFF
    ug3PxXwcatlnesrlcOPQQdhZz4ngk3z49GxPrXykzFQ5KtMCsgyOhNpXOVu6vqew
    0QmxJvF8Mo0GhwIOANlrQSn/Flt5s5GIPqteAE//RxSsAhRm6fDnxKik2aT5XOYl
    9GQvFvPDtjSR0nBHQg5BuBgtbwKBgQDLzSDr61tbU02/bV/td6CkVMSSpGHpfUU+
    0rGC9/JzBmBDr/mC5fDUN0bno1zq35HURxzhk306BJSbMMwnwmUFgWxPuJwlVo2V
    Zs3x41eYzTj7JOPZ/AphR+6pdpXlsoxpXUQRgWq1j8hq0wUqDL8s0ltzoDJFMxri
    J9N7fv6A0wKBgQChFk3Q1kKZ1sqV38XvHz8rcx/Nn51I6hwgqt/MfLXdhH+eJd59
    9R7BVluhtjLwhGMMHbuplTic8BVwatQ7/oHrNeepAdsZYNrLpRUSTnH0kQmIL+RH
    ZcMKGg6BBWbB0WmHdiBOVgy1pzV2vUyW4ImtqyPN15IID3eEZKTMYR3f/QKBgFke
    QBEp/+71hH/64gHDV/nEH5lITJB/ePI5y+nLZrepyBqRLvhweFk0Oss8Anuqe+hp
    mFWD2zStoBYkxoF0XhyENcq+nXkuWgdExzXJBhsJUqtvvDssHZXgkJqGApJI+2Fv
    qT5Ga1UtpKQh1pZGsKp26gqruI/OAyl15OKR69SFAoGADAOAADooY3Qcn9AWH1e8
    ebSDdimi4j1H9yFvcByaJkNrGhNgKwYYYeLsCvwxGLjRontoH6xOJAVdwmadV/CH
    6Ket3yJLWRIuu1N1IKvfLEqLsp2sbWKInhohEfh5yZmvCeTUjJKkz62DYS20JsN0
    1+gdBRElKgEz14GTvj7lpas=
    -----END PRIVATE KEY-----
    -----BEGIN CERTIFICATE-----
    MIIDVzCCAj+gAwIBAgIJANzkXo7TCVYVMA0GCSqGSIb3DQEBCwUAMEIxCzAJBgNV
    BAYTAlhYMRUwEwYDVQQHDAxEZWZhdWx0IENpdHkxHDAaBgNVBAoME0RlZmF1bHQg
    Q29tcGFueSBMdGQwHhcNMjAwMjIyMDMxOTMxWhcNMjEwMjIxMDMxOTMxWjBCMQsw
    CQYDVQQGEwJYWDEVMBMGA1UEBwwMRGVmYXVsdCBDaXR5MRwwGgYDVQQKDBNEZWZh
    dWx0IENvbXBhbnkgTHRkMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA
    qUagLcJELJnQOag8gFewAaEZmXQ4e2mh/TmPoB0VbTA2GxB9Xt7uKCCeQqACuBIm
    fwGQNj2+cRTvuet2MDBRLnINE3AgLNtMhW4ZEV2nNVMbBpsd81En3sevwOsscMsd
    ePxwaJYuk66qa6D8VBQxaBx3usyKoEJHY3WLbS/78K1YNsh9mdyEtakP7jfFKci/
    8mOPFUYpkLBo1/RZkbrDjbFHUWIdvYGziO+oSe6KAjRr4G16v+i+jhrum8voOjIK
    h7QVjI5zm1pFlZZ9ZUUX5ocJRjtNd+pVEbefHYiUiFj1zqUjuFJXNiTB/Q/qYAA+
    fSVOKZtT7xqLXHvliMbyfQIDAQABo1AwTjAdBgNVHQ4EFgQUaTkD399lxrjHrHkl
    Mq1se4L+yr0wHwYDVR0jBBgwFoAUaTkD399lxrjHrHklMq1se4L+yr0wDAYDVR0T
    BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAkuBFWQV2dFfwVChhVGKxynQ3JD48
    tT27b8G0YHMIM1WGkYIO7jWOx4Vvpo0ykqvwP1r7gVLHectPynCt55c1/lN9FxuV
    o+VTGN2ObA8AyEr4pPUJf7rav9GBlyJlIGL2IM4A9b0aCqfwIg0OyTSQzI5E5Cv8
    SDj1XTCPwkZT+Vq8aXorpej4dNhz//0AA872pAtwp9ex+KPOVRRZM4cQfQof3saB
    oPSkc8R2sA1TYNweeF4cWctWz2G0Vy/uo0fwcTb9NJwpzZlRBclg2S9WA9dMwnV8
    LVnyLpo2cf4R2z8zDcfDoQV7i6JxzfTQCeUO1Zy4zPTbtKt1k8g3dYfF0w==
    -----END CERTIFICATE-----
EOF
oc apply -f ca.yaml
</code></pre>
<pre><code class="language-yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: ContainerRuntimeConfig
metadata:
 name: set-log-and-pid
spec:
 machineConfigPoolSelector:
   matchLabels:
     debug-crio: config1
 containerRuntimeConfig:
   conmon_env: &quot;[ HTTP_PROXY=http://v.redhat.ren:8080, HTTPS_PROXY=http://v.redhat.ren:8080 ]&quot; 
</code></pre>
<pre><code class="language-bash">cat &lt;&lt; EOF &gt; crio.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: ContainerRuntimeConfig
metadata:
 name: set-log-and-pid
spec:
 machineConfigPoolSelector:
   matchLabels:
     debug-crio: config1
 containerRuntimeConfig:
   conmon_env: '[HTTP_PROXY=http://v.redhat.ren:8080,HTTPS_PROXY=http://v.redhat.ren:8080]'
EOF

oc apply -f crio.yaml

oc delete -f crio.yaml

oc edit MachineConfigPool/worker

oc get ContainerRuntimeConfig -o yaml

oc get MachineConfigs

python3 -c &quot;import sys, urllib.parse; print(urllib.parse.unquote(sys.argv[1]))&quot; $(oc get MachineConfig/rendered-worker-a01b5da25ec85d2f0ffabfeb1fbe996d -o YAML | grep -B4 crio.conf | grep source | tail -n 1 | cut -d, -f2) | grep conmon
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="numa"><a class="header" href="#numa">numa</a></h1>
<p>https://docs.openshift.com/container-platform/4.3/scalability_and_performance/using-topology-manager.html#topology_manager_policies_using-topology-manager</p>
<p>https://www.sharcnet.ca/help/index.php/Using_numactl</p>
<p>video</p>
<ul>
<li>https://youtu.be/J2VQQZxk3eY</li>
<li>https://www.bilibili.com/video/BV1HK4y1r7Di/</li>
</ul>
<pre><code class="language-bash">oc get featuregate/cluster -o yaml

oc patch featuregate/cluster -p '{&quot;spec&quot;: { &quot;featureSet&quot;: &quot;LatencySensitive&quot; } }' --type=merge

oc get KubeletConfig -o yaml

cat &lt;&lt; EOF &gt; cpumanager-kubeletconfig.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: cpumanager-enabled
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: cpumanager-enabled
  kubeletConfig:
     cpuManagerPolicy: static
     cpuManagerReconcilePeriod: 5s
     topologyManagerPolicy: single-numa-node 
EOF
oc apply -f cpumanager-kubeletconfig.yaml

oc project demo 

cat &lt;&lt; EOF &gt; cpumanager-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  generateName: cpumanager-
spec:
  containers:
  - name: cpumanager
    image: gcr.io/google_containers/pause-amd64:3.0
    resources:
      requests:
        cpu: 1
        memory: &quot;1G&quot;
      limits:
        cpu: 1
        memory: &quot;1G&quot;
  nodeSelector:
    cpumanager: &quot;true&quot;
EOF
oc apply -f cpumanager-pod.yaml

# on the worker node
yum install numactl
# 指定命令运行在NUMA NODE0上（CPU，内存都来自NUMA NODE0）
numactl --cpunodebind=0 --membind=0 COMMAND
# 指定命令CPU来自NUMA NODE1，内存尽可能来自NUMA NODE1，如果NUMA NODE1没有足够的内存了，则使用NUMA NODE0上的内存
numactl --cpunodebind=1 --preferred=1 COMMAND
# 获取进程cpu的mask
taskset -p &lt;pid&gt;
# pid 26624's current affinity mask: ff  这个是没设置掩码

# 进程的memory信息可以通过命令获取
numastat &lt;pid&gt;
# Per-node process memory usage (in MBs) for PID 26624 (firefox)
#                            Node 0           Total
#                   --------------- ---------------
# Huge                         0.00            0.00
# Heap                         0.00            0.00
# Stack                        0.08            0.08
# Private                    208.50          208.50
# ----------------  --------------- ---------------
# Total                      208.58          208.58
# 类似于进程，在某个NUMA Node上占用多少内存

# 查询PCI网卡设备所在numa node
cat /sys/class/net/&lt;devicename&gt;/device/numa_node


# back to normal
cat &lt;&lt; EOF &gt; cpumanager-kubeletconfig.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: cpumanager-enabled
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: cpumanager-enabled
  kubeletConfig:
     cpuManagerPolicy: static
     cpuManagerReconcilePeriod: 5s
     topologyManagerPolicy: none 
EOF
oc apply -f cpumanager-kubeletconfig.yaml

# delete them all
oc delete -f cpumanager-kubeletconfig.yaml

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-43-network-policy-demo"><a class="header" href="#openshift-43-network-policy-demo">openshift 4.3 network policy demo</a></h1>
<p>https://docs.openshift.com/container-platform/4.3/networking/configuring-networkpolicy.html</p>
<p>video</p>
<ul>
<li>https://youtu.be/pbV2VwIExVg</li>
<li>https://www.bilibili.com/video/BV1vz411B7pC/</li>
</ul>
<pre><code class="language-bash">
# 为zxcdn namespace，和demo namespace配置network policy，只放行CDN内部应用和ingress的流量，外部应用流量一律拒绝。
cat &lt;&lt; EOF &gt; demo.yaml
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-other-namespaces
spec:
  podSelector: null
  ingress:
    - from:
        - podSelector: {}
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-openshift-ingress
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          network.openshift.io/policy-group: ingress
  podSelector: {}
  policyTypes:
  - Ingress
EOF
oc apply -n zxcdn -f demo.yaml
oc apply -n demo -f demo.yaml

# 在 demo 和 zxcdn 空间中，各创建一个测试用的pod
cat &lt;&lt; EOF &gt; demo.yaml
---
kind: Deployment
apiVersion: apps/v1
metadata:
  annotations:
  name: demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo
  template:
    metadata:
      labels:
        app: demo  
    spec:
      nodeSelector:
        kubernetes.io/hostname: 'infra1.hsc.redhat.ren'
      restartPolicy: Always
      containers:
        - name: demo1
          image: &gt;- 
            registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
          env:
            - name: key
              value: value
          command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot; ]
          args: [ &quot;trap : TERM INT; sleep infinity &amp; wait&quot; ]
          imagePullPolicy: Always

EOF
oc apply -n demo -f demo.yaml
oc apply -n zxcdn -f demo.yaml

# 查找cdn的ip地址
oc get pod -o wide -n zxcdn

# 进入demo pod，ping cdn pod，应该ping不通

# 配置zxcdn namespace的network policy，放行demo namespace
oc label namespace demo name=demo

cat &lt;&lt; EOF &gt; demo.yaml
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-other-namespaces
spec:
  podSelector: null
  ingress:
    - from:
        - podSelector: {}
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-openshift-ingress
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          network.openshift.io/policy-group: ingress
  podSelector: {}
  policyTypes:
  - Ingress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-other
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: demo
  podSelector: {}
  policyTypes:
  - Ingress
EOF
oc apply -n zxcdn -f demo.yaml

# 进入demo pod，ping cdn pod，应该可以ping通


# 进入zxcdn project里面的一个pod, ping demo pod，应该ping不通
oc get pod -n demo -o wide

# 配置 demo namespace的network policy， 放行 zxcdn namespace
oc label namespace zxcdn name=zxcdn

cat &lt;&lt; EOF &gt; demo.yaml
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-other-namespaces
spec:
  podSelector: null
  ingress:
    - from:
        - podSelector: {}
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-openshift-ingress
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          network.openshift.io/policy-group: ingress
  podSelector: {}
  policyTypes:
  - Ingress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-other
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: zxcdn
  podSelector: {}
  policyTypes:
  - Ingress
EOF
oc apply -n demo -f demo.yaml

# 进入zxcdn project里面的一个pod, ping demo pod，应该能够ping通



oc delete -n zxcdn -f demo.yaml
oc delete -n demo -f demo.yaml


</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-43-multicast"><a class="header" href="#openshift-43-multicast">openshift 4.3 multicast</a></h1>
<p>本文测试 openshift pod 之间组播的功能</p>
<p>video:</p>
<ul>
<li>https://youtu.be/4UriNYHRbHk</li>
<li>https://www.bilibili.com/video/BV1wk4y1k7sS/</li>
</ul>
<p>参考资料：</p>
<p>https://docs.openshift.com/container-platform/4.3/networking/openshift_sdn/using-multicast.html</p>
<p>https://pktgen-dpdk.readthedocs.io/en/latest/getting_started.html</p>
<p>https://access.redhat.com/solutions/406553</p>
<p>https://wenku.baidu.com/view/9a7c3c3dbdd126fff705cc1755270722182e5943.html?rec_flag=default</p>
<pre><code class="language-bash"># 在相应的 project 上激活组播功能
oc annotate netnamespace demo \
    netnamespace.network.openshift.io/multicast-enabled=true

# 创建两个组播服务端pod，再创建一个组播测试pod
cat &lt;&lt; EOF &gt; demo.yaml
---
kind: Pod
apiVersion: v1
metadata:
  name: demo1
spec:
  nodeSelector:
    kubernetes.io/hostname: 'infra1.hsc.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo1
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;iperf&quot;, &quot;-s&quot;, &quot;-u &quot;,&quot;-B&quot;, &quot;224.0.0.1&quot;, &quot;-p&quot; ]
      args: [ &quot;6666&quot; ]
      imagePullPolicy: Always
---
kind: Pod
apiVersion: v1
metadata:
  name: demo2
spec:
  nodeSelector:
    kubernetes.io/hostname: 'infra1.hsc.redhat.ren'
  restartPolicy: Always
  containers:
    - name: demo1
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;iperf&quot;, &quot;-s&quot;, &quot;-u &quot;,&quot;-B&quot;, &quot;224.0.0.1&quot;, &quot;-p&quot; ]
      args: [ &quot;6666&quot; ]
      imagePullPolicy: Always
---
kind: Pod
apiVersion: v1
metadata:
  name: iperf
spec:
  nodeSelector:
    kubernetes.io/hostname: 'infra0.hsc.redhat.ren'
  restartPolicy: Always
  containers:
    - name: iperf
      image: &gt;- 
        registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
      env:
        - name: key
          value: value
      command: [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot; ]
      args: [ &quot;trap : TERM INT; sleep infinity &amp; wait&quot; ]
      imagePullPolicy: Always      
EOF
oc apply -n demo -f demo.yaml

oc project demo

# 查看 pod 运行正常，pod 分布正常
oc get pod -o wide

# 查看组播服务 pod demo1 的组播地址
oc exec -it demo1 -- ipmaddr show dev eth0
# 3:      eth0
#         link  33:33:00:00:00:01
#         link  01:00:5e:00:00:01
#         link  33:33:ff:07:a8:2e
#         inet  224.0.0.1
#         inet6 ff02::1:ff07:a82e
#         inet6 ff02::1
#         inet6 ff01::1

# 查看组播服务 pod demo2 的组播地址
oc exec -it demo2 -- ipmaddr show dev eth0
# 3:      eth0
#         link  33:33:00:00:00:01
#         link  01:00:5e:00:00:01
#         link  33:33:ff:5c:ba:66
#         inet  224.0.0.1
#         inet6 ff02::1:ff5c:ba66
#         inet6 ff02::1
#         inet6 ff01::1

# 在测试 pod iperf 上，创建目标是 224.0.0.1 的组播流量
oc exec -it iperf -- iperf -c 224.0.0.1 -u -p 6666 -t 30 -i 1

# 在服务端 pod demo1 上，监听端口，能看到目标 224.0.0.1 的组播流量
oc exec -it demo1 -- tcpdump -i eth0 -nn
# 在服务端 pod demo2 上，监听端口，能看到目标 224.0.0.1 的组播流量
oc exec -it demo2 -- tcpdump -i eth0 -nn

# 在测试 pod iperf 上，创建目标是 225.0.0.2 的组播流量
oc exec -it iperf -- iperf -c 225.0.0.2 -u -p 6666 -t 30 -i 1

# 在服务端 pod demo1 上，监听端口，能看到目标 225.0.0.2 的组播流量
oc exec -it demo1 -- tcpdump -i eth0 -nn
# 在服务端 pod demo2 上，监听端口，能看到目标 225.0.0.2 的组播流量
oc exec -it demo2 -- tcpdump -i eth0 -nn


# 恢复环境
oc delete -f demo.yaml

</code></pre>
<p>pkgen</p>
<pre><code class="language-bash">oc annotate netnamespace demo \
    netnamespace.network.openshift.io/multicast-enabled=true

# do below before create pod
modprobe pktgen

ps aux | grep pktgen

ls /proc/net/pktgen/

# create pod
oc project demo
oc get sa
oc create serviceaccount -n demo demo-app
oc adm policy add-scc-to-user privileged -z demo-app

cat &lt;&lt; EOF &gt; demo1.yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  annotations:
  name: demo1
  namespace: demo
  labels:
    app: demo1
spec:
  replicas: 2
  selector:
    matchLabels:
      app: demo1
  template:
    metadata:
      labels:
        app: demo1
    spec:
      nodeSelector:
        kubernetes.io/hostname: 'worker-0'
      restartPolicy: Always
      containers:
        - name: demo1
          image: &gt;-
            registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
          env:
            - name: key
              value: value
          command: [ &quot;/bin/bash&quot;, &quot;-c&quot;, &quot;--&quot; ]
          args: [ &quot;while true; do sleep 300000; done;&quot; ]
          imagePullPolicy: Always
          securityContext:
            privileged: true
      serviceAccount: demo-app
EOF
oc apply -f demo1.yaml

ipmaddr show dev eth0
# 3:      eth0
#         link  33:33:00:00:00:01
#         link  01:00:5e:00:00:01
#         link  33:33:ff:ff:9d:55
#         inet  224.0.0.1
#         inet6 ff02::1:ffff:9d55
#         inet6 ff02::1
#         inet6 ff01::1

export IF=if581

echo &quot;rem_device_all&quot; &gt; /proc/net/pktgen/kpktgend_0
echo &quot;add_device eth0@${IF}&quot; &gt; /proc/net/pktgen/kpktgend_0
echo &quot;max_before_softirq 100000&quot; &gt; /proc/net/pktgen/kpktgend_0

echo &quot;count 100&quot; &gt; /proc/net/pktgen/eth0@${IF}
echo &quot;clone_skb 1000000&quot; &gt; /proc/net/pktgen/eth0@${IF}
echo &quot;pkt_size 1300&quot; &gt; /proc/net/pktgen/eth0@${IF}
echo &quot;delay 0&quot; &gt; /proc/net/pktgen/eth0@${IF}
echo &quot;dst 224.0.0.2&quot; &gt; /proc/net/pktgen/eth0@${IF}
echo &quot;dst_mac 01:00:5e:00:00:02&quot; &gt; /proc/net/pktgen/eth0@${IF}

echo start &gt; /proc/net/pktgen/pgctrl

cat /proc/net/pktgen/eth0@${IF}

# oc rsh &lt;another pod&gt;
tcpdump -i eth0 -nn

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-43-firewall"><a class="header" href="#openshift-43-firewall">openshift 4.3 firewall</a></h1>
<p>本文记录，如何在openshift集群主机上应用防火墙。这对于客户有内部扫描审计来说，很有用。</p>
<p>做法很简单，就是调用systemd来注入一个新服务，启动本地定制化脚本。</p>
<p>这种做法可以用来做任何你想在coreos瞎搞的事情：）</p>
<h2 id="coreos"><a class="header" href="#coreos">coreos</a></h2>
<p>对于coreos，特别是master。</p>
<pre><code class="language-bash">
cat &lt;&lt; EOF &gt; wzh.script
#!/bin/bash

iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
iptables -A INPUT -s 127.0.0.1/32 -j ACCEPT
iptables -A INPUT -s 223.87.20.0/24 -j ACCEPT
iptables -A INPUT -s 117.177.241.0/24 -j ACCEPT
iptables -A INPUT -s 39.134.200.0/24 -j ACCEPT
iptables -A INPUT -s 192.168.7.0/24 -j ACCEPT
iptables -A INPUT -s 112.44.102.224/27 -j ACCEPT
iptables -A INPUT -s 47.93.86.113/32 -j ACCEPT
iptables -A INPUT -p tcp -j REJECT
iptables -A INPUT -p udp -j REJECT

EOF

var_local=$(cat ./wzh.script | python3 -c &quot;import sys, urllib.parse; print(urllib.parse.quote(''.join(sys.stdin.readlines())))&quot;  )

cat &lt;&lt;EOF &gt; 45-master-wzh-service.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 45-master-wzh-service
spec:
  config:
    ignition:
      version: 2.2.0
    storage:
      files:
      - contents:
          source: data:text/plain,${var_local}
          verification: {}
        filesystem: root
        mode: 0755
        path: /etc/rc.d/wzh.local
    systemd:
      units:
      - name: wzh.service
        enabled: true
        contents: |
          [Unit]
          Description=/etc/rc.d/wzh.local Compatibility
          Documentation=zhengwan@redhat.com
          ConditionFileIsExecutable=/etc/rc.d/wzh.local
          After=network.target

          [Service]
          Type=oneshot
          User=root
          Group=root
          ExecStart=/bin/bash -c /etc/rc.d/wzh.local

          [Install]
          WantedBy=multi-user.target

EOF
oc apply -f 45-master-wzh-service.yaml -n openshift-config

oc delete -f 45-wzh-service.yaml -n openshift-config

</code></pre>
<h2 id="for-rhel-with-firewalld"><a class="header" href="#for-rhel-with-firewalld">for rhel with firewalld</a></h2>
<p>https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/security_guide/sec-setting_and_controlling_ip_sets_using_firewalld</p>
<p>https://unix.stackexchange.com/questions/159873/whitelist-source-ip-addresses-in-centos-7</p>
<pre><code class="language-bash">
firewall-cmd --get-ipset-types
firewall-cmd --permanent --get-ipsets

firewall-cmd --permanent --new-ipset=my-allow-list --type=hash:net
firewall-cmd --permanent --get-ipsets

# firewall-cmd --permanent --info-ipset=my-allow-list

cat &gt; /root/ocp4/iplist.txt &lt;&lt;EOL
127.0.0.1/32
223.87.20.0/24
117.177.241.0/24
39.134.200.0/24
39.134.201.0/24
39.137.101.0/24
192.168.7.0/24
112.44.102.224/27
47.93.86.113/32
EOL

firewall-cmd --permanent --ipset=my-allow-list --add-entries-from-file=iplist.txt

firewall-cmd --permanent --ipset=my-allow-list --get-entries

firewall-cmd --permanent --zone=trusted --add-source=ipset:my-allow-list 
firewall-cmd --reload

firewall-cmd --list-all

# firewall-cmd --permanent --zone=trusted --add-source=192.168.7.0/24
firewall-cmd --get-active-zones
# firewall-cmd --zone=block --change-interface=em1

firewall-cmd --set-default-zone=block
firewall-cmd --runtime-to-permanent
firewall-cmd --reload

firewall-cmd --list-all-zones

firewall-cmd --get-default-zone

</code></pre>
<h2 id="for-rhel-with-iptables"><a class="header" href="#for-rhel-with-iptables">for rhel with iptables</a></h2>
<p>https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/security_guide/sec-setting_and_controlling_ip_sets_using_iptables</p>
<pre><code class="language-bash">
# secure for anti-scan
cat &lt;&lt; EOF &gt;&gt; /etc/rc.local

ipset create my-allow-set hash:net
ipset add my-allow-set 127.0.0.1/32
ipset add my-allow-set 223.87.20.0/24
ipset add my-allow-set 117.177.241.0/24
ipset add my-allow-set 39.134.200.0/24
ipset add my-allow-set 39.134.201.0/24
ipset add my-allow-set 39.137.101.0/24
ipset add my-allow-set 192.168.7.0/24
ipset add my-allow-set 112.44.102.224/27
ipset add my-allow-set 47.93.86.113/32

iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
iptables -A INPUT -m set --match-set my-allow-set src -j ACCEPT
iptables -A INPUT -p tcp -j REJECT
iptables -A INPUT -p udp -j REJECT

EOF

chmod +x /etc/rc.d/rc.local
systemctl enable rc-local

# systemctl start rc-local

ipset list

# 221.226.0.75
# 210.21.236.182
# 61.132.54.2
ipset add my-allow-set 221.226.0.75/32
ipset add my-allow-set 210.21.236.182/32
ipset add my-allow-set 61.132.54.2/32

</code></pre>
<h2 id="other-record"><a class="header" href="#other-record">other record</a></h2>
<pre><code class="language-bash">
# https://bugzilla.redhat.com/show_bug.cgi?id=1723327
# https://access.redhat.com/solutions/4264181
for i in $(oc get pods -n openshift-machine-config-operator -l k8s-app=machine-config-daemon -o go-template --template '{{range .items}}{{.metadata.name}}{{&quot;\n&quot;}}{{end}}' | xargs); do oc rsh -n openshift-machine-config-operator $i chroot /rootfs rm -rf /run/pivot/reboot-needed; done

rpm-ostree rollback --reboot

cat &lt;&lt; EOF &gt; wzh.service
[Unit]
Description=/etc/rc.d/wzh.local Compatibility
Documentation=zhengwan@redhat.com
ConditionFileIsExecutable=/etc/rc.d/wzh.local
After=network.target

[Service]
Type=oneshot
User=root
Group=root
ExecStart=/bin/bash -c /etc/rc.d/wzh.local

[Install]
WantedBy=multi-user.target
EOF

var_service=$(cat ./wzh.service | python3 -c &quot;import sys, urllib.parse; print(urllib.parse.quote(''.join(sys.stdin.readlines())))&quot;  )


</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-43-using-ldap"><a class="header" href="#openshift-43-using-ldap">openshift 4.3 using ldap</a></h1>
<p>演示场景如下</p>
<ul>
<li>部署openlap，并部署web前端</li>
<li>在openlap上配置2个group，一个是admins，一个是users，并给每个group配置一个user</li>
<li>ocp上配置ldap方式的用户认证</li>
<li>在ocp上使用命令行，同步ldap，查看已经生成了group和user</li>
<li>用这个用户登录ocp，发现什么都干不了</li>
<li>在ocp上使用命令行，给admins group授予cluster view的权限，给users group授予demo project view的权限。</li>
<li>重新登录/刷新页面，可以看到admin用户可以看到整个集群的内容，users的用户有了demo project的权限。</li>
</ul>
<p>video</p>
<ul>
<li>https://youtu.be/Sg3euS3ip4k</li>
<li>https://www.bilibili.com/video/BV1XA411b7N6/</li>
</ul>
<p>参考资料：</p>
<ul>
<li>https://docs.openshift.com/container-platform/4.3/authentication/identity_providers/configuring-ldap-identity-provider.html</li>
<li>https://docs.openshift.com/container-platform/4.3/authentication/ldap-syncing.html</li>
<li>https://www.cnblogs.com/ericnie/p/10063816.html</li>
<li>https://access.redhat.com/solutions/2484371</li>
<li>https://access.redhat.com/solutions/3419841</li>
</ul>
<h2 id="openldap"><a class="header" href="#openldap">openldap</a></h2>
<pre><code class="language-bash">
skopeo copy docker://docker.io/osixia/openldap:latest docker://registry.redhat.ren:5443/docker.io/osixia/openldap:latest

skopeo copy docker://docker.io/osixia/phpldapadmin:latest docker://registry.redhat.ren:5443/docker.io/osixia/phpldapadmin:latest

# 启动openldap服务
podman run -p 389:389 --name openldap --hostname ldap.redhat.ren --env LDAP_ORGANISATION=&quot;redhat&quot; --env LDAP_DOMAIN=&quot;redhat.ren&quot; --env LDAP_ADMIN_PASSWORD=&quot;ldap123&quot; --detach registry.redhat.ren:5443/docker.io/osixia/openldap:latest
# 默认登录用户名：admin

podman run -d -p 5080:80 --name phpldapadmin --env PHPLDAPADMIN_HTTPS=false --env PHPLDAPADMIN_LDAP_HOSTS=117.177.241.16 --detach registry.redhat.ren:5443/docker.io/osixia/phpldapadmin:latest
# http://helper.hsc.redhat.ren:5080
# Login DN： cn=admin,dc=redhat,dc=ren
# Password： ldap123

podman rm -fv phpldapadmin
podman rm -fv openldap

yum install -y openldap openldap-clients openldap-servers

systemctl status slapd

# 为ldap添加测试用户数据
cat &lt;&lt; EOF &gt; base.ldif
dn: ou=users,dc=redhat,dc=ren
objectClass: organizationalUnit
objectClass: top
ou: users

dn: ou=groups,dc=redhat,dc=ren
objectClass: organizationalUnit
objectClass: top
ou: groups  
EOF

ldapadd -x -D &quot;cn=admin,dc=redhat,dc=ren&quot; -w ldap123 -f base.ldif

# 创建用户密码
slappasswd -s redhat
# {SSHA}yiR9306gQWh4mdeOuJ1KUg5cxQ8uoWKK

cat &lt;&lt; EOF &gt;users.ldif 
dn: cn=ocpadm,ou=users,dc=redhat,dc=ren
objectClass: person
objectClass: organizationalPerson
objectClass: inetOrgPerson
cn: ocpadm
sn: ocpadm
uid: ocpadm
displayName: ocpadm
mail: ocpadm@redhat.ren
userPassword: {SSHA}yiR9306gQWh4mdeOuJ1KUg5cxQ8uoWKK

dn: cn=wzh,ou=users,dc=redhat,dc=ren
objectClass: person
objectClass: organizationalPerson
objectClass: inetOrgPerson
cn: wzh
sn: wzh
uid: wzh
displayName: wzh
mail: wzh@redhat.ren
userPassword: {SSHA}yiR9306gQWh4mdeOuJ1KUg5cxQ8uoWKK

dn: cn=admins,ou=groups,dc=redhat,dc=ren
objectClass: groupOfNames
cn: admins
owner: cn=admin,dc=redhat,dc=ren
member: cn=ocpadm,ou=users,dc=redhat,dc=ren

dn: cn=normals,ou=groups,dc=redhat,dc=ren
objectClass: groupOfNames
cn: normals
owner: cn=admin,dc=redhat,dc=ren
member: cn=wzh,ou=users,dc=redhat,dc=ren

EOF
ldapadd -x -D &quot;cn=admin,dc=redhat,dc=ren&quot; -w ldap123 -f users.ldif 

ldapsearch -x -D &quot;cn=admin,dc=redhat,dc=ren&quot; -w ldap123 -b dc=redhat,dc=ren 
</code></pre>
<h2 id="ocp-operation"><a class="header" href="#ocp-operation">ocp operation</a></h2>
<pre><code class="language-bash">oc get user
oc get group
oc get identity

# cleanup 垃圾用户数据
oc get user | grep ldap | awk '{print $1}' | xargs -I DEMO oc delete user DEMO
oc get identity | grep ldap | awk '{print $1}' | xargs -I DEMO oc delete identity DEMO

# 创建登录密码
oc create secret generic ldap-secret --from-literal=bindPassword=ldap123 -n openshift-config

# 创建ldap登录入口
cat &lt;&lt; EOF &gt; ldap.yaml
apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  name: cluster
spec:
  identityProviders:
  - name: &quot;Local Password&quot;
    mappingMethod: claim
    type: HTPasswd
    htpasswd:
      fileData:
        name: htpasswd
  - name: ldapidp 
    mappingMethod: claim 
    type: LDAP
    ldap:
      attributes:
        id: 
        - dn
        email: 
        - mail
        name: 
        - cn
        preferredUsername: 
        - uid
      bindDN: &quot;cn=admin,dc=redhat,dc=ren&quot;
      bindPassword: 
        name: ldap-secret
      insecure: true 
      url: &quot;ldap://registry.redhat.ren:389/ou=users,dc=redhat,dc=ren?uid&quot; 
EOF
oc apply -f ldap.yaml

# 从ldap同步group数据
cat &lt;&lt; EOF &gt; ldapsync.yaml
kind: LDAPSyncConfig
apiVersion: v1
url: ldap://registry.redhat.ren:389
insecure: true
bindDN: cn=admin,dc=redhat,dc=ren
bindPassword: ldap123 
groupUIDNameMapping:
  &quot;cn=admins,ou=groups,dc=redhat,dc=ren&quot;: Administrators 
  &quot;cn=normals,ou=groups,dc=redhat,dc=ren&quot;: NormalUsers 
rfc2307:
    groupsQuery:
        baseDN: &quot;ou=groups,dc=redhat,dc=ren&quot;
        scope: sub
        derefAliases: never
        pageSize: 0
        filter: (objectclass=groupOfNames)
    groupUIDAttribute: dn 
    groupNameAttributes: [ cn ] 
    groupMembershipAttributes: [ member ]
    usersQuery:
        baseDN: &quot;ou=users,dc=redhat,dc=ren&quot;
        scope: sub
        derefAliases: never
        pageSize: 0
    userUIDAttribute: dn 
    userNameAttributes: [ cn ]
    tolerateMemberNotFoundErrors: false
    tolerateMemberOutOfScopeErrors: false
EOF

oc adm groups sync --sync-config=ldapsync.yaml --confirm

# 删除ldap上已经删除的用户组
# oc adm prune groups --sync-config=ldapsync.yaml --confirm

# 在这个时候，可以用wzh/ocpadm登录系统，但是可以看到没有任何project的权限

# 准备为用户组赋权
oc get clusterrole
oc get role 

# 赋予admin和normal组不同的权限
oc adm policy add-cluster-role-to-group cluster-reader Administrators
oc policy add-role-to-group view NormalUsers -n demo 

# 再次登录系统，可以看到用户有了相应的权限

# 撤销用户组权限
oc adm policy remove-cluster-role-from-group cluster-reader Administrators
oc policy remove-role-from-group view NormalUsers -n demo 

# remove ldap 
# cleanup 垃圾用户数据
oc get user | grep ldap | awk '{print $1}' | xargs -I DEMO oc delete user DEMO
oc get identity | grep ldap | awk '{print $1}' | xargs -I DEMO oc delete identity DEMO

cat &lt;&lt; EOF &gt; ldap.yaml
apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  name: cluster
spec:
  identityProviders:
  - name: &quot;Local Password&quot;
    mappingMethod: claim
    type: HTPasswd
    htpasswd:
      fileData:
        name: htpasswd
EOF
oc apply -f ldap.yaml

</code></pre>
<h2 id="free-ipa"><a class="header" href="#free-ipa">free ipa</a></h2>
<pre><code class="language-bash">skopeo copy docker://docker.io/freeipa/freeipa-server:latest docker://registry.redhat.ren:5443/docker.io/freeipa/freeipa-server:latest

mkdir -p /data/freeipa
cat &lt;&lt; EOF &gt; /data/freeipa/ipa-server-install-options
--realm=redhat.ren
--ds-password=The-directory-server-password
--admin-password=The-admin-password
EOF

# setsebool -P container_manage_cgroup 1

docker run --name freeipa-server-container -ti --privileged   \
    -e IPA_SERVER_IP=10.66.208.240 \
    -p 3080:80 -p 3443:443 -p 389:389 -p 636:636 -p 88:88 -p 464:464 \
    -p 88:88/udp -p 464:464/udp -p 123:123/udp \
   -h ipa.redhat.ren \
   -v /sys/fs/cgroup:/sys/fs/cgroup:ro \
   --tmpfs /run --tmpfs /tmp \
   -v /data/freeipa:/data:Z \
   docker.io/freeipa/freeipa-server ipa-server-install

docker start -ai freeipa-server-container

docker rm -fv $(docker ps -qa)

firewall-cmd --zone=public --add-port=3443/tcp --permanent
firewall-cmd --reload

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="image-pull-secret"><a class="header" href="#image-pull-secret">image pull secret</a></h1>
<p>https://docs.openshift.com/container-platform/4.3/openshift_images/managing_images/using-image-pull-secrets.html</p>
<p>https://docs.openshift.com/container-platform/4.3/installing/install_config/installing-restricted-networks-preparations.html</p>
<pre><code class="language-bash"># accross projects
oc policy add-role-to-user \
    system:image-puller system:serviceaccount:project-a:default \
    --namespace=project-b
oc policy add-role-to-group \
    system:image-puller system:serviceaccounts:project-a \
    --namespace=project-b

# ref outside
oc create secret generic &lt;pull_secret_name&gt; \
    --from-file=.dockercfg=&lt;path/to/.dockercfg&gt; \
    --type=kubernetes.io/dockercfg
oc create secret generic &lt;pull_secret_name&gt; \
    --from-file=.dockerconfigjson=&lt;path/to/.docker/config.json&gt; \
    --type=kubernetes.io/dockerconfigjson
oc create secret docker-registry &lt;pull_secret_name&gt; \
    --docker-server=&lt;registry_server&gt; \
    --docker-username=&lt;user_name&gt; \
    --docker-password=&lt;password&gt; \
    --docker-email=&lt;email&gt;
oc secrets link default &lt;pull_secret_name&gt; --for=pull
oc secrets link builder &lt;pull_secret_name&gt;


# global
oc get secret/pull-secret -n openshift-config -o yaml

oc get secret/pull-secret -n openshift-config -o json | jq -r '.data.&quot;.dockerconfigjson&quot;' | base64 -d

oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=&lt;pull-secret-location&gt; 

cat ./pull-secret.text | jq .  &gt; &lt;path&gt;/&lt;pull-secret-file&gt;

# &lt;credentials&gt;
echo -n '&lt;user_name&gt;:&lt;password&gt;' | base64 -w0 
#   &quot;auths&quot;: {
# ...
#     &quot;&lt;local_registry_host_name&gt;:&lt;local_registry_host_port&gt;&quot;: { 
#       &quot;auth&quot;: &quot;&lt;credentials&gt;&quot;, 
#       &quot;email&quot;: &quot;you@example.com&quot;
#   },
# ...



</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-43-huge-page"><a class="header" href="#openshift-43-huge-page">openshift 4.3 huge page</a></h1>
<p>video</p>
<ul>
<li>https://youtu.be/T7R-j0B9eSY</li>
<li>https://www.bilibili.com/video/BV1De411W7JU/</li>
</ul>
<p>https://docs.openshift.com/container-platform/4.3/scalability_and_performance/what-huge-pages-do-and-how-they-are-consumed-by-apps.html</p>
<p>https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-red_hat_enterprise_linux-performance_tuning_guide-configuring_transparent_huge_pages</p>
<pre><code class="language-bash"># check original status
cat /sys/kernel/mm/transparent_hugepage/enabled
# [always] madvise never

cat /sys/kernel/mm/transparent_hugepage/defrag
# [always] madvise never

# begin to test 
oc label node infra1.hsc.redhat.ren hugepages=true

cat &lt;&lt; EOF &gt; hugepages_tuning.yaml
apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: hugepages 
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile: 
  - data: |
      [main]
      summary=Configuration for hugepages
      include=openshift-node

      [vm]
      transparent_hugepages=never

      [sysctl]
      vm.nr_hugepages=1024
    name: node-hugepages
  recommend:
  - match: 
    - label: hugepages
    priority: 30
    profile: node-hugepages
EOF

oc create -f hugepages_tuning.yaml

oc get pod -o wide -n openshift-cluster-node-tuning-operator

oc logs tuned-86g8b \
    -n openshift-cluster-node-tuning-operator | grep 'applied$' | tail -n1

# check result
cat /sys/kernel/mm/transparent_hugepage/enabled
# always madvise [never]

cat /sys/kernel/mm/transparent_hugepage/defrag
# [always] madvise never

# node feature discovery 功能已经触发了profile自动选择。

cat &lt;&lt; EOF &gt; hugepages-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  generateName: hugepages-volume-
spec:
  containers:
  - securityContext:
      privileged: true
    image: registry.redhat.ren:5443/docker.io/wangzheng422/centos:centos7-test
    imagePullPolicy: Always
    command:
    - sleep
    - inf
    name: example
    volumeMounts:
    - mountPath: /dev/hugepages
      name: hugepage
    resources:
      limits:
        hugepages-2Mi: 100Mi 
        memory: &quot;1Gi&quot;
        cpu: &quot;1&quot;
  volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages
EOF
oc create -n demo -f hugepages-pod.yaml

# login into pod
oc rsh hugepages-volume-9nwlv

mount | grep page
# nodev on /dev/hugepages type hugetlbfs (rw,relatime,seclabel,pagesize=2Mi)

# 来看看系统huge page的状态
# yum install libhugetlbfs-utils
hugeadm --explain

# 根据以下的2个帖子，hugepage是给程序分配内存用的，不能用文件操作演示
# https://serverfault.com/questions/811670/how-to-create-copy-a-file-into-hugetlbfs
# https://stackoverflow.com/questions/40285971/how-to-load-text-segments-of-shared-libraries-into-huge-pages-on-linux]

# sysbench memory --memory-hugetlb=on --memory-total-size=200M run

# restore
oc delete -f hugepages_tuning.yaml
# reboot

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-43-helm"><a class="header" href="#openshift-43-helm">openshift 4.3 helm</a></h1>
<p>本文讲述，如何在openshift 4.3 上演示helm功能</p>
<p>video</p>
<ul>
<li>https://youtu.be/L6ioq_JMOtE</li>
<li>https://www.bilibili.com/video/BV1qp4y197yH/</li>
</ul>
<p>参考资料：</p>
<p>https://docs.openshift.com/container-platform/4.3/cli_reference/helm_cli/getting-started-with-helm-on-openshift-container-platform.html</p>
<p>https://chartmuseum.com/docs/#installing-chartsinto-kubernetes</p>
<p>https://whmzsu.github.io/helm-doc-zh-cn/chart/chart_repository-zh_cn.html</p>
<h2 id="操作步骤"><a class="header" href="#操作步骤">操作步骤</a></h2>
<pre><code class="language-bash"># 环境准备
skopeo copy docker://docker.io/gogs/gogs docker://registry.redhat.ren:5443/docker.io/gogs/gogs

skopeo copy docker://docker.io/chartmuseum/chartmuseum:latest docker://registry.redhat.ren:5443/docker.io/chartmuseum/chartmuseum:latest

skopeo copy docker://docker.io/ananwaresystems/webarchive:1.0 docker://registry.redhat.ren:5443/docker.io/ananwaresystems/webarchive:1.0

skopeo copy docker://docker.io/tomcat:7.0 docker://registry.redhat.ren:5443/docker.io/tomcat:7.0 

# https://github.com/helm/charts/tree/master/stable/chartmuseum

# 运行一个helm chart repository
mkdir -p /data/ocp4/helm/charts

podman run --rm -it \
  -p 18080:8080 \
  -v /data/ocp4/helm/charts:/charts:Z \
  -e DEBUG=true \
  -e STORAGE=local \
  -e STORAGE_LOCAL_ROOTDIR=/charts \
  --privileged \
  registry.redhat.ren:5443/docker.io/chartmuseum/chartmuseum:latest

# 准备 helm 客户端
curl -L https://mirror.openshift.com/pub/openshift-v4/clients/helm/latest/helm-linux-amd64 -o /usr/local/bin/helm

chmod +x /usr/local/bin/helm

helm version

helm repo add chartmuseum http://localhost:18080
helm repo list

# 编译一个helm chart, 并上传 chart repository
cd /data/ocp4/helm/tomcat
helm lint
helm package .
curl --data-binary &quot;@tomcat-0.4.1.tgz&quot; http://localhost:18080/api/charts
helm repo update
helm search repo

# 通过 helm chart 创建 tomcat deploy
oc project demo
helm install example-tomcat chartmuseum/tomcat
helm list

# 恢复环境
helm uninstall example-tomcat
helm repo remove chartmuseum

/bin/rm -f /data/ocp4/helm/charts/*

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-tcp-router"><a class="header" href="#openshift-tcp-router">openshift tcp-router</a></h1>
<p>本文描述，如何通过定制化haproxy template, 通过给route添加annotation，就可以向外界开放tcp路由。本文相关脚本和文件，在scripts目录中。</p>
<h2 id="初衷和原理"><a class="header" href="#初衷和原理">初衷和原理</a></h2>
<p>经常会在openshift的poc中遇到L4负载均衡的测试，我们知道默认ocp router是haproxy做的，而且默认只支持http, https, 虽然tls/sni 也算是支持tcp的一种方式，但是这个也还是7层的。官方文档只是简单的说，如果有其他的需求，就定制haproxy template来满足，不过定制说的很少，例子也不多。本文就是一个通过定制化haproxy template，来达到动态监听route配置，并动态开放tcp端口。</p>
<p>定制haproxy template需要了解openshift router的一些原理要点</p>
<ul>
<li>openshift router不仅仅是haproxy，它还有一个go程序，监听了openshift的配置，并且写入了一堆的map文件，这个文件是非常关键的配置haproxy template的配置文件。</li>
<li>openshift router里面的tls passthrough方式，对应到haproxy的配置里面，就是tcp的模式，我们的定制点就是在这里。</li>
<li>定制过程集中在，屏蔽掉http/https 的edge和reencrypt部分，对于打annotation 的route，开放tls passthrough的frontend</li>
<li>route annotation 配置形式是 haproxy.router.openshift.io/external-tcp-port: &quot;13306&quot;</li>
<li>当然，ocp4现在还不支持定制化route template，所以本文直接创建了一个route的deployment。</li>
<li>现场实施的时候，注意更改router的image，每个版本的image可以去release.txt文件中找到。</li>
</ul>
<p>既然是面向poc的，就肯定有局限</p>
<ul>
<li>route annotation 定义的开放tcp端口，是手动定义，而且面向整个集群各个project开放，必然会导致tcp端口冲突。需要已有端口管理方案，这个就交给GPS吧。</li>
</ul>
<p>以下是route的配置示例</p>
<pre><code class="language-yaml">kind: Route
apiVersion: route.openshift.io/v1
metadata:
  name: ottcache-002
  annotations:
    haproxy.router.openshift.io/wzh-router-name: &quot;wzh-router-1&quot;
    haproxy.router.openshift.io/external-tcp-port: &quot;6620&quot;
spec:
  to:
    kind: Service
    name: ottcache-002-service
  port:
    targetPort: 6620
  tls:
    termination: passthrough
    insecureEdgeTerminationPolicy: None

</code></pre>
<p>以下是template里面，关键的定制点</p>
<pre><code class="language-go">{{/*try to add tcp support*/}}

{{- if eq (env &quot;WZH_ROUTER_NAME&quot; &quot;wzh-router-name&quot;) (index $cfg.Annotations &quot;haproxy.router.openshift.io/wzh-router-name&quot;) }}
  {{- if (isInteger (index $cfg.Annotations &quot;haproxy.router.openshift.io/external-tcp-port&quot;)) }} 
  frontend tcp-{{ (index $cfg.Annotations &quot;haproxy.router.openshift.io/external-tcp-port&quot;) }}
    bind *:{{ (index $cfg.Annotations &quot;haproxy.router.openshift.io/external-tcp-port&quot;) }}
    mode tcp
    default_backend {{genBackendNamePrefix $cfg.TLSTermination}}:{{$cfgIdx}}

  {{- end}}{{/* end haproxy.router.openshift.io */}}
{{- end}}{{/* end WZH_ROUTER_NAME */}}

{{/*end try to add tcp support*/}}

</code></pre>
<h2 id="测试步骤"><a class="header" href="#测试步骤">测试步骤</a></h2>
<p>测试步骤不复杂，就是创建一个新的router，然后就可以去其他project创建应用，给route打annotation就可以了。</p>
<p>本文的例子，包含两个应用，一个是web应用，一个是mysql，都通过tcp端口对外开放。</p>
<pre><code class="language-bash"># tcp-router will install in the same project with openshift router
oc project openshift-ingress

# install the tcp-router and demo
oc create configmap customrouter-wzh --from-file=haproxy-config.template
oc apply -f haproxy.router.yaml

oc apply -f haproxy.demo.yaml

# test your tcp-router, replace ip with router ip, both command will success.
curl 192.168.7.18:18080

podman run -it --rm registry.redhat.ren:5443/docker.io/mysql mysql -h 192.168.7.18 -P 13306 -u user -D db -p

# if you want to delete the tcp-router and demo
oc delete -f haproxy.router.yaml
oc delete configmap customrouter-wzh

oc delete -f haproxy.demo.yaml

# oc set volume deployment/router-wzh --add --overwrite \
#     --name=config-volume \
#     --mount-path=/var/lib/haproxy/conf/custom \
#     --source='{&quot;configMap&quot;: { &quot;name&quot;: &quot;customrouter-wzh&quot;}}'

# oc set env dc/router \
#     TEMPLATE_FILE=/var/lib/haproxy/conf/custom/haproxy-config.template

</code></pre>
<h2 id="参考"><a class="header" href="#参考">参考</a></h2>
<p>https://docs.openshift.com/container-platform/3.11/install_config/router/customized_haproxy_router.html#go-template-actions</p>
<p>https://www.haproxy.com/blog/introduction-to-haproxy-maps/</p>
<p>https://access.redhat.com/solutions/3495011</p>
<p>https://blog.zhaw.ch/icclab/openshift-custom-router-with-tcpsni-support/</p>
<h2 id="以下是弯路-1"><a class="header" href="#以下是弯路-1">以下是弯路</a></h2>
<p>分析源码，我们可以看到，openshift router还是对haproxy做了扩展的，那些map文件，都是router的扩展生成的，目的是对接endpoint，绕过service。所以我们想做tcp转发，可以借助sni-tcp来实现tcp转发。
<img src="ocp4/4.3/imgs/2020-02-23-14-04-49.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-43-grafana"><a class="header" href="#openshift-43-grafana">openshift 4.3 grafana</a></h1>
<p>展示 openshift 4.3 上的 grafana 功能</p>
<p>video</p>
<ul>
<li>https://youtu.be/xGry0_LWFNw</li>
<li>https://www.bilibili.com/video/BV1yV411d7vR/</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cpu-manager"><a class="header" href="#cpu-manager">cpu manager</a></h1>
<p>https://docs.openshift.com/container-platform/4.3/scalability_and_performance/using-cpu-manager.html</p>
<p>video</p>
<ul>
<li>https://youtu.be/gzdb2AURhvo</li>
<li>https://www.bilibili.com/video/BV1Ua4y1t7aQ/</li>
</ul>
<pre><code class="language-bash">oc get node

oc label node ip-10-0-138-181.us-west-2.compute.internal cpumanager=true
oc label node worker-0 cpumanager=true

oc label node infra0.hsc.redhat.ren --overwrite cpumanager=false
oc label node worker-0.ocpsc.redhat.ren --overwrite cpumanager=true
oc label node worker-1.ocpsc.redhat.ren --overwrite cpumanager=true
oc label node worker-2.ocpsc.redhat.ren --overwrite cpumanager=true
oc label node worker-3.ocpsc.redhat.ren --overwrite cpumanager=true

oc get machineconfigpool worker -o yaml

# oc edit machineconfigpool worker
# metadata:
#   creationTimestamp: 2019-xx-xxx
#   generation: 3
#   labels:
#     custom-kubelet: cpumanager-enabled
oc patch machineconfigpool worker -p '{&quot;metadata&quot;:{&quot;labels&quot;: { &quot;custom-kubelet&quot;: &quot;cpumanager-enabled&quot; } } }' --type=merge

cat &lt;&lt; EOF &gt; cpumanager-kubeletconfig.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: cpumanager-enabled
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: cpumanager-enabled
  kubeletConfig:
     cpuManagerPolicy: static
     cpuManagerReconcilePeriod: 6s
EOF
oc apply -f cpumanager-kubeletconfig.yaml

alias urldecode='python3 -c &quot;import sys, urllib.parse as ul; \
    print(ul.unquote_plus(sys.argv[1]))&quot;'

alias urlencode='python3 -c &quot;import sys, urllib.parse as ul; \
    print (ul.quote_plus(sys.argv[1]))&quot;'

worker_mc_kubelet_yaml=$(oc get mc | grep kubelet | grep 99 | awk '{print $1}')

urldecode $(oc get mc ${worker_mc_kubelet_yaml} -o json | jq -r .spec.config.storage.files[0].contents.source | sed &quot;s/data:text\/plain,//g&quot;) | jq

oc debug node/infra0.hsc.redhat.ren
cat /host/etc/kubernetes/kubelet.conf | grep cpuManager

# cat /etc/kubernetes/kubelet.conf | grep cpuManager

cat &lt;&lt; EOF &gt; cpumanager-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  generateName: cpumanager-
spec:
  containers:
  - name: cpumanager
    image: gcr.io/google_containers/pause-amd64:3.0
    resources:
      requests:
        cpu: 1
        memory: &quot;1G&quot;
      limits:
        cpu: 1
        memory: &quot;1G&quot;
  nodeSelector:
    cpumanager: &quot;true&quot;
EOF
oc apply -f cpumanager-pod.yaml

systemctl status
# └─kubepods.slice
#   ├─kubepods-podcc529083_9d0a_43aa_9d9f_1fc0dc3b626b.slice
#   │ ├─crio-conmon-b67ba6af381740b5f9b459482e41a14d4ced2cd8e9431598d84066d20027ef06.scope
#   │ │ └─1434963 /usr/libexec/crio/conmon -s -c b67ba6af381740b5f9b459482e41a14d4ced2cd8e9431598d84066d20027ef06 -n k8s_cpumanager_&gt;            │ ├─crio-conmon-4ab85736504471dcca960aea960ca01ab0fa582439e444d407ac8d001d6dbd2b.scope
#   │ │ └─1434127 /usr/libexec/crio/conmon -s -c 4ab85736504471dcca960aea960ca01ab0fa582439e444d407ac8d001d6dbd2b -n k8s_POD_cpumana&gt;            │ ├─crio-b67ba6af381740b5f9b459482e41a14d4ced2cd8e9431598d84066d20027ef06.scope
#   │ │ └─1434975 /pause
#   │ └─crio-4ab85736504471dcca960aea960ca01ab0fa582439e444d407ac8d001d6dbd2b.scope
#   │   └─1434151 /usr/bin/pod

cd /sys/fs/cgroup/cpuset/kubepods.slice/kubepods-podcc529083_9d0a_43aa_9d9f_1fc0dc3b626b.slice/crio-b67ba6af381740b5f9b459482e41a14d4ced2cd8e9431598d84066d20027ef06.scope

for i in `ls cpuset.cpus tasks` ; do echo -n &quot;$i &quot;; cat $i ; done
# cpuset.cpus 12
# tasks 30894

grep Cpus_allowed_list /proc/1434975/status
# Cpus_allowed_list:      12

systemctl status
# ├─kubepods-burstable.slice
# │ ├─kubepods-burstable-podb8410218_65e9_4ec2_b944_6f0f1709e6a9.slice
# │ │ │ └─6696 /usr/bin/configmap-reload --webhook-url=http://localhost:8080/-/reload --volume-dir=/etc/serving-certs-ca-bundle
# │ │ ├─crio-conmon-958273b72d8d6f1a06a640bd158aa1f5dcc9372b232c79af9f3731068b0bcb9f.scope
# │ │ │ └─6922 /usr/libexec/crio/conmon -s -c 958273b72d8d6f1a06a640bd158aa1f5dcc9372b232c79af9f3731068b0bcb9f -n k8s_kube-rbac-pr&gt;
# │ │ ├─crio-conmon-dc78df658a47a6bcad1772c5f0154c058b3b517f924c842eb9ba2c878edf86a3.scope
# │ │ │ └─6256 /usr/libexec/crio/conmon -s -c dc78df658a47a6bcad1772c5f0154c058b3b517f924c842eb9ba2c878edf86a3 -n k8s_telemeter-cl&gt;
# │ │ ├─crio-958273b72d8d6f1a06a640bd158aa1f5dcc9372b232c79af9f3731068b0bcb9f.scope
# │ │ │ └─6958 /usr/bin/kube-rbac-proxy --secure-listen-address=:8443 --upstream=http://127.0.0.1:8080/ --tls-cert-file=/etc/tls/p&gt;
# │ │ ├─crio-conmon-7a9aaeff818804cb48c6de76ef604e1241717ef25f9d2e31502bca5e03a0a126.scope
# │ │ │ └─5215 /usr/libexec/crio/conmon -s -c 7a9aaeff818804cb48c6de76ef604e1241717ef25f9d2e31502bca5e03a0a126 -n k8s_POD_telemete&gt;
# │ │ ├─crio-dc78df658a47a6bcad1772c5f0154c058b3b517f924c842eb9ba2c878edf86a3.scope
# │ │ │ └─6321 /usr/bin/telemeter-client --id=02b8c3b4-9aed-4268-b1b7-84c998b50184 --from=https://prometheus-k8s.openshift-monitor&gt;
# │ │ ├─crio-conmon-6cefa86b950deb57dac809b57246fb553e0c96fc31ae1cd7b8efa43207995749.scope
# │ │ │ └─6635 /usr/libexec/crio/conmon -s -c 6cefa86b950deb57dac809b57246fb553e0c96fc31ae1cd7b8efa43207995749 -n k8s_reload_telem&gt;
# │ │ └─crio-7a9aaeff818804cb48c6de76ef604e1241717ef25f9d2e31502bca5e03a0a126.scope
# │ │   └─5292 /usr/bin/pod

cat /sys/fs/cgroup/cpuset/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podb8410218_65e9_4ec2_b944_6f0f1709e6a9.slice/crio-dc78df658a47a6bcad1772c5f0154c058b3b517f924c842eb9ba2c878edf86a3.scope/cpuset.cpus
# 0-1,3

oc describe node ip-10-0-138-181.us-west-2.compute.internal

# 可以看到其他pod被限制了使用12号cpu，有一些进程不被限制，是控制进程。
# cd /sys/fs/cgroup/cpuset/kubepods.slice/kubepods-burstable.slice/
cd /sys/fs/cgroup/cpuset/kubepods.slice/kubepods-besteffort.slice
find . -name cpuset.cpus | grep crio | xargs -I DEMO cat DEMO
# 0-11,13-23
# 0-23
# 0-11,13-23
# 0-23
# 0-11,13-23
# 0-23
# 0-11,13-23
# 0-23
# 0-11,13-23
# 0-23
# 0-23


# in pod
cat /sys/fs/cgroup/cpuset/cpuset.cpus
# 0-1,19

cat /proc/1/status | grep -i cpus_allow
# Cpus_allowed:   7fffc
# Cpus_allowed_list:      2-18

oc get pod -A | grep Running | awk '{print $1 &quot;\t&quot; $2}' &gt; list
while read -r pod; do
  echo &quot;$pod&quot;
  oc exec -n $pod -- cat /sys/fs/cgroup/cpuset/cpuset.cpus
  oc exec -n $pod -- cat /proc/1/status | grep -i cpus_allow
done &lt; list

ls /proc | egrep '^[0-9]+$' | xargs -I DEMO echo &quot; grep -s -i name /proc/DEMO/status | tr -d '\n'; echo -n -e '\t'; grep -s -i cpus_allowed_list /proc/DEMO/status ; &quot; | sh
# Name:   systemd Cpus_allowed_list:      0-1
# Name:   ksoftirqd/0     Cpus_allowed_list:      0
# Name:   migration/10    Cpus_allowed_list:      10
# Name:   posixcputmr/10  Cpus_allowed_list:      10
# Name:   rcuc/10 Cpus_allowed_list:      10
# Name:   ksoftirqd/10    Cpus_allowed_list:      10
# Name:   kworker/10:0-mm_percpu_wq       Cpus_allowed_list:      10
# Name:   kworker/10:0H   Cpus_allowed_list:      10
# Name:   rcuop/10        Cpus_allowed_list:      0-1
# Name:   cpuhp/11        Cpus_allowed_list:      11
# Name:   watchdog/11     Cpus_allowed_list:      0-19
# Name:   migration/11    Cpus_allowed_list:      11
# Name:   systemd-journal Cpus_allowed_list:      0-1
# Name:   rcu_preempt     Cpus_allowed_list:      0-1
# Name:   posixcputmr/11  Cpus_allowed_list:      11
# Name:   rcuc/11 Cpus_allowed_list:      11
# Name:   systemd-udevd   Cpus_allowed_list:      0-1
# Name:   ksoftirqd/11    Cpus_allowed_list:      11
# Name:   kworker/11:0-mm_percpu_wq       Cpus_allowed_list:      11
# Name:   irq/149-ioat-ms Cpus_allowed_list:      0
# Name:   irq/151-ioat-ms Cpus_allowed_list:      1
# Name:   irq/152-ioat-ms Cpus_allowed_list:      0
# Name:   kworker/11:0H   Cpus_allowed_list:      11
# Name:   irq/153-ioat-ms Cpus_allowed_list:      1
# Name:   irq/154-ioat-ms Cpus_allowed_list:      0
# Name:   irq/155-ioat-ms Cpus_allowed_list:      1
# Name:   irq/156-ioat-ms Cpus_allowed_list:      1
# Name:   irq/157-mei_me  Cpus_allowed_list:      0
# Name:   irq/158-ioat-ms Cpus_allowed_list:      0
# Name:   irq/16-i801_smb Cpus_allowed_list:      1
# Name:   rcub/2  Cpus_allowed_list:      0-1
# Name:   kipmi0  Cpus_allowed_list:      0-1
# Name:   ib-comp-wq      Cpus_allowed_list:      0-19
# Name:   kworker/u41:0   Cpus_allowed_list:      0-1
# Name:   ib-comp-unb-wq  Cpus_allowed_list:      0-19
# Name:   ib_mcast        Cpus_allowed_list:      0-19
# Name:   rcuop/11        Cpus_allowed_list:      0-1
# Name:   ib_nl_sa_wq     Cpus_allowed_list:      0-19
# Name:   bnxt_re Cpus_allowed_list:      0-19
# Name:   irq/159-bnxt_qp Cpus_allowed_list:      0
# Name:   bnxt_qplib_nq   Cpus_allowed_list:      0-19
# Name:   irq/160-bnxt_qp Cpus_allowed_list:      1
# Name:   ttm_swap        Cpus_allowed_list:      0-19
# Name:   bnxt_qplib_nq   Cpus_allowed_list:      0-19
# Name:   irq/161-bnxt_qp Cpus_allowed_list:      1
# Name:   cpuhp/12        Cpus_allowed_list:      12
# Name:   bnxt_qplib_nq   Cpus_allowed_list:      0-19
# Name:   irq/162-bnxt_qp Cpus_allowed_list:      1
# Name:   bnxt_qplib_nq   Cpus_allowed_list:      0-19
# Name:   irq/163-bnxt_qp Cpus_allowed_list:      1
# Name:   bnxt_qplib_nq   Cpus_allowed_list:      0-19
# Name:   irq/164-bnxt_qp Cpus_allowed_list:      0-1
# Name:   bnxt_qplib_nq   Cpus_allowed_list:      0-19
# Name:   irq/165-bnxt_qp Cpus_allowed_list:      1
# Name:   bnxt_qplib_nq   Cpus_allowed_list:      0-19
# Name:   irq/166-bnxt_qp Cpus_allowed_list:      1
# Name:   watchdog/12     Cpus_allowed_list:      0-19
# Name:   bnxt_qplib_nq   Cpus_allowed_list:      0-19
# Name:   irq/167-bnxt_qp Cpus_allowed_list:      1
# Name:   ib_mad1 Cpus_allowed_list:      0-19
# Name:   irq/168-bnxt_qp Cpus_allowed_list:      1
# Name:   bnxt_qplib_nq   Cpus_allowed_list:      0-19
# Name:   irq/169-bnxt_qp Cpus_allowed_list:      0
# Name:   bnxt_qplib_nq   Cpus_allowed_list:      0-19
# Name:   irq/170-bnxt_qp Cpus_allowed_list:      1
# Name:   bnxt_qplib_nq   Cpus_allowed_list:      0-19
# Name:   migration/12    Cpus_allowed_list:      12
# Name:   irq/171-bnxt_qp Cpus_allowed_list:      0
# Name:   bnxt_qplib_nq   Cpus_allowed_list:      0-19
# Name:   irq/172-bnxt_qp Cpus_allowed_list:      0
# Name:   bnxt_qplib_nq   Cpus_allowed_list:      0-19
# Name:   irq/173-bnxt_qp Cpus_allowed_list:      0
# Name:   bnxt_qplib_nq   Cpus_allowed_list:      0-19
# Name:   irq/174-bnxt_qp Cpus_allowed_list:      0
# Name:   bnxt_qplib_nq   Cpus_allowed_list:      0-19
# Name:   irq/175-bnxt_qp Cpus_allowed_list:      0
# Name:   bnxt_qplib_nq   Cpus_allowed_list:      0-19
# Name:   rcub/1  Cpus_allowed_list:      0-1
# Name:   posixcputmr/12  Cpus_allowed_list:      12
# Name:   irq/176-bnxt_qp Cpus_allowed_list:      0
# Name:   nfit    Cpus_allowed_list:      0-19
# Name:   ib_mad1 Cpus_allowed_list:      0-19
# Name:   rcuc/12 Cpus_allowed_list:      12
# Name:   rdma-ndd        Cpus_allowed_list:      0-1
# Name:   ksoftirqd/12    Cpus_allowed_list:      12
# Name:   rdma_cm Cpus_allowed_list:      0-19
# Name:   kworker/12:0-mm_percpu_wq       Cpus_allowed_list:      12
# Name:   iw_cxgb4        Cpus_allowed_list:      0-19
# Name:   kworker/12:0H   Cpus_allowed_list:      12
# Name:   Register_iWARP_ Cpus_allowed_list:      0-19
# Name:   rcuop/12        Cpus_allowed_list:      0-1
# Name:   rpciod  Cpus_allowed_list:      0-19
# Name:   xprtiod Cpus_allowed_list:      0-19
# Name:   cpuhp/13        Cpus_allowed_list:      13
# Name:   watchdog/13     Cpus_allowed_list:      0-19
# Name:   migration/13    Cpus_allowed_list:      13
# Name:   posixcputmr/13  Cpus_allowed_list:      13
# Name:   irq/119-i40e-ve Cpus_allowed_list:      0
# Name:   irq/120-i40e-ve Cpus_allowed_list:      1
# Name:   irq/121-i40e-ve Cpus_allowed_list:      0
# Name:   irq/122-i40e-ve Cpus_allowed_list:      1
# Name:   irq/123-i40e-ve Cpus_allowed_list:      0
# Name:   irq/124-i40e-ve Cpus_allowed_list:      1
# Name:   irq/125-i40e-ve Cpus_allowed_list:      0
# Name:   irq/126-i40e-ve Cpus_allowed_list:      1
# Name:   irq/127-i40e-ve Cpus_allowed_list:      0
# Name:   irq/128-i40e-ve Cpus_allowed_list:      1
# Name:   irq/129-i40e-ve Cpus_allowed_list:      0
# Name:   irq/130-i40e-ve Cpus_allowed_list:      0
# Name:   irq/131-i40e-ve Cpus_allowed_list:      0
# Name:   irq/132-i40e-ve Cpus_allowed_list:      1
# Name:   irq/133-i40e-ve Cpus_allowed_list:      1
# Name:   irq/134-i40e-ve Cpus_allowed_list:      0
# Name:   irq/135-i40e-ve Cpus_allowed_list:      1
# Name:   irq/136-i40e-ve Cpus_allowed_list:      1
# Name:   irq/137-i40e-ve Cpus_allowed_list:      0
# Name:   irq/138-i40e-ve Cpus_allowed_list:      0
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   pod     Cpus_allowed_list:      0-19
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   sleep   Cpus_allowed_list:      2-18
# Name:   runc    Cpus_allowed_list:      0-1
# Name:   bash    Cpus_allowed_list:      2-18
# Name:   runc    Cpus_allowed_list:      0-1
# Name:   bash    Cpus_allowed_list:      2-18
# Name:   runc    Cpus_allowed_list:      0-1
# Name:   bash    Cpus_allowed_list:      2-18
# Name:   runc    Cpus_allowed_list:      0-1
# Name:   bash    Cpus_allowed_list:      2-18
# Name:   rcuc/0  Cpus_allowed_list:      0
# Name:   rcuc/13 Cpus_allowed_list:      13
# Name:   jbd2/sda1-8     Cpus_allowed_list:      0-1
# Name:   ext4-rsv-conver Cpus_allowed_list:      0-19
# Name:   ksoftirqd/13    Cpus_allowed_list:      13
# Name:   kworker/13:0-mm_percpu_wq       Cpus_allowed_list:      13
# Name:   kworker/13:0H   Cpus_allowed_list:      13
# Name:   srp_remove      Cpus_allowed_list:      0-19
# Name:   licManager      Cpus_allowed_list:      2-18
# Name:   sh      Cpus_allowed_list:      2-18
# Name:   rcuop/13        Cpus_allowed_list:      0-1
# Name:   post-office     Cpus_allowed_list:      2-18
# Name:   oam     Cpus_allowed_list:      2-18
# Name:   tr069-v2        Cpus_allowed_list:      2-18
# Name:   ftp-func        Cpus_allowed_list:      2-18
# Name:   o-ru-controller Cpus_allowed_list:      2-18
# Name:   lighttpd        Cpus_allowed_list:      2-18
# Name:   lighttpd        Cpus_allowed_list:      2-18
# Name:   gnb_cu_oam      Cpus_allowed_list:      2-18
# Name:   bin_reader      Cpus_allowed_list:      2-18
# Name:   duoam   Cpus_allowed_list:      17-18
# Name:   gnb_cu_son      Cpus_allowed_list:      2-4
# Name:   target_completi Cpus_allowed_list:      0-19
# Name:   xcopy_wq        Cpus_allowed_list:      0-19
# Name:   cpuhp/14        Cpus_allowed_list:      14
# Name:   licManager      Cpus_allowed_list:      2-18
# Name:   sh      Cpus_allowed_list:      2-18
# Name:   post-office     Cpus_allowed_list:      2-18
# Name:   oam     Cpus_allowed_list:      2-18
# Name:   tr069-v2        Cpus_allowed_list:      2-18
# Name:   ftp-func        Cpus_allowed_list:      2-18
# Name:   o-ru-controller Cpus_allowed_list:      2-18
# Name:   lighttpd        Cpus_allowed_list:      2-18
# Name:   lighttpd        Cpus_allowed_list:      2-18
# Name:   gnb_cu_oam      Cpus_allowed_list:      2-18
# Name:   bin_reader      Cpus_allowed_list:      18
# Name:   watchdog/14     Cpus_allowed_list:      0-19
# Name:   duoam   Cpus_allowed_list:      17-18
# Name:   migration/14    Cpus_allowed_list:      14
# Name:   licManager      Cpus_allowed_list:      2-18
# Name:   sh      Cpus_allowed_list:      2-18
# Name:   post-office     Cpus_allowed_list:      2-18
# Name:   oam     Cpus_allowed_list:      2-18
# Name:   tr069-v2        Cpus_allowed_list:      2-18
# Name:   ftp-func        Cpus_allowed_list:      2-18
# Name:   o-ru-controller Cpus_allowed_list:      2-18
# Name:   lighttpd        Cpus_allowed_list:      2-18
# Name:   lighttpd        Cpus_allowed_list:      2-18
# Name:   gnb_cu_oam      Cpus_allowed_list:      2-3
# Name:   gnb_cu_pdcp     Cpus_allowed_list:      2-5
# Name:   bin_reader      Cpus_allowed_list:      18
# Name:   bin_reader      Cpus_allowed_list:      18
# Name:   bin_reader      Cpus_allowed_list:      18
# Name:   bin_reader      Cpus_allowed_list:      18
# Name:   duoam   Cpus_allowed_list:      17-18
# Name:   dumgr   Cpus_allowed_list:      17-18
# Name:   gnb_du_layer2   Cpus_allowed_list:      5
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   gnb_cu_son      Cpus_allowed_list:      2-4
# Name:   gnb_cu_l3       Cpus_allowed_list:      7-9
# Name:   posixcputmr/14  Cpus_allowed_list:      14
# Name:   auditd  Cpus_allowed_list:      0-1
# Name:   rcuc/14 Cpus_allowed_list:      14
# Name:   licManager      Cpus_allowed_list:      2-18
# Name:   sh      Cpus_allowed_list:      2-18
# Name:   post-office     Cpus_allowed_list:      2-18
# Name:   oam     Cpus_allowed_list:      2-18
# Name:   tr069-v2        Cpus_allowed_list:      2-18
# Name:   ftp-func        Cpus_allowed_list:      2-18
# Name:   o-ru-controller Cpus_allowed_list:      2-18
# Name:   lighttpd        Cpus_allowed_list:      2-18
# Name:   lighttpd        Cpus_allowed_list:      2-18
# Name:   gnb_cu_oam      Cpus_allowed_list:      2-3
# Name:   gnb_cu_pdcp     Cpus_allowed_list:      2-5
# Name:   bin_reader      Cpus_allowed_list:      18
# Name:   bin_reader      Cpus_allowed_list:      18
# Name:   bin_reader      Cpus_allowed_list:      18
# Name:   bin_reader      Cpus_allowed_list:      18
# Name:   duoam   Cpus_allowed_list:      17-18
# Name:   dumgr   Cpus_allowed_list:      17-18
# Name:   gnb_du_layer2   Cpus_allowed_list:      5
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   gnb_cu_son      Cpus_allowed_list:      2-4
# Name:   gnb_cu_l3       Cpus_allowed_list:      7-9
# Name:   posixcputmr/0   Cpus_allowed_list:      0
# Name:   ksoftirqd/14    Cpus_allowed_list:      14
# Name:   kworker/14:0-mm_percpu_wq       Cpus_allowed_list:      14
# Name:   chronyd Cpus_allowed_list:      0-1
# Name:   sssd    Cpus_allowed_list:      0-1
# Name:   kworker/14:0H   Cpus_allowed_list:      14
# Name:   dbus-daemon     Cpus_allowed_list:      0-1
# Name:   licManager      Cpus_allowed_list:      2-18
# Name:   sh      Cpus_allowed_list:      2-18
# Name:   rcuop/14        Cpus_allowed_list:      0-1
# Name:   post-office     Cpus_allowed_list:      2-18
# Name:   oam     Cpus_allowed_list:      2-18
# Name:   tr069-v2        Cpus_allowed_list:      2-18
# Name:   ftp-func        Cpus_allowed_list:      2-18
# Name:   o-ru-controller Cpus_allowed_list:      2-18
# Name:   lighttpd        Cpus_allowed_list:      2-18
# Name:   lighttpd        Cpus_allowed_list:      2-18
# Name:   gnb_cu_oam      Cpus_allowed_list:      2-3
# Name:   gnb_cu_pdcp     Cpus_allowed_list:      2-4,9
# Name:   bin_reader      Cpus_allowed_list:      18
# Name:   bin_reader      Cpus_allowed_list:      18
# Name:   bin_reader      Cpus_allowed_list:      18
# Name:   bin_reader      Cpus_allowed_list:      18
# Name:   duoam   Cpus_allowed_list:      17-18
# Name:   dumgr   Cpus_allowed_list:      17-18
# Name:   gnb_du_layer2   Cpus_allowed_list:      5
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   gnb_cu_son      Cpus_allowed_list:      2-4
# Name:   gnb_cu_rrm      Cpus_allowed_list:      5-6
# Name:   gnb_cu_l3       Cpus_allowed_list:      2-18
# Name:   cpuhp/15        Cpus_allowed_list:      15
# Name:   watchdog/15     Cpus_allowed_list:      0-19
# Name:   migration/15    Cpus_allowed_list:      15
# Name:   posixcputmr/15  Cpus_allowed_list:      15
# Name:   sssd_be Cpus_allowed_list:      0-1
# Name:   licManager      Cpus_allowed_list:      2-18
# Name:   sh      Cpus_allowed_list:      2-18
# Name:   post-office     Cpus_allowed_list:      2-18
# Name:   oam     Cpus_allowed_list:      2-18
# Name:   tr069-v2        Cpus_allowed_list:      2-18
# Name:   ftp-func        Cpus_allowed_list:      2-18
# Name:   o-ru-controller Cpus_allowed_list:      2-18
# Name:   lighttpd        Cpus_allowed_list:      2-18
# Name:   lighttpd        Cpus_allowed_list:      2-18
# Name:   gnb_cu_oam      Cpus_allowed_list:      2-3
# Name:   gnb_cu_pdcp     Cpus_allowed_list:      2-4,9
# Name:   bin_reader      Cpus_allowed_list:      18
# Name:   bin_reader      Cpus_allowed_list:      18
# Name:   bin_reader      Cpus_allowed_list:      18
# Name:   bin_reader      Cpus_allowed_list:      18
# Name:   duoam   Cpus_allowed_list:      17-18
# Name:   dumgr   Cpus_allowed_list:      17-18
# Name:   gnb_du_layer2   Cpus_allowed_list:      5
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   bin_reader      Cpus_allowed_list:      2
# Name:   gnb_cu_son      Cpus_allowed_list:      2-4
# Name:   gnb_cu_rrm      Cpus_allowed_list:      5-6
# Name:   gnb_cu_l3       Cpus_allowed_list:      2-18
# Name:   kworker/u40:0-events_unbound    Cpus_allowed_list:      0-1
# Name:   rcuc/15 Cpus_allowed_list:      15
# Name:   ksoftirqd/15    Cpus_allowed_list:      15
# Name:   migration/0     Cpus_allowed_list:      0
# Name:   kworker/15:0-mm_percpu_wq       Cpus_allowed_list:      15
# Name:   sssd_nss        Cpus_allowed_list:      0-1
# Name:   kworker/15:0H   Cpus_allowed_list:      15
# Name:   systemd-logind  Cpus_allowed_list:      0-1
# Name:   rcuop/15        Cpus_allowed_list:      0-1
# Name:   vim     Cpus_allowed_list:      2-18
# Name:   ovsdb-server    Cpus_allowed_list:      0-1
# Name:   cpuhp/16        Cpus_allowed_list:      16
# Name:   watchdog/16     Cpus_allowed_list:      0-19
# Name:   migration/16    Cpus_allowed_list:      16
# Name:   posixcputmr/16  Cpus_allowed_list:      16
# Name:   rcuc/16 Cpus_allowed_list:      16
# Name:   ksoftirqd/16    Cpus_allowed_list:      16
# Name:   kworker/16:0-mm_percpu_wq       Cpus_allowed_list:      16
# Name:   watchdog/0      Cpus_allowed_list:      0
# Name:   kworker/16:0H   Cpus_allowed_list:      16
# Name:   rcuop/16        Cpus_allowed_list:      0-1
# Name:   cpuhp/17        Cpus_allowed_list:      17
# Name:   ovs-vswitchd    Cpus_allowed_list:      0-1
# Name:   watchdog/17     Cpus_allowed_list:      0-19
# Name:   kworker/u40:1-events_unbound    Cpus_allowed_list:      0-1
# Name:   migration/17    Cpus_allowed_list:      17
# Name:   kworker/0:0-events      Cpus_allowed_list:      0
# Name:   posixcputmr/17  Cpus_allowed_list:      17
# Name:   rcuc/17 Cpus_allowed_list:      17
# Name:   NetworkManager  Cpus_allowed_list:      0-1
# Name:   kworker/0:1-events      Cpus_allowed_list:      0
# Name:   ksoftirqd/17    Cpus_allowed_list:      17
# Name:   kworker/u40:2-events_unbound    Cpus_allowed_list:      0-1
# Name:   sshd    Cpus_allowed_list:      0-1
# Name:   sshd    Cpus_allowed_list:      0-1
# Name:   bash    Cpus_allowed_list:      0-1
# Name:   sudo    Cpus_allowed_list:      0-1
# Name:   bash    Cpus_allowed_list:      0-1
# Name:   kworker/17:0-mm_percpu_wq       Cpus_allowed_list:      17
# Name:   irq/79-i40e-ens Cpus_allowed_list:      1
# Name:   irq/80-i40e-ens Cpus_allowed_list:      0
# Name:   irq/81-i40e-ens Cpus_allowed_list:      0
# Name:   irq/82-i40e-ens Cpus_allowed_list:      1
# Name:   kworker/17:0H   Cpus_allowed_list:      17
# Name:   irq/83-i40e-ens Cpus_allowed_list:      0
# Name:   irq/84-i40e-ens Cpus_allowed_list:      0
# Name:   kworker/1:1-xfs-cil/dm-0        Cpus_allowed_list:      1
# Name:   irq/85-i40e-ens Cpus_allowed_list:      0
# Name:   irq/86-i40e-ens Cpus_allowed_list:      0
# Name:   irq/87-i40e-ens Cpus_allowed_list:      0
# Name:   irq/88-i40e-ens Cpus_allowed_list:      0
# Name:   irq/89-i40e-ens Cpus_allowed_list:      0
# Name:   irq/90-i40e-ens Cpus_allowed_list:      0
# Name:   irq/91-i40e-ens Cpus_allowed_list:      0
# Name:   irq/92-i40e-ens Cpus_allowed_list:      0
# Name:   cpuhp/0 Cpus_allowed_list:      0
# Name:   rcuop/17        Cpus_allowed_list:      0-1
# Name:   irq/93-i40e-ens Cpus_allowed_list:      1
# Name:   irq/94-i40e-ens Cpus_allowed_list:      0
# Name:   irq/95-i40e-ens Cpus_allowed_list:      1
# Name:   irq/96-i40e-ens Cpus_allowed_list:      0
# Name:   irq/97-i40e-ens Cpus_allowed_list:      1
# Name:   irq/98-i40e-ens Cpus_allowed_list:      1
# Name:   kworker/1:4-xfs-cil/dm-0        Cpus_allowed_list:      1
# Name:   cpuhp/18        Cpus_allowed_list:      18
# Name:   kworker/0:0H-xfs-log/dm-0       Cpus_allowed_list:      0
# Name:   kworker/u40:3-events_unbound    Cpus_allowed_list:      0-1
# Name:   watchdog/18     Cpus_allowed_list:      0-19
# Name:   kworker/1:0-events      Cpus_allowed_list:      1
# Name:   migration/18    Cpus_allowed_list:      18
# Name:   sleep   Cpus_allowed_list:      0-1,19
# Name:   sleep   Cpus_allowed_list:      0-1,19
# Name:   sh      Cpus_allowed_list:      0-1
# Name:   posixcputmr/18  Cpus_allowed_list:      18
# Name:   agetty  Cpus_allowed_list:      0-1
# Name:   agetty  Cpus_allowed_list:      0-1
# Name:   rcuc/18 Cpus_allowed_list:      18
# Name:   ksoftirqd/18    Cpus_allowed_list:      18
# Name:   kworker/18:0-mm_percpu_wq       Cpus_allowed_list:      18
# Name:   kworker/18:0H   Cpus_allowed_list:      18
# Name:   rcuop/18        Cpus_allowed_list:      0-1
# Name:   cpuhp/1 Cpus_allowed_list:      1
# Name:   cpuhp/19        Cpus_allowed_list:      19
# Name:   irq/70-ens81f0n Cpus_allowed_list:      0
# Name:   irq/71-ens81f0n Cpus_allowed_list:      1
# Name:   irq/72-ens81f0n Cpus_allowed_list:      0
# Name:   irq/73-ens81f0n Cpus_allowed_list:      1
# Name:   irq/74-ens81f0n Cpus_allowed_list:      0
# Name:   watchdog/19     Cpus_allowed_list:      0-19
# Name:   irq/75-ens81f0n Cpus_allowed_list:      1
# Name:   irq/76-ens81f0n Cpus_allowed_list:      0
# Name:   irq/77-ens81f0n Cpus_allowed_list:      1
# Name:   migration/19    Cpus_allowed_list:      19
# Name:   posixcputmr/19  Cpus_allowed_list:      19
# Name:   rcuc/19 Cpus_allowed_list:      19
# Name:   irq/110-ens81f1 Cpus_allowed_list:      0
# Name:   irq/111-ens81f1 Cpus_allowed_list:      1
# Name:   irq/112-ens81f1 Cpus_allowed_list:      0
# Name:   irq/113-ens81f1 Cpus_allowed_list:      1
# Name:   irq/114-ens81f1 Cpus_allowed_list:      0
# Name:   irq/115-ens81f1 Cpus_allowed_list:      1
# Name:   irq/116-ens81f1 Cpus_allowed_list:      0
# Name:   irq/117-ens81f1 Cpus_allowed_list:      1
# Name:   ksoftirqd/19    Cpus_allowed_list:      19
# Name:   kworker/19:0-mm_percpu_wq       Cpus_allowed_list:      19
# Name:   kworker/19:0H   Cpus_allowed_list:      19
# Name:   rcuop/19        Cpus_allowed_list:      0-1
# Name:   watchdog/1      Cpus_allowed_list:      0-19
# Name:   irq/4-ttyS0     Cpus_allowed_list:      0
# Name:   kdevtmpfs       Cpus_allowed_list:      0-1
# Name:   netns   Cpus_allowed_list:      0-19
# Name:   rcu_tasks_kthre Cpus_allowed_list:      0-1
# Name:   kauditd Cpus_allowed_list:      0-1
# Name:   sshd    Cpus_allowed_list:      0-1
# Name:   rpcbind Cpus_allowed_list:      0-1
# Name:   rpc.statd       Cpus_allowed_list:      0-1
# Name:   khungtaskd      Cpus_allowed_list:      0-1
# Name:   oom_reaper      Cpus_allowed_list:      0-1
# Name:   kthreadd        Cpus_allowed_list:      0-1
# Name:   migration/1     Cpus_allowed_list:      1
# Name:   writeback       Cpus_allowed_list:      0-19
# Name:   kcompactd0      Cpus_allowed_list:      0-1
# Name:   ksmd    Cpus_allowed_list:      0-1
# Name:   crypto  Cpus_allowed_list:      0-19
# Name:   kintegrityd     Cpus_allowed_list:      0-19
# Name:   kblockd Cpus_allowed_list:      0-19
# Name:   irq/9-acpi      Cpus_allowed_list:      0
# Name:   tpm_dev_wq      Cpus_allowed_list:      0-19
# Name:   posixcputmr/1   Cpus_allowed_list:      1
# Name:   md      Cpus_allowed_list:      0-19
# Name:   crio    Cpus_allowed_list:      0-1
# Name:   edac-poller     Cpus_allowed_list:      0-19
# Name:   watchdogd       Cpus_allowed_list:      0-1
# Name:   rcuc/1  Cpus_allowed_list:      1
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   ksoftirqd/1     Cpus_allowed_list:      1
# Name:   kswapd0 Cpus_allowed_list:      0-1
# Name:   pod     Cpus_allowed_list:      0-1
# Name:   pod     Cpus_allowed_list:      0-1
# Name:   pod     Cpus_allowed_list:      0-1
# Name:   pod     Cpus_allowed_list:      0-1
# Name:   pod     Cpus_allowed_list:      0-1
# Name:   pod     Cpus_allowed_list:      0-1
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   kworker/2:1-mm_percpu_wq        Cpus_allowed_list:      2
# Name:   pod     Cpus_allowed_list:      0-1
# Name:   kworker/3:1-mm_percpu_wq        Cpus_allowed_list:      3
# Name:   kworker/4:1-mm_percpu_wq        Cpus_allowed_list:      4
# Name:   kworker/1:1H-kblockd    Cpus_allowed_list:      1
# Name:   kworker/5:1-mm_percpu_wq        Cpus_allowed_list:      5
# Name:   kworker/6:1-mm_percpu_wq        Cpus_allowed_list:      6
# Name:   kworker/7:1-mm_percpu_wq        Cpus_allowed_list:      7
# Name:   kworker/8:1-mm_percpu_wq        Cpus_allowed_list:      8
# Name:   kworker/9:1-mm_percpu_wq        Cpus_allowed_list:      9
# Name:   kworker/10:1-mm_percpu_wq       Cpus_allowed_list:      10
# Name:   kworker/11:1-mm_percpu_wq       Cpus_allowed_list:      11
# Name:   kworker/12:1-mm_percpu_wq       Cpus_allowed_list:      12
# Name:   kworker/13:1-mm_percpu_wq       Cpus_allowed_list:      13
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   kworker/14:1-mm_percpu_wq       Cpus_allowed_list:      14
# Name:   kworker/15:1-mm_percpu_wq       Cpus_allowed_list:      15
# Name:   cpuhp/2 Cpus_allowed_list:      2
# Name:   kworker/16:1-mm_percpu_wq       Cpus_allowed_list:      16
# Name:   sh      Cpus_allowed_list:      0-1,19
# Name:   kworker/17:1-mm_percpu_wq       Cpus_allowed_list:      17
# Name:   tail    Cpus_allowed_list:      0-1,19
# Name:   kworker/18:1-mm_percpu_wq       Cpus_allowed_list:      18
# Name:   kworker/19:1-mm_percpu_wq       Cpus_allowed_list:      19
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   watchdog/2      Cpus_allowed_list:      0-19
# Name:   kubelet Cpus_allowed_list:      0-1
# Name:   systemd Cpus_allowed_list:      0-1
# Name:   (sd-pam)        Cpus_allowed_list:      0-1
# Name:   podman pause    Cpus_allowed_list:      0-1
# Name:   machine-config- Cpus_allowed_list:      0-1,19
# Name:   kworker/0:1H-kblockd    Cpus_allowed_list:      0
# Name:   openshift-sdn-n Cpus_allowed_list:      0-1,19
# Name:   migration/2     Cpus_allowed_list:      2
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   run     Cpus_allowed_list:      0-1,19
# Name:   posixcputmr/2   Cpus_allowed_list:      2
# Name:   rcu_gp  Cpus_allowed_list:      0-19
# Name:   rcuc/2  Cpus_allowed_list:      2
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   oauth-proxy     Cpus_allowed_list:      0-1,19
# Name:   kube-rbac-proxy Cpus_allowed_list:      0-1,19
# Name:   openshift-tuned Cpus_allowed_list:      0-1,19
# Name:   ksoftirqd/2     Cpus_allowed_list:      2
# Name:   kworker/1:2H-kblockd    Cpus_allowed_list:      1
# Name:   polkitd Cpus_allowed_list:      0-1
# Name:   kworker/2:0-mm_percpu_wq        Cpus_allowed_list:      2
# Name:   journalctl      Cpus_allowed_list:      0-1,19
# Name:   kworker/2:0H    Cpus_allowed_list:      2
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   node_exporter   Cpus_allowed_list:      0-1,19
# Name:   rcuop/2 Cpus_allowed_list:      0-1
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   kthrotld        Cpus_allowed_list:      0-19
# Name:   irq/24-PCIe PME Cpus_allowed_list:      1
# Name:   irq/26-PCIe PME Cpus_allowed_list:      1
# Name:   kube-rbac-proxy Cpus_allowed_list:      0-1,19
# Name:   irq/26-pciehp   Cpus_allowed_list:      1
# Name:   irq/26-s-pciehp Cpus_allowed_list:      1
# Name:   irq/27-PCIe PME Cpus_allowed_list:      1
# Name:   cpuhp/3 Cpus_allowed_list:      3
# Name:   irq/65-PCIe PME Cpus_allowed_list:      0
# Name:   irq/65-aerdrv   Cpus_allowed_list:      0
# Name:   irq/65-s-aerdrv Cpus_allowed_list:      0
# Name:   acpi_thermal_pm Cpus_allowed_list:      0-19
# Name:   kmpath_rdacd    Cpus_allowed_list:      0-19
# Name:   kaluad  Cpus_allowed_list:      0-19
# Name:   irq/66-xhci_hcd Cpus_allowed_list:      1
# Name:   irq/8-rtc0      Cpus_allowed_list:      0
# Name:   ipv6_addrconf   Cpus_allowed_list:      0-19
# Name:   kstrp   Cpus_allowed_list:      0-19
# Name:   watchdog/3      Cpus_allowed_list:      0-19
# Name:   migration/3     Cpus_allowed_list:      3
# Name:   posixcputmr/3   Cpus_allowed_list:      3
# Name:   rcuc/3  Cpus_allowed_list:      3
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   pod     Cpus_allowed_list:      0-1
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   network-metrics Cpus_allowed_list:      0-1,19
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   rcu_par_gp      Cpus_allowed_list:      0-19
# Name:   ksoftirqd/3     Cpus_allowed_list:      3
# Name:   pod     Cpus_allowed_list:      0-1
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   kube-rbac-proxy Cpus_allowed_list:      0-1,19
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   kworker/3:0-mm_percpu_wq        Cpus_allowed_list:      3
# Name:   entrypoint.sh   Cpus_allowed_list:      0-1,19
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   coredns Cpus_allowed_list:      0-1,19
# Name:   kworker/3:0H    Cpus_allowed_list:      3
# Name:   rcuop/3 Cpus_allowed_list:      0-1
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   kube-rbac-proxy Cpus_allowed_list:      0-1,19
# Name:   cpuhp/4 Cpus_allowed_list:      4
# Name:   conmon  Cpus_allowed_list:      0-1
# Name:   bash    Cpus_allowed_list:      0-1,19
# Name:   watchdog/4      Cpus_allowed_list:      0-19
# Name:   migration/4     Cpus_allowed_list:      4
# Name:   posixcputmr/4   Cpus_allowed_list:      4
# Name:   tuned   Cpus_allowed_list:      0-1,19
# Name:   rcuc/4  Cpus_allowed_list:      4
# Name:   ksoftirqd/4     Cpus_allowed_list:      4
# Name:   irqbalance      Cpus_allowed_list:      0-1
# Name:   stalld  Cpus_allowed_list:      0-1
# Name:   kworker/4:0-mm_percpu_wq        Cpus_allowed_list:      4
# Name:   kworker/4:0H    Cpus_allowed_list:      4
# Name:   iscsi_eh        Cpus_allowed_list:      0-19
# Name:   rcuop/4 Cpus_allowed_list:      0-1
# Name:   cpuhp/5 Cpus_allowed_list:      5
# Name:   watchdog/5      Cpus_allowed_list:      0-19
# Name:   migration/5     Cpus_allowed_list:      5
# Name:   posixcputmr/5   Cpus_allowed_list:      5
# Name:   rcuc/5  Cpus_allowed_list:      5
# Name:   ksoftirqd/5     Cpus_allowed_list:      5
# Name:   kworker/5:0-mm_percpu_wq        Cpus_allowed_list:      5
# Name:   kworker/5:0H    Cpus_allowed_list:      5
# Name:   rcuop/5 Cpus_allowed_list:      0-1
# Name:   cpuhp/6 Cpus_allowed_list:      6
# Name:   watchdog/6      Cpus_allowed_list:      0-19
# Name:   cnic_wq Cpus_allowed_list:      0-19
# Name:   bnx2i_thread/0  Cpus_allowed_list:      0
# Name:   bnx2i_thread/1  Cpus_allowed_list:      1
# Name:   bnx2i_thread/2  Cpus_allowed_list:      2
# Name:   bnx2i_thread/3  Cpus_allowed_list:      3
# Name:   bnx2i_thread/4  Cpus_allowed_list:      4
# Name:   bnx2i_thread/5  Cpus_allowed_list:      5
# Name:   bnx2i_thread/6  Cpus_allowed_list:      6
# Name:   migration/6     Cpus_allowed_list:      6
# Name:   bnx2i_thread/7  Cpus_allowed_list:      7
# Name:   bnx2i_thread/8  Cpus_allowed_list:      8
# Name:   bnx2i_thread/9  Cpus_allowed_list:      9
# Name:   bnx2i_thread/10 Cpus_allowed_list:      10
# Name:   bnx2i_thread/11 Cpus_allowed_list:      11
# Name:   bnx2i_thread/12 Cpus_allowed_list:      12
# Name:   bnx2i_thread/13 Cpus_allowed_list:      13
# Name:   bnx2i_thread/14 Cpus_allowed_list:      14
# Name:   bnx2i_thread/15 Cpus_allowed_list:      15
# Name:   bnx2i_thread/16 Cpus_allowed_list:      16
# Name:   posixcputmr/6   Cpus_allowed_list:      6
# Name:   bnx2i_thread/17 Cpus_allowed_list:      17
# Name:   bnx2i_thread/18 Cpus_allowed_list:      18
# Name:   bnx2i_thread/19 Cpus_allowed_list:      19
# Name:   rcuc/6  Cpus_allowed_list:      6
# Name:   ksoftirqd/6     Cpus_allowed_list:      6
# Name:   kworker/6:0-mm_percpu_wq        Cpus_allowed_list:      6
# Name:   kmpathd Cpus_allowed_list:      0-19
# Name:   kworker/6:0H    Cpus_allowed_list:      6
# Name:   kmpath_handlerd Cpus_allowed_list:      0-19
# Name:   rcuop/6 Cpus_allowed_list:      0-1
# Name:   cpuhp/7 Cpus_allowed_list:      7
# Name:   watchdog/7      Cpus_allowed_list:      0-19
# Name:   migration/7     Cpus_allowed_list:      7
# Name:   posixcputmr/7   Cpus_allowed_list:      7
# Name:   rcuc/7  Cpus_allowed_list:      7
# Name:   ksoftirqd/7     Cpus_allowed_list:      7
# Name:   kworker/7:0-mm_percpu_wq        Cpus_allowed_list:      7
# Name:   kworker/7:0H    Cpus_allowed_list:      7
# Name:   ata_sff Cpus_allowed_list:      0-19
# Name:   i40e    Cpus_allowed_list:      0-19
# Name:   rcuop/7 Cpus_allowed_list:      0-1
# Name:   bnxt_pf_wq      Cpus_allowed_list:      0-19
# Name:   irq/67-ahci[000 Cpus_allowed_list:      0
# Name:   scsi_eh_0       Cpus_allowed_list:      0-1
# Name:   scsi_tmf_0      Cpus_allowed_list:      0-19
# Name:   scsi_eh_1       Cpus_allowed_list:      0-1
# Name:   scsi_tmf_1      Cpus_allowed_list:      0-19
# Name:   scsi_eh_2       Cpus_allowed_list:      0-1
# Name:   scsi_tmf_2      Cpus_allowed_list:      0-19
# Name:   cpuhp/8 Cpus_allowed_list:      8
# Name:   scsi_eh_3       Cpus_allowed_list:      0-1
# Name:   scsi_tmf_3      Cpus_allowed_list:      0-19
# Name:   scsi_eh_4       Cpus_allowed_list:      0-1
# Name:   scsi_tmf_4      Cpus_allowed_list:      0-19
# Name:   scsi_eh_5       Cpus_allowed_list:      0-1
# Name:   scsi_tmf_5      Cpus_allowed_list:      0-19
# Name:   watchdog/8      Cpus_allowed_list:      0-19
# Name:   irq/99-i40e-000 Cpus_allowed_list:      0
# Name:   irq/78-i40e-000 Cpus_allowed_list:      0
# Name:   irq/109-ahci[00 Cpus_allowed_list:      1
# Name:   scsi_eh_6       Cpus_allowed_list:      0-1
# Name:   migration/8     Cpus_allowed_list:      8
# Name:   scsi_tmf_6      Cpus_allowed_list:      0-19
# Name:   scsi_eh_7       Cpus_allowed_list:      0-1
# Name:   scsi_tmf_7      Cpus_allowed_list:      0-19
# Name:   scsi_eh_8       Cpus_allowed_list:      0-1
# Name:   scsi_tmf_8      Cpus_allowed_list:      0-19
# Name:   scsi_eh_9       Cpus_allowed_list:      0-1
# Name:   scsi_tmf_9      Cpus_allowed_list:      0-19
# Name:   scsi_eh_10      Cpus_allowed_list:      0-1
# Name:   scsi_tmf_10     Cpus_allowed_list:      0-19
# Name:   posixcputmr/8   Cpus_allowed_list:      8
# Name:   scsi_eh_11      Cpus_allowed_list:      0-1
# Name:   scsi_tmf_11     Cpus_allowed_list:      0-19
# Name:   scsi_eh_12      Cpus_allowed_list:      0-1
# Name:   scsi_tmf_12     Cpus_allowed_list:      0-19
# Name:   scsi_eh_13      Cpus_allowed_list:      0-1
# Name:   scsi_tmf_13     Cpus_allowed_list:      0-19
# Name:   irq/139-i40e-00 Cpus_allowed_list:      0
# Name:   rcuc/8  Cpus_allowed_list:      8
# Name:   irq/118-i40e-00 Cpus_allowed_list:      1
# Name:   ksoftirqd/8     Cpus_allowed_list:      8
# Name:   kworker/8:0-mm_percpu_wq        Cpus_allowed_list:      8
# Name:   kworker/8:0H    Cpus_allowed_list:      8
# Name:   rcuop/8 Cpus_allowed_list:      0-1
# Name:   cpuhp/9 Cpus_allowed_list:      9
# Name:   mm_percpu_wq    Cpus_allowed_list:      0-19
# Name:   watchdog/9      Cpus_allowed_list:      0-19
# Name:   migration/9     Cpus_allowed_list:      9
# Name:   posixcputmr/9   Cpus_allowed_list:      9
# Name:   kdmflush        Cpus_allowed_list:      0-19
# Name:   rcuc/9  Cpus_allowed_list:      9
# Name:   xfsalloc        Cpus_allowed_list:      0-19
# Name:   xfs_mru_cache   Cpus_allowed_list:      0-19
# Name:   ksoftirqd/9     Cpus_allowed_list:      9
# Name:   xfs-buf/dm-0    Cpus_allowed_list:      0-19
# Name:   xfs-conv/dm-0   Cpus_allowed_list:      0-19
# Name:   xfs-cil/dm-0    Cpus_allowed_list:      0-19
# Name:   xfs-reclaim/dm- Cpus_allowed_list:      0-19
# Name:   xfs-log/dm-0    Cpus_allowed_list:      0-19
# Name:   xfs-eofblocks/d Cpus_allowed_list:      0-19
# Name:   xfsaild/dm-0    Cpus_allowed_list:      0-1
# Name:   kworker/9:0-mm_percpu_wq        Cpus_allowed_list:      9
# Name:   kworker/9:0H    Cpus_allowed_list:      9
# Name:   rcuop/9 Cpus_allowed_list:      0-1
# Name:   cpuhp/10        Cpus_allowed_list:      10
# Name:   watchdog/10     Cpus_allowed_list:      0-19

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-43-build-config--hpa"><a class="header" href="#openshift-43-build-config--hpa">openshift 4.3 build config &amp; hpa</a></h1>
<p>video for build config &amp; scale up</p>
<ul>
<li>https://youtu.be/O0TjPBisMVo</li>
<li>https://www.bilibili.com/video/BV1rT4y137QJ/</li>
<li>https://www.ixigua.com/i6824464593977344525/</li>
</ul>
<p>video for scale up &amp; service</p>
<ul>
<li>https://youtu.be/6fMe7T4RlCI</li>
<li>https://www.bilibili.com/video/BV1Xt4y1y7xG/</li>
<li>https://www.ixigua.com/i6824739572237206023/</li>
</ul>
<h2 id="php-build-config"><a class="header" href="#php-build-config">php build config</a></h2>
<pre><code class="language-bash">
# 准备一个php的测试镜像
cat &lt;&lt; EOF &gt; php.dockerfile
FROM php:apache
COPY . /var/www/html/
EOF

cat &lt;&lt;EOF &gt; index.php
&lt;?php
ECHO &quot;Hello!&lt;br&gt;&quot;;
echo &quot;Welcome to RedHat Developer&lt;br&gt;&quot;;
EcHo &quot;Enjoy all of the ad-free articles&lt;br&gt;&quot;;
?&gt;
EOF

buildah build-using-dockerfile -t docker.io/wangzheng422/php:demo -f php.dockerfile .

podman run -it --rm -p 18080:80 --name my-running-app docker.io/wangzheng422/php:demo

# 创建一个git服务器，用gogs，启动以后要做一些配置。
# 配置 resolve.conf
# 配置 app.ini
# [webhook]
# SKIP_TLS_VERIFY  = true

mkdir -p /data/ocp4/gogs
podman run -d --name=gogs -p 10022:22 -p 10080:3000 -v /data/ocp4/gogs:/data:Z registry.redhat.ren:5443/docker.io/gogs/gogs

podman stop gogs
podman start gogs
# http://registry.redhat.ren:10080

# 在demo项目中，创建编译配置
oc project demo

oc import-image php:apache-wzh --from=registry.redhat.ren:5443/docker.io/library/php:apache-wzh --confirm

# oc import-image php:apache-wzh --from=registry.redhat.ren:5443/docker.io/wangzheng422/php:apache --confirm

oc create is php-sample -n demo

cat &lt;&lt; EOF &gt; bc.is.yaml
kind: BuildConfig
apiVersion: build.openshift.io/v1
metadata:
  name: &quot;php-sample-build&quot; 
spec:
  runPolicy: &quot;Serial&quot; 
  triggers: 
    - type: &quot;Generic&quot;
      generic:
        secret: &quot;secret101&quot;
    -
      type: &quot;ImageChange&quot;
  source: 
    git:
      uri: &quot;http://registry.redhat.ren:10080/root/php&quot;
    dockerfile: &quot;FROM php:apache\nCOPY . /var/www/html/&quot; 
  strategy: 
    dockerStrategy:
      from:
        kind: &quot;ImageStreamTag&quot;
        name: &quot;php:apache-wzh&quot;
  output: 
    to:
      kind: &quot;ImageStreamTag&quot;
      name: &quot;php-sample:demo&quot;
EOF
oc apply -f bc.is.yaml

# 在界面上操作，通过镜像创建应用，并通过代码更改，触发应用的重新部署。

</code></pre>
<h2 id="hpa"><a class="header" href="#hpa">hpa</a></h2>
<p>我们在这里展示openshift如何根据cpu的负载，自动扩缩pod的数量。</p>
<p>video</p>
<ul>
<li>https://youtu.be/_UTncz3StXE</li>
<li>https://www.bilibili.com/video/BV1Tk4y1r7Be/</li>
</ul>
<pre><code class="language-bash">
# oc autoscale dc/php-sample \
#   --min 1 \
#   --max 3 \
#   --cpu-percent=50 

# 根据已经创建的deployment，创建HPA
cat &lt;&lt; 'EOF' &gt; demo.hpa.yaml
kind: HorizontalPodAutoscaler
apiVersion: autoscaling/v2beta1
metadata:
  name: php-sample
  namespace: demo
spec:
  scaleTargetRef:
    kind: Deployment
    name: php-sample
    apiVersion: apps/v1
  minReplicas: 1
  maxReplicas: 3
  metrics:
    - type: Resource
      resource:
        name: cpu
        targetAverageUtilization: 50
EOF
oc apply -n demo -f demo.hpa.yaml

# 为了不影响其他测试，我们把php pod定点到同一个交换机主机上
cat &lt;&lt; 'EOF'
nodeSelector:
    kubernetes.io/hostname: 'infra1.hsc.redhat.ren'
EOF

# 为了更好的展示效果，我们限制cpu使用量
cat &lt;&lt; 'EOF'
    resources:
      requests:
        cpu: '100m'
        memory: &quot;1G&quot;
      limits:
        cpu: '100m'
        memory: &quot;1G&quot;
EOF

# 开始压力
ab -c 100 -n 99999 http://php-sample-demo.apps.ocpsc.redhat.ren/

# 查看当前hpa的状态
oc describe hpa/php-sample

</code></pre>
<h2 id="弯路"><a class="header" href="#弯路">弯路</a></h2>
<pre><code class="language-bash">skopeo copy docker://docker.io/php:apache docker-archive:///root/tmp/php.tar
gzip php.tar

skopeo copy docker-archive:///data/ocp4/tmp/php.tar.gz docker://registry.redhat.ren:5443/docker.io/library/php:apache

skopeo copy docker://docker.io/wangzheng422/php:apache docker://registry.redhat.ren:5443/docker.io/wangzheng422/php:apache

cat &lt;&lt; EOF &gt; docker.php.sh
#!/usr/bin/env bash

set -e
set -x

buildah from --name onbuild-container docker.io/php:apache
buildah run onbuild-container sed -i &quot;s/80/8080/g&quot; /etc/apache2/sites-available/000-default.conf /etc/apache2/ports.conf
buildah umount onbuild-container 
buildah config -p 8080 onbuild-container
buildah commit --squash --rm --format=docker onbuild-container docker.io/wangzheng422/php:apache
buildah push docker.io/wangzheng422/php:apache
EOF
bash docker.php.sh

cat &lt;&lt; EOF &gt; docker.php.sh
#!/usr/bin/env bash

set -e
set -x

buildah from --name onbuild-container registry.redhat.ren:5443/docker.io/library/php:apache
buildah run onbuild-container sed -i &quot;s/80/8080/g&quot; /etc/apache2/sites-available/000-default.conf /etc/apache2/ports.conf
buildah umount onbuild-container 
buildah config -p 8080 onbuild-container
buildah commit --squash --rm --format=docker onbuild-container registry.redhat.ren:5443/docker.io/library/php:apache-wzh
buildah push registry.redhat.ren:5443/docker.io/library/php:apache-wzh
EOF
bash docker.php.sh

# 我们不需要复杂的 template
oc get template -n openshift | grep php

# 用 source to image 功能就可以，所有找一下image stream
oc get is -A | grep php

# 我们把sample operator的状态改一下
oc get configs.samples.operator.openshift.io/cluster -o yaml

oc patch configs.samples.operator.openshift.io/cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Unmanaged&quot;}}' --type=merge

export LOCAL_REG='registry.redhat.ren:5443'

var_is_name='php'
var_json=$(oc get is ${var_is_name} -n openshift -o json)

var_j=0
for var_is_tag in $(echo $var_json | jq -r &quot;.spec.tags[].name&quot;); do
    var_is_image_name=$(echo $var_json | jq -r &quot;.spec.tags[${var_j}].from.name&quot;)
        
    var_is_image_kind=$(echo $var_json | jq -r &quot;.spec.tags[${var_j}].from.kind&quot;)
    
    if [[ $var_is_image_kind =~ 'DockerImage'  ]]; then
        var_new_is_image_name=&quot;${LOCAL_REG}/$var_is_image_name&quot;
        
        echo &quot;###############################&quot;
        echo $var_is_image_name
        echo $var_is_image_kind
        echo $var_new_is_image_name
        echo $var_is_tag

        oc patch -n openshift is ${var_is_name} --type='json' -p=&quot;[{\&quot;op\&quot;: \&quot;replace\&quot;, \&quot;path\&quot;: \&quot;/spec/tags/${var_j}/from/name\&quot;, \&quot;value\&quot;:\&quot;${var_new_is_image_name}\&quot;}]&quot;
    fi
    var_j=$((var_j+1))
done

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="containered-cloud-native-ccn-roadshow-离线部署"><a class="header" href="#containered-cloud-native-ccn-roadshow-离线部署">containered cloud-native (ccn) roadshow 离线部署</a></h1>
<p>CCN是一个不错的演示openshift之上，ci/cd, cloud-native, istio, serverless的演示教材，教学的内容非常丰富。</p>
<p>第一个模块，着重讲解如何拆分单体应用，以及拆分的应用如何上云。</p>
<p>第二个模块，讲解如何在线debug, 如何监控上云的应用</p>
<p>第三个模块，应用转换到服务网格service mesh/istio架构</p>
<p>第四个模块，应用使用无服务架构serverless/knative架构开发</p>
<p>培训过程视频</p>
<ul>
<li>The Containers and Cloud-Native Roadshow Dev Track - Module 1
<ul>
<li><a href="https://www.bilibili.com/read/cv6713699">bilibili</a></li>
<li><a href="https://www.ixigua.com/6847386067889455630">xigua</a></li>
<li><a href="https://www.youtube.com/playlist?list=PLRL8GgzNL9SgeODjqT1aU8ya95KfwgElv">youtube</a></li>
<li><a href="https://1drv.ms/b/s!AqLmU5b8zhHEoxFp6ef1t5bpS7Dy?e=oi3IKg">教材</a></li>
</ul>
</li>
<li>The Containers and Cloud-Native Roadshow Dev Track - Module 2
<ul>
<li><a href="https://www.bilibili.com/read/cv6868575">bilibili</a></li>
<li><a href="https://www.ixigua.com/6848440375573479949">xigua</a></li>
<li><a href="https://www.youtube.com/watch?v=Kr4dwz2QaeU&amp;list=PLRL8GgzNL9SjG9YZRxKHhCdN5XK32QpOA">youtube</a></li>
<li><a href="https://1drv.ms/b/s!AqLmU5b8zhHEoxOzpffwSNSetYtI?e=ktNJoo">教材</a></li>
</ul>
</li>
<li>The Containers and Cloud-Native Roadshow Dev Track - Module 3
<ul>
<li><a href="https://www.bilibili.com/read/cv6888587">bilibili</a></li>
<li><a href="https://www.ixigua.com/6849151451068006919">xigua</a></li>
<li><a href="https://www.youtube.com/watch?v=rxzr3Tjydzw&amp;list=PLRL8GgzNL9SjBFitrp01FUNmrSODdG8P8">youtube</a></li>
<li><a href="https://1drv.ms/b/s!AqLmU5b8zhHEoxJ8hlmSHS_KEo1D?e=eWLsnl">教材</a></li>
</ul>
</li>
<li>The Containers and Cloud-Native Roadshow Dev Track - Module 4
<ul>
<li><a href="https://www.bilibili.com/read/cv6889572">bilibili</a></li>
<li><a href="https://www.ixigua.com/6850338543265382915">xigua</a></li>
<li><a href="https://www.youtube.com/watch?v=bSFIDNVJ4hc&amp;list=PLRL8GgzNL9SjIYb-08yjJt0r1JdUIz4yC">youtube</a></li>
<li><a href="https://1drv.ms/b/s!AqLmU5b8zhHEoxQ07qUqKv15U3nT?e=jhBexg">教材</a></li>
</ul>
</li>
</ul>
<p>安装过程视频</p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1CZ4y1u7Nf/">bilibili</a></li>
<li><a href="https://www.ixigua.com/6853055649233043979/">xigua</a></li>
<li><a href="https://youtu.be/Arzo0slgD5I">youtube</a></li>
</ul>
<p>不过 upstream 的 CCN 是基于 rh demo system 的，必须在线，这里就做了一个离线的版本，供给客户离线使用。</p>
<h2 id="离线部署架构描述"><a class="header" href="#离线部署架构描述">离线部署架构描述</a></h2>
<p>本次CCN离线，是基于ocp 4.4.7制作。一共有4个module。</p>
<p>做CCN的离线，主要有以下3部分工作</p>
<ul>
<li>github 离线</li>
<li>maven, npm 离线</li>
<li>需要的镜像离线</li>
</ul>
<p>在实验室的部署架构如下，供参考：</p>
<p><img src="ocp4/4.4/./4.4.ccn.devops.deploy.arch.drawio.svg" alt="" /></p>
<p>可以看到，与标准的部署架构没什么区别，就是在helper节点上面，加了gogs, nexus。</p>
<h2 id="安装介质下载"><a class="header" href="#安装介质下载">安装介质下载</a></h2>
<p>请到如下的链接，下载安装介质，注意，这个安装介质是基于ocp 4.4.7 制作。</p>
<ul>
<li>链接: https://pan.baidu.com/s/1f3EcbojFss5cDDQBPBzA-A  密码: 1jun</li>
</ul>
<p>由于上传的时候，安装5G大小切分，下载以后，合并使用如下的命令范例：</p>
<pre><code class="language-bash">cat registry.?? &gt; registry.tgz
</code></pre>
<p>百度盘上还会有补丁文件，比如，当有一个 agnosticd.zip 文件时, 这个就是补丁文件，上传到helper上，替换ocp4.tgz解压缩出来的同名文件即可。</p>
<h2 id="教材修订"><a class="header" href="#教材修订">教材修订</a></h2>
<p>教材根据上游的项目做了修订，主要是源代码，为了应对纯离线环境，做了小的修改。如果在教学现场，发现有步骤做不下去，多半是因为离线环境的问题，请参考教学视频录像，里面会有如何绕过离线环境问题的技巧。</p>
<h2 id="基础ocp44环境的部署细节"><a class="header" href="#基础ocp44环境的部署细节">基础ocp4.4环境的部署细节</a></h2>
<ul>
<li>按照离线的方法安装ocp4，里面要特别注意要有这些安装细节
<ul>
<li>打上离线registries.conf的补丁</li>
<li>打上local image registry ca的补丁</li>
<li>配置image registry</li>
<li>配置sample operator，并打上image stream的补丁</li>
<li>部署离线operator hub</li>
</ul>
</li>
</ul>
<h2 id="ccn-for-ocp-44-安装步骤"><a class="header" href="#ccn-for-ocp-44-安装步骤">ccn for ocp-4.4 安装步骤</a></h2>
<p>建议用独立的ocp4集群来安装ccn教材，因为ccn教材会全局的激活多个operator，这些operator也许对集群中的其他环境有影响。</p>
<pre><code class="language-bash"># on helper
# deploy gogs
export LOCAL_REG='registry.redhat.ren:5443/'
# export LOCAL_REG=''
# gogs_var_date='2020-07-06'
podman stop gogs
podman rm -fv gogs
cd /data/ccn
rm -rf /data/ccn/gogs
podman run -d --name gogs-fs --entrypoint &quot;tail&quot; ${LOCAL_REG}docker.io/wangzheng422/gogs-fs:2020-07-17-1412 -f /dev/null
podman cp gogs-fs:/gogs.tgz /data/ccn/
tar zxf gogs.tgz
podman rm -fv gogs-fs

# change /data/ccn/gogs/resolv.conf to fit your env
# change /data/ccn/gogs/gogs/conf/app.ini to fit your env

# generally, tag latest works
podman run -d --name=gogs -p 10022:22 -p 10080:3000 -v /data/ccn/gogs:/data:Z -v /data/ccn/gogs/resolv.conf:/etc/resolv.conf:Z ${LOCAL_REG}docker.io/gogs/gogs
# for those not using provided source, try a specific tag.
podman run -d --name=gogs -p 10022:22 -p 10080:3000 -v /data/ccn/gogs:/data:Z -v /data/ccn/gogs/resolv.conf:/etc/resolv.conf:Z ${LOCAL_REG}docker.io/gogs/gogs:0.12.3

# restore if you need.
podman stop gogs
podman rm -fv gogs

# deploy nexus
mkdir -p /data/ccn/nexus
cd /data/ccn
rm -rf /data/ccn/nexus
podman run -d --name nexus-fs --entrypoint &quot;tail&quot; ${LOCAL_REG}docker.io/wangzheng422/nexus-fs:2020-07-20-0320 -f /dev/null
podman cp nexus-fs:/nexus.tgz /data/ccn/
tar zxf nexus.tgz ./
podman rm -fv nexus-fs

chown -R 200:root /data/ccn/nexus

# generally, tag latest works
podman run -d -p 8081:8081 -it --name nexus -v /data/ccn/nexus:/nexus-data:Z ${LOCAL_REG}docker.io/sonatype/nexus3:latest
# for those not using provided source, try a specific tag.
podman run -d -p 8081:8081 -it --name nexus -v /data/ccn/nexus:/nexus-data:Z ${LOCAL_REG}docker.io/sonatype/nexus3:3.26.1


# restore if you need.
podman stop nexus
podman rm -fv nexus

# deploy etherpad
mkdir -p /data/ccn/etherpad
chown -R 5001 /data/ccn/etherpad

podman run -d -p 9001:9001 -it --name etherpad -v /data/ccn/etherpad:/opt/etherpad-lite/var:z ${LOCAL_REG}docker.io/etherpad/etherpad:latest

# restore if you need.
podman stop etherpad
podman rm -fv etherpad

# agnosticd on helper

mkdir -p /data/pip3
cd /data/pip3
podman create --name swap registry.redhat.ren:5443/docker.io/wangzheng422/base-fs:pip3-whl-2020-07-05 ls
podman cp swap:/wheelhouse.tar.gz - &gt; wheelhouse.tar.gz
tar vxf wheelhouse.tar.gz
podman rm -fv swap

pip3 install --user --upgrade -r wheelhouse/requirements.txt --no-index --find-links wheelhouse

# yum downgrade ansible-2.8.12-1.el7ae

# 安装ccn环境的参数
oc login -u kubeadmin
# oc login -u system:admin
# TARGET_HOST=&quot;bastion.rhte-b5c8.openshiftworkshop.com&quot;
OCP_USERNAME=&quot;system:admin&quot;
WORKLOAD=&quot;ocp4-workload-ccnrd&quot;
GUID=b5c8
USER_COUNT=2
MODULE_TYPE=&quot;m1;m2;m3;m4&quot;
SSH_KEY=~/.ssh/id_rsa
WZH_SUBDOMIN_BASE=base.ocp4.redhat.ren
WZH_REGISTRY_SERVER=registry.redhat.ren:5443
WZH_GOGS_SERVER=gogs.redhat.ren:10080

# create users
BASE_DIR=&quot;/root/ocp4&quot;
mkdir -p ${BASE_DIR}
cd ${BASE_DIR}
/bin/rm -f ${BASE_DIR}/htpasswd
touch ${BASE_DIR}/htpasswd

for i in $(seq 1 $USER_COUNT)
do 
    htpasswd -Bb ${BASE_DIR}/htpasswd user${i} redhat
done

oc create secret generic htpasswd --from-file=${BASE_DIR}/htpasswd -n openshift-config

oc apply -f - &lt;&lt;EOF
apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  name: cluster
spec:
  identityProviders:
  - name: Local Password
    mappingMethod: claim
    type: HTPasswd
    htpasswd:
      fileData:
        name: htpasswd
EOF

# oc delete secret htpasswd -n openshift-config

# 以下是安装步骤
# a TARGET_HOST is specified in the command line, without using an inventory file
cd /root/ocp4/agnosticd/ansible
ansible-playbook -i localhost, ./configs/ocp-workloads/ocp-workload.yml \
    -e&quot;ansible_ssh_private_key_file=${SSH_KEY}&quot; \
    -e&quot;ansible_user=root&quot; \
    -e&quot;ocp_username=${OCP_USERNAME}&quot; \
    -e&quot;ocp_workload=${WORKLOAD}&quot; \
    -e&quot;silent=False&quot; \
    -e&quot;guid=${GUID}&quot; \
    -e&quot;num_users=${USER_COUNT}&quot; \
    -e&quot;user_count=${USER_COUNT}&quot; \
    -e&quot;module_type=${MODULE_TYPE}&quot; \
    -e&quot;wzh_registry_server=${WZH_REGISTRY_SERVER}&quot; \
    -e&quot;wzh_gogs_server=${WZH_GOGS_SERVER}&quot; \
    -e&quot;ansible_python_interpreter=/usr/bin/python3&quot; \
    -e&quot;subdomain_base=${WZH_SUBDOMIN_BASE}&quot; \
    -v \
    -e&quot;ACTION=create&quot;

# 由于实验环境里面的演示网站，会用到一些在线的静态文件，如果客户端浏览器不能联网
# 或者不能沟通&quot;快速&quot;上网，那么需要给这些在线资源做dns解析，解析到平台的router上来
# 离线的安装介质里面，有static-html，用来提供这些静态文件服务。
# at.alicdn.com
# maxcdn.bootstrapcdn.com
# cdnjs.cloudflare.com
# ajax.googleapis.com
# code.jquery.com

# 以下是删除ccn的步骤，注意大部分的operator不会删除。
# a TARGET_HOST is specified in the command line, without using an inventory file
ansible-playbook -i localhost, ./configs/ocp-workloads/ocp-workload.yml \
    -e&quot;ansible_ssh_private_key_file=${SSH_KEY}&quot; \
    -e&quot;ansible_user=root&quot; \
    -e&quot;ocp_username=${OCP_USERNAME}&quot; \
    -e&quot;ocp_workload=${WORKLOAD}&quot; \
    -e&quot;silent=False&quot; \
    -e&quot;guid=${GUID}&quot; \
    -e&quot;num_users=${USER_COUNT}&quot; \
    -e&quot;user_count=${USER_COUNT}&quot; \
    -e&quot;module_type=${MODULE_TYPE}&quot; \
    -e&quot;wzh_registry_server=${WZH_REGISTRY_SERVER}&quot; \
    -e&quot;wzh_gogs_server=${WZH_GOGS_SERVER}&quot; \
    -e&quot;ansible_python_interpreter=/usr/bin/python3&quot; \
    -e&quot;subdomain_base=${WZH_SUBDOMIN_BASE}&quot; \
    -v \
    -e&quot;ACTION=remove&quot;


</code></pre>
<h2 id="其他备忘"><a class="header" href="#其他备忘">其他备忘</a></h2>
<pre><code class="language-bash">yum install -y wget jq

# Keycloak credentials: admin / 2kBdjDwcZK94
# STACK_ID: stacksq1xbet4os1uioep

</code></pre>
<p>manully patch image stream</p>
<ul>
<li>jenkins:2 to registry.redhat.ren/ocp4/openshift4@sha256:*****</li>
<li>jenkins:latest to registry.redhat.ren/ocp4/openshift4@sha256:*****</li>
</ul>
<p>tips</p>
<pre><code class="language-bash">oc get istio-io -n opentlc-mgr-tutorial

oc new-build -i openshift/redhat-openjdk18-openshift:1.5 --binary --name=inventory-quarkus -l app=inventory-quarkus

npm run nodeshift --  --dockerImage=registry.redhat.ren:5443/docker.io/wangzheng422/cloudnative-workspaces-quarkus  --imageTag=nodejs-10-2020-07-16-2155

</code></pre>
<p>todo</p>
<ul>
<li>PPT</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="离线ccn-containered-cloud-native-制作"><a class="header" href="#离线ccn-containered-cloud-native-制作">离线ccn, containered cloud native 制作</a></h1>
<p>基本思路</p>
<ul>
<li>需要一个离线的github
<ul>
<li>目前看，gogs没有体现在离线部署脚本中。</li>
<li>gogs集群外部署，不外置数据库。以后在考虑如何集群内部署，如何pv import</li>
<li>研究gogs api，批量创建用户和project</li>
</ul>
</li>
<li>需要一个maven的离线proxy
<ul>
<li>目前看，没有包含在离线脚本中，但是crw里面有个配置，指向了离线proxy，似乎好做。</li>
<li>nexus集群外部署.</li>
</ul>
</li>
<li>需要各种镜像
<ul>
<li>目前看，用的大多是image stream，反而好做</li>
</ul>
</li>
</ul>
<p>additional need:</p>
<ul>
<li>maven repository cache</li>
<li>github clone site
<ul>
<li>https://github.com/wangzheng422/cloud-native-workshop-v2m1-guides</li>
<li>https://github.com/wangzheng422/cloud-native-workshop-v2m2-guides</li>
<li>https://github.com/wangzheng422/cloud-native-workshop-v2m3-guides</li>
<li>https://github.com/RedHat-Middleware-Workshops/cloud-native-workshop-v2m4-guides</li>
<li>https://github.com/wangzheng422/cloud-native-workshop-v2-infra
<ul>
<li>branch: dev-ocp-4.2</li>
</ul>
</li>
<li>https://github.com/wangzheng422/cloud-native-workshop-v2m1-labs</li>
<li>https://github.com/wangzheng422/cloud-native-workshop-v2m2-labs</li>
<li>https://github.com/wangzheng422/cloud-native-workshop-v2m3-labs</li>
<li>https://github.com/RedHat-Middleware-Workshops/cloud-native-workshop-v2m4-labs</li>
</ul>
</li>
</ul>
<p>image need:</p>
<ul>
<li>gitlab/gitlab-ce:latest</li>
<li>quay.io/osevg/workshopper</li>
<li>quay.io/openshiftlabs/rhamt-web-openshift-messaging-executor:4.2.1.Final</li>
<li>quay.io/openshiftlabs/rhamt-web-openshift:4.2.1.Final</li>
<li>registry.redhat.io/openshift-service-mesh/istio-rhel8-operator:1.0.3</li>
<li>is: jenkins:2 from ocp 4.2 install </li>
<li>is: quarkus-stack:1.3 quay.io/openshiftlabs/cloudnative-workspaces-quarkus:1.3 to change .m2/settings.xml to add my mirror</li>
</ul>
<p>reference:</p>
<ul>
<li>https://github.com/RedHat-Middleware-Workshops/cloud-native-workshop-v2-infra/tree/ocp-3.11 , we use ocp-4.2 branch right now.</li>
</ul>
<p>my upstream repository</p>
<ul>
<li>https://github.com/wangzheng422/cloud-native-workshop-v2-infra</li>
<li>quay.io/wangzheng422/gogs-fs</li>
<li>quay.io/wangzheng422/nexus-fs</li>
</ul>
<h2 id="build-github-clone-site-using-gogs"><a class="header" href="#build-github-clone-site-using-gogs">build github clone site, using gogs，</a></h2>
<p>似乎gogs并没有在离线部署脚本中</p>
<pre><code class="language-bash"># http://gogs.redhat.ren:10080/

yum install firewalld
systemctl enable firewalld
systemctl start firewalld

yum -y install podman pigz skopeo buildah

podman stop gogs || true
podman rm -fv gogs || true
podman stop nexus || true
podman rm -fv nexus || true
podman stop etherpad || true
podman rm -fv etherpad || true

podman image prune -a

cd /data/ccn
rm -rf /data/ccn/gogs
podman run -d --name gogs-fs --entrypoint &quot;tail&quot; docker.io/wangzheng422/gogs-fs:init -f /dev/null
podman cp gogs-fs:/gogs.tgz /data/ccn/
tar zxf gogs.tgz
podman rm -fv gogs-fs

firewall-cmd --permanent --add-port=10080/tcp
firewall-cmd --reload
firewall-cmd --list-all

podman run -d --name=gogs -p 10022:22 -p 10080:3000 -v /data/ccn/gogs:/data:Z docker.io/gogs/gogs

# Custom config '/data/ccn/gogs/gogs/conf/app.ini'
# find the access key in pwd file
export ACCESS_KEY=&quot;&quot;

# curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X DELETE http://gogs.redhat.ren:10080/api/v1/repos/root/cloud-native-workshop-v2m1-guides

# curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X DELETE http://gogs.redhat.ren:10080/api/v1/repos/root/cloud-native-workshop-v2m2-guides

# curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X DELETE http://gogs.redhat.ren:10080/api/v1/repos/root/cloud-native-workshop-v2m3-guides

# curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X DELETE http://gogs.redhat.ren:10080/api/v1/repos/root/cloud-native-workshop-v2m4-guides

# curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X DELETE http://gogs.redhat.ren:10080/api/v1/repos/root/cloud-native-workshop-v2m1-labs

# curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X DELETE http://gogs.redhat.ren:10080/api/v1/repos/root/cloud-native-workshop-v2m2-labs

# curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X DELETE http://gogs.redhat.ren:10080/api/v1/repos/root/cloud-native-workshop-v2m3-labs

# curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X DELETE http://gogs.redhat.ren:10080/api/v1/repos/root/cloud-native-workshop-v2m4-labs

curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X POST http://gogs.redhat.ren:10080/api/v1/repos/migrate \
        -H &quot;Content-Type: application/json&quot; \
        -d '{&quot;clone_addr&quot;: &quot;'&quot;https://github.com/wangzheng422/cloud-native-workshop-v2m1-guides.git&quot;'&quot;, &quot;uid&quot;: '&quot;1&quot;', &quot;repo_name&quot;: &quot;'&quot;cloud-native-workshop-v2m1-guides&quot;'&quot; }' 

curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X POST http://gogs.redhat.ren:10080/api/v1/repos/migrate \
        -H &quot;Content-Type: application/json&quot; \
        -d '{&quot;clone_addr&quot;: &quot;'&quot;https://github.com/wangzheng422/cloud-native-workshop-v2m2-guides.git&quot;'&quot;, &quot;uid&quot;: '&quot;1&quot;', &quot;repo_name&quot;: &quot;'&quot;cloud-native-workshop-v2m2-guides&quot;'&quot; }' 

curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X POST http://gogs.redhat.ren:10080/api/v1/repos/migrate \
        -H &quot;Content-Type: application/json&quot; \
        -d '{&quot;clone_addr&quot;: &quot;'&quot;https://github.com/wangzheng422/cloud-native-workshop-v2m3-guides.git&quot;'&quot;, &quot;uid&quot;: '&quot;1&quot;', &quot;repo_name&quot;: &quot;'&quot;cloud-native-workshop-v2m3-guides&quot;'&quot; }' 

curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X POST http://gogs.redhat.ren:10080/api/v1/repos/migrate \
        -H &quot;Content-Type: application/json&quot; \
        -d '{&quot;clone_addr&quot;: &quot;'&quot;https://github.com/wangzheng422/cloud-native-workshop-v2m4-guides.git&quot;'&quot;, &quot;uid&quot;: '&quot;1&quot;', &quot;repo_name&quot;: &quot;'&quot;cloud-native-workshop-v2m4-guides&quot;'&quot; }' 

curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X POST http://gogs.redhat.ren:10080/api/v1/repos/migrate \
        -H &quot;Content-Type: application/json&quot; \
        -d '{&quot;clone_addr&quot;: &quot;'&quot;https://github.com/wangzheng422/cloud-native-workshop-v2m1-labs.git&quot;'&quot;, &quot;uid&quot;: '&quot;1&quot;', &quot;repo_name&quot;: &quot;'&quot;cloud-native-workshop-v2m1-labs&quot;'&quot; }' 

curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X POST http://gogs.redhat.ren:10080/api/v1/repos/migrate \
        -H &quot;Content-Type: application/json&quot; \
        -d '{&quot;clone_addr&quot;: &quot;'&quot;https://github.com/wangzheng422/cloud-native-workshop-v2m2-labs.git&quot;'&quot;, &quot;uid&quot;: '&quot;1&quot;', &quot;repo_name&quot;: &quot;'&quot;cloud-native-workshop-v2m2-labs&quot;'&quot; }' 

curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X POST http://gogs.redhat.ren:10080/api/v1/repos/migrate \
        -H &quot;Content-Type: application/json&quot; \
        -d '{&quot;clone_addr&quot;: &quot;'&quot;https://github.com/wangzheng422/cloud-native-workshop-v2m3-labs.git&quot;'&quot;, &quot;uid&quot;: '&quot;1&quot;', &quot;repo_name&quot;: &quot;'&quot;cloud-native-workshop-v2m3-labs&quot;'&quot; }' 

curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X POST http://gogs.redhat.ren:10080/api/v1/repos/migrate \
        -H &quot;Content-Type: application/json&quot; \
        -d '{&quot;clone_addr&quot;: &quot;'&quot;https://github.com/wangzheng422/cloud-native-workshop-v2m4-labs.git&quot;'&quot;, &quot;uid&quot;: '&quot;1&quot;', &quot;repo_name&quot;: &quot;'&quot;cloud-native-workshop-v2m4-labs&quot;'&quot; }' 

curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X POST http://gogs.redhat.ren:10080/api/v1/repos/migrate \
        -H &quot;Content-Type: application/json&quot; \
        -d '{&quot;clone_addr&quot;: &quot;'&quot;https://github.com/spring-projects/spring-petclinic.git&quot;'&quot;, &quot;uid&quot;: '&quot;1&quot;', &quot;repo_name&quot;: &quot;'&quot;spring-petclinic&quot;'&quot; }' 


podman logs -f gogs

podman stop gogs
podman rm -fv gogs

# bash demo.env.build.sh
cd /data/ccn

var_date=$(date '+%Y-%m-%d-%H%M')
echo $var_date

podman stop gogs
podman rm -fv gogs

tar cf - ./gogs | pigz -c &gt; gogs.tgz
buildah from --name onbuild-container docker.io/library/centos:centos7
buildah copy onbuild-container gogs.tgz /
buildah umount onbuild-container 
buildah commit --rm --format=docker onbuild-container docker.io/wangzheng422/gogs-fs:$var_date
# buildah rm onbuild-container
buildah push docker.io/wangzheng422/gogs-fs:$var_date
echo &quot;docker.io/wangzheng422/gogs-fs:$var_date&quot;


</code></pre>
<h2 id="build-maven-repository-cache"><a class="header" href="#build-maven-repository-cache">build maven repository cache</a></h2>
<pre><code class="language-bash"># http://nexus.redhat.ren:8081
mkdir -p cd /data/ccn
cd /data/ccn
rm -rf /data/ccn/nexus
podman run -d --name nexus-fs --entrypoint &quot;tail&quot; docker.io/wangzheng422/nexus-fs:2020-10-25-0919 -f /dev/null
podman cp nexus-fs:/nexus.tgz /data/ccn/
tar zxf nexus.tgz ./
podman rm -fv nexus-fs

podman run -d -p 8081:8081 -it --name nexus -v /data/ccn/nexus:/nexus-data:Z docker.io/sonatype/nexus3:3.26.1


## change code ready workspace
# change maven settings.xml for maven proxy

var_date=$(date '+%Y-%m-%d-%H%M')
echo $var_date

# on vultr init stack image
# mkdir -p /data/ccn/workspaces
# cd /data/ccn/workspaces
# # wget -O settings.xml https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.4/ccn/settings.xml
# wget -O settings.xml https://raw.githubusercontent.com/wangzheng422/agnosticd/wzh-dev/ansible/roles/ocp4-workload-ccnrd/files/settings.xml
# wget -O .npmrc https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.4/ccn/.npmrc
# wget -O stack.Dockerfile https://raw.githubusercontent.com/wangzheng422/agnosticd/wzh-dev/ansible/roles/ocp4-workload-ccnrd/files/stack.Dockerfile

# buildah bud --squash --format=docker -t docker.io/wangzheng422/cloudnative-workspaces-quarkus:init-2.1 -f stack.Dockerfile .

# buildah push docker.io/wangzheng422/cloudnative-workspaces-quarkus:init-2.1

# on vultr update stack update
var_date=$(date '+%Y-%m-%d-%H%M')
echo $var_date

mkdir -p /data/ccn/workspaces
cd /data/ccn/workspaces
# /bin/cp -f /data/order-service.tgz ./
wget -O settings.xml https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.4/ccn/settings.xml
wget -O .npmrc https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.4/ccn/.npmrc
wget -O .bowerrc https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.4/ccn/.bowerrc
wget --no-check-certificate --no-cache --no-cookies -O stack.Dockerfile https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.4/ccn/stack.dev.Dockerfile

buildah bud --squash --format=docker -t docker.io/wangzheng422/cloudnative-workspaces-quarkus:$var_date -f stack.Dockerfile .

buildah push docker.io/wangzheng422/cloudnative-workspaces-quarkus:$var_date

# on site stack update
buildah from --name onbuild-container registry.redhat.ren:5443/docker.io/wangzheng422/cloudnative-workspaces-quarkus:2020-07-08-1594213447
buildah run onbuild-container /bin/rm -rf /tmp/*
buildah umount onbuild-container 
buildah commit --rm --squash --format=docker onbuild-container registry.redhat.ren:5443/docker.io/wangzheng422/cloudnative-workspaces-quarkus:$var_date
# buildah rm onbuild-container
buildah push registry.redhat.ren:5443/docker.io/wangzheng422/cloudnative-workspaces-quarkus:$var_date
echo &quot;registry.redhat.ren:5443/docker.io/wangzheng422/cloudnative-workspaces-quarkus:$var_date&quot;


# get nexus fs
podman stop nexus
podman rm -fv nexus

cd /data/ccn

tar cf - ./nexus | pigz -c &gt; nexus.tgz 
buildah from --name onbuild-container docker.io/library/centos:centos7
buildah copy onbuild-container nexus.tgz /
buildah umount onbuild-container 
buildah commit --rm --format=docker onbuild-container docker.io/wangzheng422/nexus-fs:$var_date
# buildah rm onbuild-container
buildah push docker.io/wangzheng422/nexus-fs:$var_date
echo &quot;docker.io/wangzheng422/nexus-fs:$var_date&quot;
# docker.io/wangzheng422/nexus-fs:2020-10-25-0919

</code></pre>
<h2 id="nodejs-image"><a class="header" href="#nodejs-image">nodejs image</a></h2>
<pre><code class="language-bash">
# on vultr update stack update
var_date=$(date '+%Y-%m-%d-%H%M')
echo $var_date

mkdir -p /data/ccn/workspaces
cd /data/ccn/workspaces
# /bin/cp -f /data/order-service.tgz ./
wget -O settings.xml https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.4/ccn/settings.xml
wget -O .npmrc https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.4/ccn/.npmrc
wget --no-check-certificate --no-cache --no-cookies -O stack.Dockerfile https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.4/ccn/nodejs-10.Dockerfile

buildah bud --format=docker -t docker.io/wangzheng422/cloudnative-workspaces-quarkus:nodejs-10-$var_date -f stack.Dockerfile .

buildah push docker.io/wangzheng422/cloudnative-workspaces-quarkus:nodejs-10-$var_date


</code></pre>
<h2 id="build-static-html-file"><a class="header" href="#build-static-html-file">build static html file</a></h2>
<pre><code class="language-bash">
# get source to image 
# https://github.com/openshift/source-to-image
wget -O source-to-image.tgz https://github.com/openshift/source-to-image/releases/download/v1.3.0/source-to-image-v1.3.0-eed2850f-linux-amd64.tar.gz
tar zvxf source-to-image.tgz
mv s2i /usr/local/bin/

var_date=$(date '+%Y-%m-%d-%H%M')
echo $var_date

rm -rf /data/ccn/static-html
mkdir -p /data/ccn/static-html/files
cd /data/ccn/static-html/files

mkdir -p bootstrap/3.3.5/css/
wget -O bootstrap/3.3.5/css/bootstrap.min.css https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css
wget -O bootstrap/3.3.5/css/bootstrap-theme.min.css https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css

mkdir -p bootstrap/3.3.5/js/
wget -O bootstrap/3.3.5/js/bootstrap.min.js https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js

mkdir -p ajax/libs/jquery/2.1.4/
wget -O ajax/libs/jquery/2.1.4/jquery.min.js https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js

mkdir -p bootstrap/3.3.5/fonts/
wget -O bootstrap/3.3.5/fonts/glyphicons-halflings-regular.woff2  https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.4/ccn/glyphicons-halflings-regular.woff2
wget -O bootstrap/3.3.5/fonts/glyphicons-halflings-regular.woff https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.4/ccn/glyphicons-halflings-regular.woff
wget -O bootstrap/3.3.5/fonts/glyphicons-halflings-regular.ttf https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.4/ccn/glyphicons-halflings-regular.ttf

mkdir -p t/
wget -O t/font_148784_v4ggb6wrjmkotj4i.woff      https://at.alicdn.com/t/font_148784_v4ggb6wrjmkotj4i.woff
wget -O t/font_148784_v4ggb6wrjmkotj4i.ttf       https://at.alicdn.com/t/font_148784_v4ggb6wrjmkotj4i.ttf

mkdir -p bootstrap/4.0.0-beta/css/
wget -O bootstrap/4.0.0-beta/css/bootstrap.min.css       https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css

mkdir -p ajax/libs/patternfly/3.24.0/css/
wget -O ajax/libs/patternfly/3.24.0/css/patternfly.min.css      https://cdnjs.cloudflare.com/ajax/libs/patternfly/3.24.0/css/patternfly.min.css
wget -O ajax/libs/patternfly/3.24.0/css/patternfly-additions.min.css    https://cdnjs.cloudflare.com/ajax/libs/patternfly/3.24.0/css/patternfly-additions.min.css

wget -O jquery-3.2.1.min.js     https://code.jquery.com/jquery-3.2.1.min.js

mkdir -p ajax/libs/jquery-timeago/1.6.1/
wget -O ajax/libs/jquery-timeago/1.6.1/jquery.timeago.min.js    https://cdnjs.cloudflare.com/ajax/libs/jquery-timeago/1.6.1/jquery.timeago.min.js

mkdir -p ajax/libs/angularjs/1.4.8/
wget -O ajax/libs/angularjs/1.4.8/angular.min.js        https://ajax.googleapis.com/ajax/libs/angularjs/1.4.8/angular.min.js

cd /data/ccn/static-html/

s2i build --rm  files/  registry.redhat.io/rhscl/nginx-114-rhel7:latest  nginx-sample-app

docker tag nginx-sample-app docker.io/wangzheng422/cloudnative-workspaces-quarkus:swap-$var_date
docker push docker.io/wangzheng422/cloudnative-workspaces-quarkus:swap-$var_date
echo docker.io/wangzheng422/cloudnative-workspaces-quarkus:swap-$var_date

wget -O mime.types https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.4/ccn/mime.types
wget -O nginx.conf https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.4/ccn/nginx.conf

cat &lt;&lt; EOF &gt; nginx.Dockerfile
FROM docker.io/wangzheng422/cloudnative-workspaces-quarkus:swap-$var_date

USER root
COPY mime.types /etc/nginx/
COPY nginx.conf /etc/nginx/

USER 1001
EOF

buildah bud --format=docker -t docker.io/wangzheng422/cloudnative-workspaces-quarkus:static-html-$var_date -f nginx.Dockerfile .

buildah push docker.io/wangzheng422/cloudnative-workspaces-quarkus:static-html-$var_date
echo &quot;docker.io/wangzheng422/cloudnative-workspaces-quarkus:static-html-$var_date&quot;


docker image prune -f
podman image prune -a

# oc -n labs-infra create route edge static-html-0 --service=static-html --hostname=maxcdn.bootstrapcdn.com 
# oc -n labs-infra create route edge static-html-1 --service=static-html   --hostname=ajax.googleapis.com 
# oc -n labs-infra create route edge static-html-2 --service=static-html   --hostname=at.alicdn.com
# oc -n labs-infra create route edge static-html-3 --service=static-html   --hostname=cdnjs.cloudflare.com
# oc -n labs-infra create route edge static-html-4 --service=static-html   --hostname=code.jquery.com

</code></pre>
<h2 id="pip-for-agnosticd"><a class="header" href="#pip-for-agnosticd">pip for agnosticd</a></h2>
<pre><code class="language-bash"># on vultr perpare pip
# https://www.linuxtechi.com/use-ansible-galaxy-roles-ansible-playbook/
# https://docs.ansible.com/ansible/latest/scenario_guides/guide_kubernetes.html
# https://stackoverflow.com/questions/11091623/how-to-install-packages-offline
# https://www.activestate.com/resources/quick-reads/how-to-update-all-python-packages/
# yum install -y python2-pip
mkdir -p /data/pip3
cd /data/pip3
# pip install --upgrade pip
pip3 install --user --upgrade kubernetes openshift requests
pip3 freeze &gt; requirements.txt
pip3 install -r requirements.txt --upgrade
mkdir -p wheelhouse
pip2 download -r requirements.txt -d wheelhouse
/bin/cp -f requirements.txt wheelhouse/
tar -zcf wheelhouse.tar.gz wheelhouse


var_date=$(date '+%Y-%m-%d')
echo $var_date

buildah from --name onbuild-container scratch
buildah copy onbuild-container wheelhouse.tar.gz /
buildah umount onbuild-container 
buildah commit --rm --format=docker onbuild-container docker.io/wangzheng422/base-fs:pip3-whl-$var_date
# buildah rm onbuild-container
buildah push docker.io/wangzheng422/base-fs:pip3-whl-$var_date
echo &quot;docker.io/wangzheng422/base-fs:pip3-whl-$var_date&quot;

</code></pre>
<h2 id="labs-sync"><a class="header" href="#labs-sync">labs sync</a></h2>
<pre><code class="language-bash">
rsync -e ssh --info=progress2 -P --delete -arz bastion.fd21.example.opentlc.com:/data/ccn/nexus/  /data/ccn/nexus/

rsync -e ssh -P --delete -arz root@bastion.fd21.example.opentlc.com:/data/ccn/nexus/  ./nexus/ 

rsync -e ssh -P --delete -arz  ./nexus/  root@192.168.7.11:/data/ccn/nexus/   

chown -R 200:root nexus

rsync -e ssh --info=progress2 -P --delete -arz   192.168.252.11:/data/ccn/nexus/   ./nexus/   



</code></pre>
<h2 id="other-tips"><a class="header" href="#other-tips">other tips</a></h2>
<h3 id="find-object-blocks-deleting-namespaceproject"><a class="header" href="#find-object-blocks-deleting-namespaceproject">find object blocks deleting namespace/project</a></h3>
<ul>
<li>https://access.redhat.com/solutions/4165791</li>
</ul>
<pre><code class="language-bash">PROJECT_NAME=user1-cloudnativeapps

oc api-resources --verbs=list --namespaced -o name | xargs -n 1 oc get --show-kind --ignore-not-found -n $PROJECT_NAME

oc api-resources --verbs=list --cached --namespaced -o name | xargs -n 1 oc get --show-kind --ignore-not-found -n $PROJECT_NAME


configuration.serving.knative.dev/payment
service.serving.knative.dev/payment
route.serving.knative.dev/payment


</code></pre>
<h3 id="service-mesh--knative"><a class="header" href="#service-mesh--knative">service mesh &amp; knative</a></h3>
<pre><code class="language-bash">oc project istio-system
oc get pod -o json | jq -r '.items[].spec.containers[].image' &gt; tmp.list

oc project istio-operator
oc get pod -o json | jq -r '.items[].spec.containers[].image' &gt;&gt; tmp.list

oc project knative-eventing
oc get pod -o json | jq -r '.items[].spec.containers[].image' &gt;&gt; tmp.list

oc project knative-serving
oc get pod -o json | jq -r '.items[].spec.containers[].image' &gt;&gt; tmp.list

oc project tekton-pipelines
oc get pod -o json | jq -r '.items[].spec.containers[].image' &gt;&gt; tmp.list

oc get pod -o json | jq -r '.items[].spec.initContainers[].image' &gt;&gt; tmp.list

oc project openshift-operators
oc get pod -o json | jq -r '.items[].spec.containers[].image' &gt;&gt; tmp.list



cat tmp.list | sort | uniq

oc project user0-catalog
oc get pod -o json | jq -r '.items[].spec.containers[].image'| sort | uniq 


</code></pre>
<h2 id="try-the-install-shell"><a class="header" href="#try-the-install-shell">try the install shell</a></h2>
<pre><code class="language-bash">cd
git clone https://github.com/wangzheng422/cloud-native-workshop-v2-infra
cd cloud-native-workshop-v2-infra
git fetch origin 
git checkout -b dev-ocp-4.2 origin/dev-ocp-4.2

# in local vm
rsync -e ssh --info=progress2 -P --delete -arz /data/registry-add root@base-pvg.redhat.ren:/data/

# on base-pvg
ansible localhost -m lineinfile -a 'path=/etc/hosts line=&quot;127.0.0.1 registry-add.redhat.ren&quot;'

cat &gt; /etc/dnsmasq.d/origin-upstream-dns.conf &lt;&lt; EOF 
server=10.66.208.137
EOF

systemctl restart dnsmasq

podman run -d --name mirror-registry \
-p 5000:5000 --restart=always \
-v /data/registry-add:/var/lib/registry:z \
-v /etc/crts/:/certs:z \
-e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/redhat.ren.crt \
-e REGISTRY_HTTP_TLS_KEY=/certs/redhat.ren.key \
registry:2


###
skopeo copy docker://docker.io/wangzheng422/gogs-fs:2020-01-01 docker://registry.redhat.ren/docker.io/wangzheng422/gogs-fs:2020-01-01 
skopeo copy docker://docker.io/wangzheng422/nexus-fs:2020-01-01 docker://registry.redhat.ren/docker.io/wangzheng422/nexus-fs:2020-01-01 


# spring.datasource.initialization-mode: always
</code></pre>
<h2 id="tips-1"><a class="header" href="#tips-1">tips</a></h2>
<ul>
<li>spring.datasource.initialization-mode=always</li>
<li>prometheus: [ url ]</li>
</ul>
<h2 id="nodejs"><a class="header" href="#nodejs">nodejs</a></h2>
<pre><code class="language-bash">git clone https://github.com/wangzheng422/cloud-native-workshop-v2m4-labs &amp;&amp; cd cloud-native-workshop-v2m4-labs &amp;&amp; git checkout ocp-4.4 &amp;&amp; cd coolstore-ui

cat &lt;&lt; EOF &gt; Dockerfile
FROM docker.io/wangzheng422/cloudnative-workspaces-quarkus:nodejs-10-2020-07-16-2155

# Add application sources to a directory that the assemble script expects them
# and set permissions so that the container runs without root access
USER 0
ADD . /tmp/src
RUN chown -R 1001:0 /tmp/src
USER 1001

# Install the dependencies
RUN /usr/libexec/s2i/assemble

# Set the default command for the resulting image
CMD /usr/libexec/s2i/run
EOF


cat &lt;&lt; &quot;EOF&quot; &gt; post_install.sh
#!/bin/bash
var_new_domain=&quot;static-html-labs-infra.apps.redhat.container-contest.top&quot;
var_new_domain_enc=$(echo $var_new_domain | sed &quot;s/\./\\\./g&quot;)

# node_modules/.bin/bower install

# grep -rni &quot;at.alicdn.com&quot; *
# grep -rl 'at.alicdn.com' * | xargs sed -i &quot;s/at\.alicdn\.com/$var_new_domain_enc/g&quot; 
grep -rl 'code.jquery.com' * | xargs sed -i &quot;s/code\.jquery\.com/$var_new_domain_enc/g&quot; 

grep -rni &quot;code.jquery.com&quot; * || true
EOF

# change package.json
# change postinsall to the shell
# and try to fix domain issues.

podman build -t node-app .

</code></pre>
<h2 id="以下是弯路-2"><a class="header" href="#以下是弯路-2">以下是弯路</a></h2>
<p>build github clone site, using gitlab</p>
<pre><code class="language-bash">yum -y install podman

rm -rf /data/ccn/gitlab
mkdir -p /data/ccn/gitlab/config
mkdir -p /data/ccn/gitlab/logs
mkdir -p /data/ccn/gitlab/data


# podman run --detach \
#   --hostname local.redhat.ren \
#   --env GITLAB_OMNIBUS_CONFIG=&quot;external_url 'http://local.redhat.ren:7080/'; gitlab_rails['lfs_enabled'] = true;&quot; \
#   --publish 7443:443 --publish 7080:80 --publish 7022:22 \
#   --name gitlab \
#   --restart always \
#   --volume /data/ocp4/demo/gitlab/config:/etc/gitlab:Z \
#   --volume /data/ocp4/demo/gitlab/logs:/var/log/gitlab:Z \
#   --volume /data/ocp4/demo/gitlab/data:/var/opt/gitlab:Z \
#   gitlab/gitlab-ce:latest

podman run --detach \
  --hostname local.redhat.ren \
  --publish 7443:443 --publish 7080:80 --publish 7022:22 \
  --name gitlab \
  --restart always \
  --volume /data/ccn/gitlab/config:/etc/gitlab:Z \
  --volume /data/ccn/gitlab/logs:/var/log/gitlab:Z \
  --volume /data/ccn/gitlab/data:/var/opt/gitlab:Z \
  gitlab/gitlab-ce:latest

# set default username / password
# root / redhat2019

podman stop gitlab

podman rm -fv gitlab

cd /data/ccn
# tar zcf gitlab.tgz ./gitlab 
cat &lt;&lt; EOF &gt; /data/ccn/gitlab.files.Dockerfile
FROM registry.redhat.io/ubi7/ubi
COPY gitlab /gitlab
EOF
podman build --no-cache -f /data/ccn/gitlab.files.Dockerfile -t quay.io/wangzheng422/gitlab-fs /data/ccn/
podman push quay.io/wangzheng422/gitlab-fs

podman exec -it gitlab update-permissions
podman restart gitlab
podman logs -f gitlab
getfacl /data/ccn/gitlab/

# now we try to use it
rm -rf /data/ccn/gitlab
podman run -d --name gitlab-fs --entrypoint &quot;tail&quot; quay.io/wangzheng422/gitlab-fs -f /dev/null
podman cp gitlab-fs:/gitlab /data/ccn/
podman rm -fv gitlab-fs
# tar zxf gitlab.tgz
# chown -R root: /data/ccn/gitlab/
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="containered-cloud-native-ccn-roadshow-离线部署-1"><a class="header" href="#containered-cloud-native-ccn-roadshow-离线部署-1">containered cloud-native (ccn) roadshow 离线部署</a></h1>
<p>CCN是一个不错的演示openshift之上，ci/cd, cloud-native, istio, serverless的演示教材，教学的内容非常丰富。</p>
<p>第一个模块，着重讲解如何拆分单体应用，以及拆分的应用如何上云。</p>
<p>第二个模块，讲解如何在线debug, 如何监控上云的应用</p>
<p>第三个模块，应用转换到服务网格service mesh/istio架构</p>
<p>第四个模块，应用使用无服务架构serverless/knative架构开发</p>
<p>培训过程视频</p>
<ul>
<li>The Containers and Cloud-Native Roadshow Dev Track - Module 1
<ul>
<li><a href="https://www.bilibili.com/read/cv6713699">bilibili</a></li>
<li><a href="https://www.ixigua.com/6847386067889455630">xigua</a></li>
<li><a href="https://www.youtube.com/playlist?list=PLRL8GgzNL9SgeODjqT1aU8ya95KfwgElv">youtube</a></li>
<li><a href="https://1drv.ms/b/s!AqLmU5b8zhHEoxFp6ef1t5bpS7Dy?e=oi3IKg">教材</a></li>
</ul>
</li>
<li>The Containers and Cloud-Native Roadshow Dev Track - Module 2
<ul>
<li><a href="https://www.bilibili.com/read/cv6868575">bilibili</a></li>
<li><a href="https://www.ixigua.com/6848440375573479949">xigua</a></li>
<li><a href="https://www.youtube.com/watch?v=Kr4dwz2QaeU&amp;list=PLRL8GgzNL9SjG9YZRxKHhCdN5XK32QpOA">youtube</a></li>
<li><a href="https://1drv.ms/b/s!AqLmU5b8zhHEoxOzpffwSNSetYtI?e=ktNJoo">教材</a></li>
</ul>
</li>
<li>The Containers and Cloud-Native Roadshow Dev Track - Module 3
<ul>
<li><a href="https://www.bilibili.com/read/cv6888587">bilibili</a></li>
<li><a href="https://www.ixigua.com/6849151451068006919">xigua</a></li>
<li><a href="https://www.youtube.com/watch?v=rxzr3Tjydzw&amp;list=PLRL8GgzNL9SjBFitrp01FUNmrSODdG8P8">youtube</a></li>
<li><a href="https://1drv.ms/b/s!AqLmU5b8zhHEoxJ8hlmSHS_KEo1D?e=eWLsnl">教材</a></li>
</ul>
</li>
<li>The Containers and Cloud-Native Roadshow Dev Track - Module 4
<ul>
<li><a href="https://www.bilibili.com/read/cv6889572">bilibili</a></li>
<li><a href="https://www.ixigua.com/6850338543265382915">xigua</a></li>
<li><a href="https://www.youtube.com/watch?v=bSFIDNVJ4hc&amp;list=PLRL8GgzNL9SjIYb-08yjJt0r1JdUIz4yC">youtube</a></li>
<li><a href="https://1drv.ms/b/s!AqLmU5b8zhHEoxQ07qUqKv15U3nT?e=jhBexg">教材</a></li>
</ul>
</li>
</ul>
<p>安装过程视频</p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1CZ4y1u7Nf/">bilibili</a></li>
<li><a href="https://www.ixigua.com/6853055649233043979/">xigua</a></li>
<li><a href="https://youtu.be/Arzo0slgD5I">youtube</a></li>
</ul>
<p>不过 upstream 的 CCN 是基于 rh demo system 的，必须在线，这里就做了一个离线的版本，供给客户离线使用。</p>
<h2 id="离线部署架构描述-1"><a class="header" href="#离线部署架构描述-1">离线部署架构描述</a></h2>
<p>本次CCN离线，是基于ocp 4.6.9 制作。一共有4个module。</p>
<p>做CCN的离线，主要有以下3部分工作</p>
<ul>
<li>github 离线</li>
<li>maven, npm 离线</li>
<li>需要的镜像离线</li>
</ul>
<p>在实验室的部署架构如下，供参考：</p>
<p><img src="ocp4/4.6/./4.6.ccn.devops.deploy.arch.drawio.svg" alt="" /></p>
<p>可以看到，与标准的部署架构没什么区别，就是在helper节点上面，加了gogs, nexus。</p>
<h2 id="安装介质下载-1"><a class="header" href="#安装介质下载-1">安装介质下载</a></h2>
<p>请到如下的链接，下载安装介质，注意，这个安装介质是基于ocp 4.6.9 制作。</p>
<p>链接: https://pan.baidu.com/s/1jJU0HLnZMnvCNMNq1OEDxA  密码: uaaw</p>
<p>其中包括如下类型的文件：</p>
<ul>
<li>ocp4.tgz  这个文件包含了iso等安装介质，以及各种安装脚本，全部下载的镜像列表等。需要复制到宿主机，以及工具机上去。</li>
<li>registry.tgz  这个文件也是docker image registry的仓库打包文件。需要先补充镜像的话，按照这里操作: <a href="ocp4/4.6/./4.6.add.image.html">4.6.add.image.md</a></li>
<li>nexus-image.tgz 这个是nexus的镜像仓库打包，集群的镜像proxy指向nexus，由nexus提供镜像的cache</li>
<li>poc.image.tgz 这个是给registry.tgz补充的一些镜像，主要是ccn使用，补充的镜像列表在这里 <a href="ocp4/4.6/./ccn/poc.image.list">poc.image.list</a> ，按照这里操作: <a href="ocp4/4.6/./4.6.add.image.html">4.6.add.image.md</a></li>
</ul>
<p>由于上传的时候，安装5G大小切分，下载以后，合并使用如下的命令范例：</p>
<pre><code class="language-bash">cat registry.?? &gt; registry.tgz
</code></pre>
<p>百度盘上还会有补丁文件，比如，当有一个 agnosticd.zip 文件时, 这个就是补丁文件，上传到helper上，替换ocp4.tgz解压缩出来的同名文件即可。</p>
<h2 id="教材修订-1"><a class="header" href="#教材修订-1">教材修订</a></h2>
<p>教材根据上游的项目做了修订，主要是源代码，为了应对纯离线环境，做了小的修改。如果在教学现场，发现有步骤做不下去，多半是因为离线环境的问题，请参考教学视频录像，里面会有如何绕过离线环境问题的技巧。</p>
<h2 id="基础ocp46环境的部署细节"><a class="header" href="#基础ocp46环境的部署细节">基础ocp4.6环境的部署细节</a></h2>
<ul>
<li>按照离线的方法安装ocp4，里面要特别注意要有这些安装细节
<ul>
<li>部署nexus镜像仓库代理</li>
<li>打上离线registries.conf的补丁，指向nexus</li>
<li>给ingress配置真证书</li>
<li>配置image registry</li>
<li>配置sample operator，并打上image stream的补丁</li>
<li>部署离线operator hub</li>
</ul>
</li>
</ul>
<h2 id="ccn-for-ocp-46-安装步骤"><a class="header" href="#ccn-for-ocp-46-安装步骤">ccn for ocp-4.6 安装步骤</a></h2>
<p>建议用独立的ocp4集群来安装ccn教材，因为ccn教材会全局的激活多个operator，这些operator也许对集群中的其他环境有影响。</p>
<pre><code class="language-bash"># on helper
# deploy gitea
export LOCAL_REG='registry.ocp4.redhat.ren:5443'
# export LOCAL_REG=''
# gogs_var_date='2020-07-06'
podman stop gitea
podman rm -fv gitea

mkdir -p /data/ccn/gitea

cd /data/ccn
podman create --name swap $LOCAL_REG/wangzheng422/gogs-fs:gitea-2020-12-26-1325 ls
podman cp swap:/gitea.tgz /data/ccn/gitea.tgz
podman rm -fv swap
tar zvxf gitea.tgz
rm -f gitea.tgz
chown -R 1000:1000 /data/ccn/gitea

podman run -d --name gitea \
  -v /data/ccn/gitea:/data:Z \
  -e USER_UID=1000 \
  -e USER_GID=1000 \
  -p 10080:3000 \
  -p 10022:22 \
  ${LOCAL_REG}/gitea/gitea:1.13.0

# deploy nexus for maven
mkdir -p /data/ccn/nexus
cd /data/ccn/
podman create --name swap $LOCAL_REG/wangzheng422/nexus-fs:maven-2020-12-25-2024 ls
podman cp swap:/nexus.tgz /data/ccn/nexus.tgz
podman rm -fv swap
tar zvxf nexus.tgz
rm -f nexus.tgz
chown -R 200 /data/ccn/nexus

podman run -d -p 8081:8081 --name nexus -v /data/ccn/nexus:/nexus-data:Z $LOCAL_REG/sonatype/nexus3:3.29.0


# deploy etherpad for notes
mkdir -p /data/ccn/etherpad
chown -R 5001 /data/ccn/etherpad

podman run -d -p 9001:9001 -it --name etherpad -v /data/ccn/etherpad:/opt/etherpad-lite/var:z $LOCAL_REG/etherpad/etherpad:latest

# deploy mta vscode extenstion to helper web server
mkdir -p /data/ccn/vscode
mkdir -p /var/www/html/ccn/
cd /data/ccn/vscode
podman create --name swap $LOCAL_REG/wangzheng422/imgs:mta-vscode-extension.vsix-2020-12-30-1012 ls
podman cp swap:/mta-vscode-extension.vsix /var/www/html/ccn/mta-vscode-extension.vsix
podman cp swap:/logo-eclipseche.svg /var/www/html/ccn/logo-eclipseche.svg
podman rm -fv swap


# agnosticd on helper
mkdir -p /data/pip3
cd /data/pip3
podman create --name swap $LOCAL_REG/wangzheng422/base-fs:pip3-whl-2020-07-05 ls
podman cp swap:/wheelhouse.tar.gz wheelhouse.tar.gz
tar vxf wheelhouse.tar.gz
podman rm -fv swap

pip3 install --user --upgrade -r wheelhouse/requirements.txt --no-index --find-links wheelhouse

# 集群证书
# ccn 环境，高度依赖ingress证书，需要配置一个公网CA签发的真证书，给 *.apps.ocp4.redhat.ren

# install chrome on kvm host
wget https://dl.google.com/linux/direct/google-chrome-stable_current_x86_64.rpm
yum install ./google-chrome-stable_current_*.rpm
google-chrome --no-sandbox --ignore-certificate-errors &amp;

# fix js cache issue
cat &lt;&lt; EOF &gt;&gt; /etc/hosts
127.0.0.1 maxcdn.bootstrapcdn.com ajax.googleapis.com at.alicdn.com cdnjs.cloudflare.com code.jquery.com
EOF

# 安装ccn环境的参数
# oc login -u kubeadmin
oc login -u system:admin
# TARGET_HOST=&quot;bastion.rhte-b5c8.openshiftworkshop.com&quot;
OCP_USERNAME=&quot;system:admin&quot;
WORKLOAD=&quot;ocp4-workload-ccnrd&quot;
GUID=b5c8
USER_COUNT=2
MODULE_TYPE=&quot;m1;m2;m3;m4&quot;
SSH_KEY=~/.ssh/helper_rsa
WZH_SUBDOMIN_BASE=base.ocp4.redhat.ren
WZH_REGISTRY_SERVER=nexus.ocp4.redhat.ren:8083
WZH_GOGS_SERVER=git.ocp4.redhat.ren:10080
WZH_WEB_SERVER=helper.ocp4.redhat.ren:8080

ssh-copy-id -i ~/.ssh/helper_rsa.pub root@localhost

# create users
BASE_DIR=&quot;/data/install&quot;
mkdir -p ${BASE_DIR}
cd ${BASE_DIR}
/bin/rm -f ${BASE_DIR}/htpasswd
touch ${BASE_DIR}/htpasswd

for i in $(seq 1 $USER_COUNT)
do 
    htpasswd -Bb ${BASE_DIR}/htpasswd user${i} redhat
done

oc create secret generic htpasswd --from-file=${BASE_DIR}/htpasswd -n openshift-config

oc apply -f - &lt;&lt;EOF
apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  name: cluster
spec:
  identityProviders:
  - name: HTPassword
    mappingMethod: claim
    type: HTPasswd
    htpasswd:
      fileData:
        name: htpasswd
EOF

# oc delete secret htpasswd -n openshift-config

# 以下是安装步骤
# a TARGET_HOST is specified in the command line, without using an inventory file
oc project default
cd /data/ocp4/agnosticd/ansible
ansible-playbook -i localhost, ./configs/ocp-workloads/ocp-workload.yml \
    -e&quot;ansible_ssh_private_key_file=${SSH_KEY}&quot; \
    -e&quot;ansible_user=root&quot; \
    -e&quot;ocp_username=${OCP_USERNAME}&quot; \
    -e&quot;ocp_workload=${WORKLOAD}&quot; \
    -e&quot;silent=False&quot; \
    -e&quot;guid=${GUID}&quot; \
    -e&quot;num_users=${USER_COUNT}&quot; \
    -e&quot;user_count=${USER_COUNT}&quot; \
    -e&quot;module_type=${MODULE_TYPE}&quot; \
    -e&quot;wzh_registry_server=${WZH_REGISTRY_SERVER}&quot; \
    -e&quot;wzh_gogs_server=${WZH_GOGS_SERVER}&quot; \
    -e&quot;wzh_web_server=${WZH_WEB_SERVER}&quot; \
    -e&quot;ansible_python_interpreter=/usr/bin/python3&quot; \
    -e&quot;subdomain_base=${WZH_SUBDOMIN_BASE}&quot; \
    -v \
    -e&quot;ACTION=create&quot;

# 由于实验环境里面的演示网站，会用到一些在线的静态文件，如果客户端浏览器不能联网
# 或者不能沟通&quot;快速&quot;上网，那么需要给这些在线资源做dns解析，解析到平台的router上来
# 离线的安装介质里面，有static-html，用来提供这些静态文件服务。
# at.alicdn.com
# maxcdn.bootstrapcdn.com
# cdnjs.cloudflare.com
# ajax.googleapis.com
# code.jquery.com

# 以下是删除ccn的步骤，注意大部分的operator不会删除。
# a TARGET_HOST is specified in the command line, without using an inventory file
cd /data/ocp4/agnosticd/ansible
ansible-playbook -i localhost, ./configs/ocp-workloads/ocp-workload.yml \
    -e&quot;ansible_ssh_private_key_file=${SSH_KEY}&quot; \
    -e&quot;ansible_user=root&quot; \
    -e&quot;ocp_username=${OCP_USERNAME}&quot; \
    -e&quot;ocp_workload=${WORKLOAD}&quot; \
    -e&quot;silent=False&quot; \
    -e&quot;guid=${GUID}&quot; \
    -e&quot;num_users=${USER_COUNT}&quot; \
    -e&quot;user_count=${USER_COUNT}&quot; \
    -e&quot;module_type=${MODULE_TYPE}&quot; \
    -e&quot;wzh_registry_server=${WZH_REGISTRY_SERVER}&quot; \
    -e&quot;wzh_gogs_server=${WZH_GOGS_SERVER}&quot; \
    -e&quot;wzh_web_server=${WZH_WEB_SERVER}&quot; \
    -e&quot;ansible_python_interpreter=/usr/bin/python3&quot; \
    -e&quot;subdomain_base=${WZH_SUBDOMIN_BASE}&quot; \
    -v \
    -e&quot;ACTION=remove&quot;


</code></pre>
<h2 id="做练习中需要注意的地方"><a class="header" href="#做练习中需要注意的地方">做练习中需要注意的地方</a></h2>
<pre><code class="language-bash"># git 链接要改成gitea上的地址
# http://git.ocp4.redhat.ren:10080/root/cloud-native-workshop-v2m1-labs.git
# http://git.ocp4.redhat.ren:10080/root/cloud-native-workshop-v2m2-labs.git
# http://git.ocp4.redhat.ren:10080/root/cloud-native-workshop-v2m3-labs.git
# http://git.ocp4.redhat.ren:10080/root/cloud-native-workshop-v2m4-labs.git
# http://git.ocp4.redhat.ren:10080/root/vote-api.git
# http://git.ocp4.redhat.ren:10080/root/vote-ui.git

# oc 命令有引用镜像的地方，都要改成nexus上的地址
oc new-build --docker-image=nexus.ocp4.redhat.ren:8083/ubi8/openjdk-11 --binary --name=catalog-springboot -l app=catalog-springboot

# in module 4, nodeshift编译命令要改一下。
npm run nodeshift --  --dockerImage=nexus.ocp4.redhat.ren:8083/wangzheng422/imgs --imageTag=nodejs-10-wzh-2021-01-05

</code></pre>
<h2 id="其他备忘-1"><a class="header" href="#其他备忘-1">其他备忘</a></h2>
<pre><code class="language-bash">yum install -y wget jq

# Keycloak credentials: admin / 2kBdjDwcZK94
# STACK_ID: stacksq1xbet4os1uioep

</code></pre>
<p>todo</p>
<ul>
<li>PPT</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="离线ccn-containered-cloud-native-制作-1"><a class="header" href="#离线ccn-containered-cloud-native-制作-1">离线ccn, containered cloud native 制作</a></h1>
<p>基本思路</p>
<ul>
<li>需要一个离线的github
<ul>
<li>目前看，gogs没有体现在离线部署脚本中。</li>
<li>gogs集群外部署，不外置数据库。以后在考虑如何集群内部署，如何pv import</li>
<li>研究gogs api，批量创建用户和project</li>
</ul>
</li>
<li>需要一个maven的离线proxy
<ul>
<li>目前看，没有包含在离线脚本中，但是crw里面有个配置，指向了离线proxy，似乎好做。</li>
<li>nexus集群外部署.</li>
</ul>
</li>
<li>需要各种镜像
<ul>
<li>目前看，用的大多是image stream，反而好做</li>
</ul>
</li>
</ul>
<p>additional need:</p>
<ul>
<li>maven repository cache</li>
<li>github clone site
<ul>
<li>https://github.com/wangzheng422/cloud-native-workshop-v2m1-guides</li>
<li>https://github.com/wangzheng422/cloud-native-workshop-v2m2-guides</li>
<li>https://github.com/wangzheng422/cloud-native-workshop-v2m3-guides</li>
<li>https://github.com/RedHat-Middleware-Workshops/cloud-native-workshop-v2m4-guides</li>
<li>https://github.com/wangzheng422/cloud-native-workshop-v2-infra
<ul>
<li>branch: dev-ocp-4.2</li>
</ul>
</li>
<li>https://github.com/wangzheng422/cloud-native-workshop-v2m1-labs</li>
<li>https://github.com/wangzheng422/cloud-native-workshop-v2m2-labs</li>
<li>https://github.com/wangzheng422/cloud-native-workshop-v2m3-labs</li>
<li>https://github.com/RedHat-Middleware-Workshops/cloud-native-workshop-v2m4-labs</li>
</ul>
</li>
</ul>
<p>image need:</p>
<ul>
<li>registry.redhat.io/openshift-service-mesh/istio-rhel8-operator:1.0.3</li>
<li>is: jenkins:2 from ocp 4.2 install </li>
<li>is: quarkus-stack:1.3 quay.io/openshiftlabs/cloudnative-workspaces-quarkus:1.3 to change .m2/settings.xml to add my mirror</li>
</ul>
<p>reference:</p>
<ul>
<li>https://github.com/RedHat-Middleware-Workshops/cloud-native-workshop-v2-infra/tree/ocp-3.11 , we use ocp-4.2 branch right now.</li>
</ul>
<p>my upstream repository</p>
<ul>
<li>quay.io/wangzheng422/gogs-fs</li>
<li>quay.io/wangzheng422/nexus-fs</li>
</ul>
<h2 id="build-github-clone-site-using-gitea"><a class="header" href="#build-github-clone-site-using-gitea">build github clone site, using gitea</a></h2>
<p>似乎 gitea 并没有在离线部署脚本中</p>
<pre><code class="language-bash"># http://git.ocp4.redhat.ren:10080/

cat &lt;&lt; EOF &gt;&gt;  /etc/hosts
127.0.0.1 registry.ocp4.redhat.ren nexus.ocp4.redhat.ren git.ocp4.redhat.ren
EOF

yum install -y firewalld
systemctl disable --now firewalld
# systemctl start firewalld

yum -y install podman pigz skopeo buildah

podman image prune -a

############################################
# build init fs
mkdir -p /data/ccn/gitea
cd /data/ccn
rm -rf /data/ccn/gitea

mkdir -p /data/ccn/gitea
chown -R 1000:1000 /data/ccn/gitea

podman run -d --name gitea \
  -v /data/ccn/gitea:/data:Z \
  -e USER_UID=1000 \
  -e USER_GID=1000 \
  -p 10080:3000 \
  -p 10022:22 \
  docker.io/gitea/gitea:1.13.0

# admin user: root / redhat
# api call token : 6d47a0172d53e567737f7a81bbb6dbff4c1565d1

cd /data/ccn
tar cf - ./gitea | pigz -c &gt; gitea.tgz 
buildah from --name onbuild-container scratch
buildah copy onbuild-container gitea.tgz  /
buildah umount onbuild-container 
buildah commit --rm --format=docker onbuild-container docker.io/wangzheng422/gogs-fs:gitea-init
rm -f gitea.tgz 
buildah push docker.io/wangzheng422/gogs-fs:gitea-init
echo &quot;docker.io/wangzheng422/gogs-fs:gitea-init&quot;

######################################################
# build gitea based on init fs
mkdir -p /data/ccn/gitea
cd /data/ccn
rm -rf /data/ccn/gitea

mkdir -p /data/ccn/gitea
chown -R 1000:1000 /data/ccn/gitea

cd /data/ccn
podman create --name swap docker.io/wangzheng422/gogs-fs:gitea-init ls
podman cp swap:/gitea.tgz - &gt; gitea.tgz
podman rm -fv swap
tar zvxf gitea.tgz
rm -f gitea.tgz
chown -R 1000:1000 /data/ccn/gitea

podman run -d --name gitea \
  -v /data/ccn/gitea:/data:Z \
  -e USER_UID=1000 \
  -e USER_GID=1000 \
  -p 10080:3000 \
  -p 10022:22 \
  docker.io/gitea/gitea:1.13.0


# Custom config '/data/ccn/gogs/gogs/conf/app.ini'
# find the access key in pwd file
export ACCESS_KEY=&quot;6d47a0172d53e567737f7a81bbb6dbff4c1565d1&quot;

# curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X DELETE http://gogs.redhat.ren:10080/api/v1/repos/root/cloud-native-workshop-v2m1-guides

# curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X DELETE http://gogs.redhat.ren:10080/api/v1/repos/root/cloud-native-workshop-v2m2-guides

# curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X DELETE http://gogs.redhat.ren:10080/api/v1/repos/root/cloud-native-workshop-v2m3-guides

# curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X DELETE http://gogs.redhat.ren:10080/api/v1/repos/root/cloud-native-workshop-v2m4-guides

# curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X DELETE http://gogs.redhat.ren:10080/api/v1/repos/root/cloud-native-workshop-v2m1-labs

# curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X DELETE http://gogs.redhat.ren:10080/api/v1/repos/root/cloud-native-workshop-v2m2-labs

# curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X DELETE http://gogs.redhat.ren:10080/api/v1/repos/root/cloud-native-workshop-v2m3-labs

# curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X DELETE http://gogs.redhat.ren:10080/api/v1/repos/root/cloud-native-workshop-v2m4-labs

curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X POST http://git.ocp4.redhat.ren:10080/api/v1/repos/migrate \
        -H &quot;Content-Type: application/json&quot; \
        -d '{&quot;clone_addr&quot;: &quot;'&quot;https://github.com/wangzheng422/cloud-native-workshop-v2m1-guides.git&quot;'&quot;, &quot;uid&quot;: '&quot;1&quot;', &quot;repo_name&quot;: &quot;'&quot;cloud-native-workshop-v2m1-guides&quot;'&quot; }' 

curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X POST http://git.ocp4.redhat.ren:10080/api/v1/repos/migrate \
        -H &quot;Content-Type: application/json&quot; \
        -d '{&quot;clone_addr&quot;: &quot;'&quot;https://github.com/wangzheng422/cloud-native-workshop-v2m2-guides.git&quot;'&quot;, &quot;uid&quot;: '&quot;1&quot;', &quot;repo_name&quot;: &quot;'&quot;cloud-native-workshop-v2m2-guides&quot;'&quot; }' 

curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X POST http://git.ocp4.redhat.ren:10080/api/v1/repos/migrate \
        -H &quot;Content-Type: application/json&quot; \
        -d '{&quot;clone_addr&quot;: &quot;'&quot;https://github.com/wangzheng422/cloud-native-workshop-v2m3-guides.git&quot;'&quot;, &quot;uid&quot;: '&quot;1&quot;', &quot;repo_name&quot;: &quot;'&quot;cloud-native-workshop-v2m3-guides&quot;'&quot; }' 

curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X POST http://git.ocp4.redhat.ren:10080/api/v1/repos/migrate \
        -H &quot;Content-Type: application/json&quot; \
        -d '{&quot;clone_addr&quot;: &quot;'&quot;https://github.com/wangzheng422/cloud-native-workshop-v2m4-guides.git&quot;'&quot;, &quot;uid&quot;: '&quot;1&quot;', &quot;repo_name&quot;: &quot;'&quot;cloud-native-workshop-v2m4-guides&quot;'&quot; }' 

curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X POST http://git.ocp4.redhat.ren:10080/api/v1/repos/migrate \
        -H &quot;Content-Type: application/json&quot; \
        -d '{&quot;clone_addr&quot;: &quot;'&quot;https://github.com/wangzheng422/cloud-native-workshop-v2m1-labs.git&quot;'&quot;, &quot;uid&quot;: '&quot;1&quot;', &quot;repo_name&quot;: &quot;'&quot;cloud-native-workshop-v2m1-labs&quot;'&quot; }' 

curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X POST http://git.ocp4.redhat.ren:10080/api/v1/repos/migrate \
        -H &quot;Content-Type: application/json&quot; \
        -d '{&quot;clone_addr&quot;: &quot;'&quot;https://github.com/wangzheng422/cloud-native-workshop-v2m2-labs.git&quot;'&quot;, &quot;uid&quot;: '&quot;1&quot;', &quot;repo_name&quot;: &quot;'&quot;cloud-native-workshop-v2m2-labs&quot;'&quot; }' 

curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X POST http://git.ocp4.redhat.ren:10080/api/v1/repos/migrate \
        -H &quot;Content-Type: application/json&quot; \
        -d '{&quot;clone_addr&quot;: &quot;'&quot;https://github.com/wangzheng422/cloud-native-workshop-v2m3-labs.git&quot;'&quot;, &quot;uid&quot;: '&quot;1&quot;', &quot;repo_name&quot;: &quot;'&quot;cloud-native-workshop-v2m3-labs&quot;'&quot; }' 

curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X POST http://git.ocp4.redhat.ren:10080/api/v1/repos/migrate \
        -H &quot;Content-Type: application/json&quot; \
        -d '{&quot;clone_addr&quot;: &quot;'&quot;https://github.com/wangzheng422/cloud-native-workshop-v2m4-labs.git&quot;'&quot;, &quot;uid&quot;: '&quot;1&quot;', &quot;repo_name&quot;: &quot;'&quot;cloud-native-workshop-v2m4-labs&quot;'&quot; }' 

curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X POST http://git.ocp4.redhat.ren:10080/api/v1/repos/migrate \
        -H &quot;Content-Type: application/json&quot; \
        -d '{&quot;clone_addr&quot;: &quot;'&quot;https://github.com/spring-projects/spring-petclinic.git&quot;'&quot;, &quot;uid&quot;: '&quot;1&quot;', &quot;repo_name&quot;: &quot;'&quot;spring-petclinic&quot;'&quot; }' 

curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X POST http://git.ocp4.redhat.ren:10080/api/v1/repos/migrate \
        -H &quot;Content-Type: application/json&quot; \
        -d '{&quot;clone_addr&quot;: &quot;'&quot;https://github.com/wangzheng422/vote-api.git&quot;'&quot;, &quot;uid&quot;: '&quot;1&quot;', &quot;repo_name&quot;: &quot;'&quot;vote-api&quot;'&quot; }' 

curl -v -s -w '%{http_code}' -H &quot;Authorization: token ${ACCESS_KEY}&quot; -X POST http://git.ocp4.redhat.ren:10080/api/v1/repos/migrate \
        -H &quot;Content-Type: application/json&quot; \
        -d '{&quot;clone_addr&quot;: &quot;'&quot;https://github.com/wangzheng422/vote-ui.git&quot;'&quot;, &quot;uid&quot;: '&quot;1&quot;', &quot;repo_name&quot;: &quot;'&quot;vote-ui&quot;'&quot; }' 


podman logs -f gitea

podman stop gitea
podman rm -fv gitea

# bash demo.env.build.sh
cd /data/ccn

var_date=$(date '+%Y-%m-%d-%H%M')
echo $var_date

tar cf - ./gitea | pigz -c &gt; gitea.tgz
buildah from --name onbuild-container scratch
buildah copy onbuild-container gitea.tgz /
buildah umount onbuild-container 
buildah commit --rm --format=docker onbuild-container docker.io/wangzheng422/gogs-fs:gitea-$var_date
rm -f gitea.tgz
buildah push docker.io/wangzheng422/gogs-fs:gitea-$var_date
echo &quot;docker.io/wangzheng422/gogs-fs:gitea-$var_date&quot;

# docker.io/wangzheng422/gogs-fs:gitea-2021-01-06-0652

</code></pre>
<h2 id="create-an-online-nexus-maven-proxy"><a class="header" href="#create-an-online-nexus-maven-proxy">create an online nexus maven proxy</a></h2>
<p>我们使用一个在线的nexus proxy，来cache maven</p>
<ul>
<li>https://blog.csdn.net/kq1983/article/details/83066102</li>
</ul>
<pre><code class="language-bash"># get old fs
mkdir -p /data/ccn/nexus
cd /data/ccn/
podman create --name swap docker.io/wangzheng422/nexus-fs:2020-10-25-0919 ls
podman cp swap:/nexus.tgz - &gt; /data/ccn/nexus.tgz
podman rm -fv swap
tar zvxf nexus.tgz
rm -f nexus.tgz

chown -R 200 /data/ccn/nexus

#####################################################
# init build the nexus fs
mkdir -p /data/ccn/nexus
chown -R 200 /data/ccn/nexus

podman run -d -p 8081:8081 --name nexus -v /data/ccn/nexus:/nexus-data:Z docker.io/sonatype/nexus3:3.29.0

podman stop nexus
podman rm nexus

# get the admin password
cat /data/ccn/nexus/admin.password &amp;&amp; echo
# 8c9862da-5dcd-430c-a026-e3557539459a

# open http://nexus.ocp4.redhat.ren:8081

# add aliyun maven proxy
# https://blog.csdn.net/kq1983/article/details/83066102

######################################################
# dump the nexus image fs out

var_date=$(date '+%Y-%m-%d-%H%M')
echo $var_date
cd /data/ccn

tar cf - ./nexus | pigz -c &gt; nexus.tgz 
buildah from --name onbuild-container scratch
buildah copy onbuild-container nexus.tgz  /
buildah umount onbuild-container 
buildah commit --rm --format=docker onbuild-container docker.io/wangzheng422/nexus-fs:maven-$var_date
# buildah rm onbuild-container
rm -f nexus.tgz 
buildah push docker.io/wangzheng422/nexus-fs:maven-$var_date
echo &quot;docker.io/wangzheng422/nexus-fs:maven-$var_date&quot;

# docker.io/wangzheng422/nexus-fs:maven-2021-01-06-1456

</code></pre>
<h2 id="create-code-ready-workspace-image"><a class="header" href="#create-code-ready-workspace-image">create code ready workspace image</a></h2>
<p>CRW 给每个session启动了一个container，这个container的image就是用的操作台，我们定制一下这个操作台，让maven什么的都指向内网的proxy</p>
<pre><code class="language-bash">
mkdir -p /data/ccn/workspaces
cd /data/ccn/workspaces
# /bin/cp -f /data/order-service.tgz ./
wget -O settings.xml https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.6/ccn/settings.xml
wget -O .npmrc https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.6/ccn/.npmrc
wget -O .bowerrc https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.6/ccn/.bowerrc
wget --no-check-certificate --no-cache --no-cookies -O stack.Dockerfile https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.6/ccn/stack.dev.Dockerfile

buildah bud --format=docker -t docker.io/wangzheng422/cloudnative-workspaces-quarkus:2.4.1-wzh -f stack.Dockerfile .

buildah push docker.io/wangzheng422/cloudnative-workspaces-quarkus:2.4.1-wzh


</code></pre>
<h2 id="mta-vscode-extension"><a class="header" href="#mta-vscode-extension">mta vscode extension</a></h2>
<p>ccn 4.6 做了一个vscode上的extension，这个需要做离线</p>
<pre><code class="language-bash">################################3
## build mta extension
# install nodejs
curl -sL https://rpm.nodesource.com/setup_10.x | sudo bash -
yum install -y nodejs
npm install -g typescript vsce

mkdir -p /data/ccn/vscode
cd /data/ccn/vscode
git clone https://github.com/wangzheng422/rhamt-vscode-extension
cd rhamt-vscode-extension
git checkout ocp-4.6-ccn

npm install
npm run vscode:prepublish
vsce package -o mta-vscode-extension.vsix

cp mta-vscode-extension.vsix ../
cd /data/ccn/vscode

###################################
## use redhat upstream
var_date=$(date '+%Y-%m-%d-%H%M')
echo $var_date

mkdir -p /data/ccn/vscode
cd /data/ccn/vscode
# wget -O mta-vscode-extension.vsix https://download.jboss.org/jbosstools/adapters/snapshots/mta-vscode-extension/mta-vscode-extension-0.0.48-662.vsix
wget https://www.eclipse.org/che/images/logo-eclipseche.svg

buildah from --name onbuild-container scratch
buildah copy onbuild-container mta-vscode-extension.vsix  /
buildah copy onbuild-container logo-eclipseche.svg  /
buildah umount onbuild-container 
buildah commit --rm --format=docker onbuild-container docker.io/wangzheng422/imgs:mta-vscode-extension.vsix-$var_date
cd /data/ccn
# rm -rf /data/ccn/vscode
buildah push docker.io/wangzheng422/imgs:mta-vscode-extension.vsix-$var_date
echo &quot;docker.io/wangzheng422/imgs:mta-vscode-extension.vsix-$var_date&quot;
# docker.io/wangzheng422/imgs:mta-vscode-extension.vsix-2020-12-30-1012

##############################
# use real upstream
var_date=$(date '+%Y-%m-%d-%H%M')
echo $var_date

buildah from --name onbuild-container quay.io/windupeng/mta-vscode-extension
buildah umount onbuild-container 
buildah commit --rm --format=docker onbuild-container docker.io/wangzheng422/imgs:mta-vscode-extension.base-$var_date
buildah push docker.io/wangzheng422/imgs:mta-vscode-extension.base-$var_date
echo &quot;docker.io/wangzheng422/imgs:mta-vscode-extension.base-$var_date&quot;
# docker.io/wangzheng422/imgs:mta-vscode-extension.base-2020-12-30-1340

# if you want to use prebuild newer version
# https://raw.githubusercontent.com/windup/rhamt-che-demo/master/meta.yaml
mkdir -p /data/ccn/vscode
cd /data/ccn/vscode
wget -O mta-vscode-extension.vsix https://download.jboss.org/jbosstools/adapters/snapshots/mta-vscode-extension/mta-vscode-extension-0.0.58-790.vsix
wget https://www.eclipse.org/che/images/logo-eclipseche.svg

buildah from --name onbuild-container scratch
buildah copy onbuild-container mta-vscode-extension.vsix  /
buildah copy onbuild-container logo-eclipseche.svg  /
buildah umount onbuild-container 
buildah commit --rm --format=docker onbuild-container docker.io/wangzheng422/imgs:mta-vscode-extension.vsix-0.0.48-662
cd /data/ccn
# rm -rf /data/ccn/vscode
buildah push docker.io/wangzheng422/imgs:mta-vscode-extension.vsix-0.0.48-662

oc get pod -o json | jq -r .items[0].metadata.name
oc get pod -o json | jq -r .items[0].spec.containers[].name
oc get pod -o json | jq -r .items[0].spec.initContainers[].name

oc rsh -c $(oc get pod -o json | jq -r '.items[0].spec.containers[] | select( .name | contains(&quot;rhamt-extension&quot;) ) | .name')  $(oc get pod -o json | jq -r .items[0].metadata.name)

oc logs $(oc get pod -o json | jq -r .items[0].metadata.name) -c $(oc get pod -o json | jq -r '.items[0].spec.containers[] | select( .name | contains(&quot;rhamt-extension&quot;) ) | .name')

oc logs $(oc get pod -o json | jq -r .items[0].metadata.name) -c $(oc get pod -o json | jq -r '.items[0].spec.containers[] | select( .name | contains(&quot;theia-ide&quot;) ) | .name')

oc logs $(oc get pod -o json | jq -r .items[0].metadata.name) -c $(oc get pod -o json | jq -r '.items[0].spec.containers[] | select( .name | contains(&quot;vscode-quarkus&quot;) ) | .name')

oc logs $(oc get pod -o json | jq -r .items[0].metadata.name) -c $(oc get pod -o json | jq -r '.items[0].spec.containers[] | select( .name | contains(&quot;che-jwtproxy&quot;) ) | .name')

oc logs $(oc get pod -o json | jq -r .items[0].metadata.name) -c $(oc get pod -o json | jq -r '.items[0].spec.containers[] | select( .name | contains(&quot;quarkus-tools&quot;) ) | .name')

oc logs $(oc get pod -o json | jq -r .items[0].metadata.name) -c $(oc get pod -o json | jq -r '.items[0].spec.containers[] | select( .name | contains(&quot;che-machine-exe&quot;) ) | .name')

oc logs $(oc get pod -o json | jq -r .items[0].metadata.name) -c $(oc get pod -o json | jq -r '.items[0].spec.initContainers[] | select( .name | contains(&quot;remote-runtime-inject&quot;) ) | .name')

oc logs $(oc get pod -o json | jq -r .items[0].metadata.name) -c $(oc get pod -o json | jq -r '.items[0].spec.initContainers[] | select( .name | contains(&quot;pluginbroker-artifacts-rhel8&quot;) ) | .name')

oc exec $(oc get pod -o json | jq -r .items[0].metadata.name) -c $(oc get pod -o json | jq -r '.items[0].spec.containers[] | select( .name | contains(&quot;rhamt-extension&quot;) ) | .name') -- /usr/sbin/killall5


</code></pre>
<h2 id="build-static-html-file-1"><a class="header" href="#build-static-html-file-1">build static html file</a></h2>
<pre><code class="language-bash">
# get source to image 
# https://github.com/openshift/source-to-image
wget -O source-to-image.tgz https://github.com/openshift/source-to-image/releases/download/v1.3.0/source-to-image-v1.3.0-eed2850f-linux-amd64.tar.gz
tar zvxf source-to-image.tgz
mv s2i /usr/local/bin/

var_date=$(date '+%Y-%m-%d-%H%M')
echo $var_date

rm -rf /data/ccn/static-html
mkdir -p /data/ccn/static-html/files
cd /data/ccn/static-html/files

download_url() {
  # https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css
  var_url=$1

  # bootstrap/3.3.5/css/bootstrap.min.css
  var_file=${var_url#*.*/}
  
  # bootstrap/3.3.5/css
  var_path=${var_file%/*}
  
  mkdir -p $var_path
  wget -O $var_file $var_url

}

download_url https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css
download_url https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css
download_url https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css
download_url https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js
download_url https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css

download_url https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.map
download_url https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.map
download_url https://ajax.googleapis.com/ajax/libs/angularjs/1.4.8/angular.min.js

download_url https://at.alicdn.com/t/font_148784_v4ggb6wrjmkotj4i.woff
download_url https://at.alicdn.com/t/font_148784_v4ggb6wrjmkotj4i.ttf

download_url https://cdnjs.cloudflare.com/ajax/libs/patternfly/3.24.0/css/patternfly.min.css
download_url https://cdnjs.cloudflare.com/ajax/libs/patternfly/3.24.0/css/patternfly-additions.min.css
download_url https://cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.js
download_url https://cdnjs.cloudflare.com/ajax/libs/jquery-timeago/1.6.1/jquery.timeago.min.js

wget -O jquery-3.2.1.min.js     https://code.jquery.com/jquery-3.2.1.min.js
wget -O jquery-latest.min.js    http://code.jquery.com/jquery-latest.min.js

mkdir -p bootstrap/3.3.5/fonts/
wget -O bootstrap/3.3.5/fonts/glyphicons-halflings-regular.woff2  https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.4/ccn/glyphicons-halflings-regular.woff2
wget -O bootstrap/3.3.5/fonts/glyphicons-halflings-regular.woff https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.4/ccn/glyphicons-halflings-regular.woff
wget -O bootstrap/3.3.5/fonts/glyphicons-halflings-regular.ttf https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.4/ccn/glyphicons-halflings-regular.ttf

cd /data/ccn/static-html/

s2i build --rm  files/  registry.redhat.io/rhscl/nginx-114-rhel7:latest  nginx-sample-app

docker tag nginx-sample-app docker.io/wangzheng422/cloudnative-workspaces-quarkus:swap-$var_date
docker push docker.io/wangzheng422/cloudnative-workspaces-quarkus:swap-$var_date
echo docker.io/wangzheng422/cloudnative-workspaces-quarkus:swap-$var_date

wget -O mime.types https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.4/ccn/mime.types
wget -O nginx.conf https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.4/ccn/nginx.conf

cat &lt;&lt; EOF &gt; nginx.Dockerfile
FROM docker.io/wangzheng422/cloudnative-workspaces-quarkus:swap-$var_date

USER root
COPY mime.types /etc/nginx/
COPY nginx.conf /etc/nginx/

USER 1001
EOF

buildah bud --format=docker -t docker.io/wangzheng422/cloudnative-workspaces-quarkus:static-html-$var_date -f nginx.Dockerfile .

buildah push docker.io/wangzheng422/cloudnative-workspaces-quarkus:static-html-$var_date
echo &quot;docker.io/wangzheng422/cloudnative-workspaces-quarkus:static-html-$var_date&quot;


docker image prune -f
podman image prune -a

# oc -n labs-infra create route edge static-html-0 --service=static-html --hostname=maxcdn.bootstrapcdn.com 
# oc -n labs-infra create route edge static-html-1 --service=static-html   --hostname=ajax.googleapis.com 
# oc -n labs-infra create route edge static-html-2 --service=static-html   --hostname=at.alicdn.com
# oc -n labs-infra create route edge static-html-3 --service=static-html   --hostname=cdnjs.cloudflare.com
# oc -n labs-infra create route edge static-html-4 --service=static-html   --hostname=code.jquery.com

</code></pre>
<h2 id="pip-for-agnosticd-1"><a class="header" href="#pip-for-agnosticd-1">pip for agnosticd</a></h2>
<pre><code class="language-bash"># on vultr perpare pip
# https://www.linuxtechi.com/use-ansible-galaxy-roles-ansible-playbook/
# https://docs.ansible.com/ansible/latest/scenario_guides/guide_kubernetes.html
# https://stackoverflow.com/questions/11091623/how-to-install-packages-offline
# https://www.activestate.com/resources/quick-reads/how-to-update-all-python-packages/
# yum install -y python2-pip
mkdir -p /data/pip3
cd /data/pip3
# pip install --upgrade pip
pip3 install --user --upgrade kubernetes openshift requests
pip3 freeze --user &gt; requirements.txt
# pip3 install -r requirements.txt --upgrade
mkdir -p wheelhouse
pip3 download -r requirements.txt -d wheelhouse
/bin/cp -f requirements.txt wheelhouse/
tar -zcf wheelhouse.tar.gz wheelhouse


var_date=$(date '+%Y-%m-%d')
echo $var_date

buildah from --name onbuild-container scratch
buildah copy onbuild-container wheelhouse.tar.gz /
buildah umount onbuild-container 
buildah commit --rm --format=docker onbuild-container docker.io/wangzheng422/base-fs:pip3-whl-$var_date
# buildah rm onbuild-container
buildah push docker.io/wangzheng422/base-fs:pip3-whl-$var_date
echo &quot;docker.io/wangzheng422/base-fs:pip3-whl-$var_date&quot;

</code></pre>
<h2 id="nodejs-1"><a class="header" href="#nodejs-1">nodejs</a></h2>
<pre><code class="language-bash"># docker.io/wangzheng422/cloudnative-workspaces-quarkus:nodejs-10-2020-07-16-2155
# this docker file is build using nodejs-10.Dockerfile

mkdir -p /data/ccn/nodejs
cd /data/ccn/nodejs

var_date=$(date '+%Y-%m-%d')
echo $var_date

wget -O .npmrc https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.6/ccn/.npmrc
wget -O .bowerrc https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.6/ccn/.bowerrc
wget https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.6/ccn/nodejs-10.Dockerfile

buildah bud --format=docker -t docker.io/wangzheng422/imgs:nodejs-10-wzh-$var_date -f nodejs-10.Dockerfile .
buildah push docker.io/wangzheng422/imgs:nodejs-10-wzh-$var_date 

echo &quot;docker.io/wangzheng422/imgs:nodejs-10-wzh-$var_date&quot;

# docker.io/wangzheng422/imgs:nodejs-10-wzh-2021-01-05

</code></pre>
<h2 id="build-dist"><a class="header" href="#build-dist">build dist</a></h2>
<pre><code class="language-bash">cd /data/ocp4
wget -O poc.image.list https://raw.githubusercontent.com/wangzheng422/docker_env/dev/redhat/ocp4/4.6/ccn/poc.image.list

export MIRROR_DIR='/data/poc.image'
/bin/rm -rf ${MIRROR_DIR}
bash add.image.sh poc.image.list ${MIRROR_DIR}


</code></pre>
<h2 id="labs-sync-1"><a class="header" href="#labs-sync-1">labs sync</a></h2>
<pre><code class="language-bash">
rsync -e ssh --info=progress2 -P --delete -arz bastion.fd21.example.opentlc.com:/data/ccn/nexus/  /data/ccn/nexus/

rsync -e ssh -P --delete -arz root@bastion.fd21.example.opentlc.com:/data/ccn/nexus/  ./nexus/ 

rsync -e ssh -P --delete -arz  ./nexus/  root@192.168.7.11:/data/ccn/nexus/   

chown -R 200:root nexus

rsync -e ssh --info=progress2 -P --delete -arz   192.168.252.11:/data/ccn/nexus/   ./nexus/   



</code></pre>
<h2 id="other-tips-1"><a class="header" href="#other-tips-1">other tips</a></h2>
<h3 id="find-object-blocks-deleting-namespaceproject-1"><a class="header" href="#find-object-blocks-deleting-namespaceproject-1">find object blocks deleting namespace/project</a></h3>
<ul>
<li>https://access.redhat.com/solutions/4165791</li>
</ul>
<pre><code class="language-bash">PROJECT_NAME=user1-cloudnativeapps

oc api-resources --verbs=list --namespaced -o name | xargs -n 1 oc get --show-kind --ignore-not-found -n $PROJECT_NAME

oc api-resources --verbs=list --cached --namespaced -o name | xargs -n 1 oc get --show-kind --ignore-not-found -n $PROJECT_NAME


configuration.serving.knative.dev/payment
service.serving.knative.dev/payment
route.serving.knative.dev/payment


</code></pre>
<h3 id="service-mesh--knative-1"><a class="header" href="#service-mesh--knative-1">service mesh &amp; knative</a></h3>
<pre><code class="language-bash">oc project istio-system
oc get pod -o json | jq -r '.items[].spec.containers[].image' &gt; tmp.list

oc project istio-operator
oc get pod -o json | jq -r '.items[].spec.containers[].image' &gt;&gt; tmp.list

oc project knative-eventing
oc get pod -o json | jq -r '.items[].spec.containers[].image' &gt;&gt; tmp.list

oc project knative-serving
oc get pod -o json | jq -r '.items[].spec.containers[].image' &gt;&gt; tmp.list

oc project tekton-pipelines
oc get pod -o json | jq -r '.items[].spec.containers[].image' &gt;&gt; tmp.list

oc get pod -o json | jq -r '.items[].spec.initContainers[].image' &gt;&gt; tmp.list

oc project openshift-operators
oc get pod -o json | jq -r '.items[].spec.containers[].image' &gt;&gt; tmp.list



cat tmp.list | sort | uniq

oc project user0-catalog
oc get pod -o json | jq -r '.items[].spec.containers[].image'| sort | uniq 


</code></pre>
<h2 id="以下是弯路-3"><a class="header" href="#以下是弯路-3">以下是弯路</a></h2>
<p>build github clone site, using gitlab</p>
<pre><code class="language-bash">yum -y install podman

rm -rf /data/ccn/gitlab
mkdir -p /data/ccn/gitlab/config
mkdir -p /data/ccn/gitlab/logs
mkdir -p /data/ccn/gitlab/data


# podman run --detach \
#   --hostname local.redhat.ren \
#   --env GITLAB_OMNIBUS_CONFIG=&quot;external_url 'http://local.redhat.ren:7080/'; gitlab_rails['lfs_enabled'] = true;&quot; \
#   --publish 7443:443 --publish 7080:80 --publish 7022:22 \
#   --name gitlab \
#   --restart always \
#   --volume /data/ocp4/demo/gitlab/config:/etc/gitlab:Z \
#   --volume /data/ocp4/demo/gitlab/logs:/var/log/gitlab:Z \
#   --volume /data/ocp4/demo/gitlab/data:/var/opt/gitlab:Z \
#   gitlab/gitlab-ce:latest

podman run --detach \
  --hostname local.redhat.ren \
  --publish 7443:443 --publish 7080:80 --publish 7022:22 \
  --name gitlab \
  --restart always \
  --volume /data/ccn/gitlab/config:/etc/gitlab:Z \
  --volume /data/ccn/gitlab/logs:/var/log/gitlab:Z \
  --volume /data/ccn/gitlab/data:/var/opt/gitlab:Z \
  gitlab/gitlab-ce:latest

# set default username / password
# root / redhat2019

podman stop gitlab

podman rm -fv gitlab

cd /data/ccn
# tar zcf gitlab.tgz ./gitlab 
cat &lt;&lt; EOF &gt; /data/ccn/gitlab.files.Dockerfile
FROM registry.redhat.io/ubi7/ubi
COPY gitlab /gitlab
EOF
podman build --no-cache -f /data/ccn/gitlab.files.Dockerfile -t quay.io/wangzheng422/gitlab-fs /data/ccn/
podman push quay.io/wangzheng422/gitlab-fs

podman exec -it gitlab update-permissions
podman restart gitlab
podman logs -f gitlab
getfacl /data/ccn/gitlab/

# now we try to use it
rm -rf /data/ccn/gitlab
podman run -d --name gitlab-fs --entrypoint &quot;tail&quot; quay.io/wangzheng422/gitlab-fs -f /dev/null
podman cp gitlab-fs:/gitlab /data/ccn/
podman rm -fv gitlab-fs
# tar zxf gitlab.tgz
# chown -R root: /data/ccn/gitlab/
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="virus-test-for-docker-image-security-scanning"><a class="header" href="#virus-test-for-docker-image-security-scanning">virus test for docker image security scanning</a></h1>
<p>几乎所有的容器平台，都有容器安全的方案，比如大名鼎鼎的clair，但是他们的扫描原理并不是深度扫描，而是通过容器内部yum, apk等包管理工具，扫描包管理工具的历史数据库，看装在容器里面的软件版本，从而判断是否有漏洞的。</p>
<p>这种扫描方法，当然是为了性能，但是也给日常实践带来了困扰，一般工程师很容易的以为，我们上了一个容器安全平台，就可以高枕无忧了，其实不是这样的。</p>
<p>以下我们就举一个实际的例子，看看效果，然后我们再来想想怎么应对。</p>
<h1 id="test-quay--clair--docker-hub"><a class="header" href="#test-quay--clair--docker-hub">test quay / clair / docker hub</a></h1>
<p>我们用网上的测试病毒，复制到容器里面去，打包，上传到镜像仓库，看看镜像仓库的扫描结果。为了更有代表性，我们把病毒复制成java，然后我们把镜像分别上传quay.io, docker hub</p>
<pre><code class="language-bash">mkdir -p /data/tmp
cd /data/tmp

cat &lt;&lt; EOF &gt; ./virus.Dockerfile
FROM registry.access.redhat.com/ubi8/ubi-minimal
ADD https://www.ikarussecurity.com/wp-content/downloads/eicar_com.zip /wzh
ADD https://github.com/MalwareSamples/Linux-Malware-Samples/blob/main/00ae07c9fe63b080181b8a6d59c6b3b6f9913938858829e5a42ab90fb72edf7a /wzh01
ADD https://github.com/MalwareSamples/Linux-Malware-Samples/blob/main/00ae07c9fe63b080181b8a6d59c6b3b6f9913938858829e5a42ab90fb72edf7a /usr/bin/java

RUN chmod +x /wzh*
RUN chmod +x /usr/bin/java
EOF

buildah bud -t quay.io/wangzheng422/qimgs:virus -f virus.Dockerfile ./

buildah push quay.io/wangzheng422/qimgs:virus

buildah bud -t docker.io/wangzheng422/virus -f virus.Dockerfile ./

buildah push docker.io/wangzheng422/virus

</code></pre>
<p>我们发现，包含病毒的镜像，在两个容器镜像平台上，都扫描不出来。这也就证明了，普通的扫描工具，只能扫描包管理工具的历史数据库，而不能扫描容器内部的软件版本。</p>
<h1 id="log4jshell"><a class="header" href="#log4jshell">log4jshell</a></h1>
<p>现在大名顶顶的log4j的漏洞，各地平台扫描能不一样，能扫描的也是检测容器里面的jar文件，然后看这个jar文件里面的MANIFEST.MF文件，在这个文件里面看看对应软件包的版本号，然后报警。</p>
<p>我们可以看看 quay.io/apoczeka/log4shell-vuln 这个容器，在quay.io上面的扫描结果，可以看到他无法发现log4j的漏洞。</p>
<h2 id="acs"><a class="header" href="#acs">ACS</a></h2>
<p>那么我们看看红帽RHACS容器安全平台能不能扫出来。</p>
<pre><code class="language-bash"># on vultr
wget https://mirror.openshift.com/pub/rhacs/assets/latest/bin/linux/roxctl
install -m 755 roxctl /usr/local/bin/

# on ACS platform
# Integrations -&gt; API Token -&gt; Create Integration
# role -&gt; continous-integration -&gt; create
# copy the API token out
export ROX_API_TOKEN=&lt;api_token&gt;
export ROX_CENTRAL_ADDRESS=central-stackrox.apps.cluster-ms246.ms246.sandbox1059.opentlc.com:443

roxctl -e &quot;$ROX_CENTRAL_ADDRESS&quot; --insecure-skip-tls-verify image scan -i docker.io/elastic/logstash:7.13.0 | jq '.scan.components[] | .vulns[]? | select(.cve == &quot;CVE-2021-44228&quot;) | .cve'
# &quot;CVE-2021-44228&quot;
# &quot;CVE-2021-44228&quot;

roxctl -e &quot;$ROX_CENTRAL_ADDRESS&quot; --insecure-skip-tls-verify image scan -i quay.io/apoczeka/log4shell-vuln | jq '.scan.components[] | .vulns[]? | select(.cve == &quot;CVE-2021-44228&quot;) | .cve'
# &quot;CVE-2021-44228&quot;

roxctl -e &quot;$ROX_CENTRAL_ADDRESS&quot; --insecure-skip-tls-verify image check -r 0 -o json -i docker.io/elastic/logstash:7.13.0 

</code></pre>
<p>我们可以看到ACS成功的检测到了log4j的漏洞。这样我们就可以把ACS继承到我们CI/CD流水线里面去，完成漏洞扫描。</p>
<p>当然，我们可以使用ACS内置的界面工具，快速的定义规则，并且第一时间禁止相关的漏洞。</p>
<p><img src="notes/2021/imgs/2021-12-17-16-32-12.png" alt="" /></p>
<p>这里是配置生效以后（要有点下发的时间，如果集群比较繁忙的话），ACS阻止有漏洞的镜像运行的效果。</p>
<p><img src="notes/2021/imgs/2021-12-17-16-34-15.png" alt="" /></p>
<p>不过，目前ACS只支持deployment模式的部署，你要是修改deployment, 或者干脆用pod直接部署，都会绕过ACS的检测，这个以后看ACS升级解决吧。</p>
<h1 id="grype"><a class="header" href="#grype">grype</a></h1>
<p>类似ACS的命令行工具，还有很多其他的选择，这里举个例子。</p>
<pre><code class="language-bash"># https://github.com/anchore/grype

grype -q quay.io/apoczeka/log4shell-vuln | grep log4j
# log4j-api          2.14.1       2.15.0       GHSA-jfh8-c2jp-5v3q  Critical
# log4j-api          2.14.1       2.16.0       GHSA-7rjr-3q55-vv33  Medium
# log4j-api          2.14.1                    CVE-2021-44228       Critical
# log4j-core         2.14.1       2.15.0       GHSA-jfh8-c2jp-5v3q  Critical
# log4j-core         2.14.1       2.16.0       GHSA-7rjr-3q55-vv33  Medium
# log4j-core         2.14.1                    CVE-2021-44228       Critical
# log4j-jul          2.14.1                    CVE-2021-44228       Critical
# log4j-slf4j-impl   2.14.1                    CVE-2021-44228       Critical


</code></pre>
<h1 id="trivy"><a class="header" href="#trivy">trivy</a></h1>
<p>https://github.com/aquasecurity/trivy</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-install-cnv-with-ocs-and-external-ceph"><a class="header" href="#openshift-install-cnv-with-ocs-and-external-ceph">openshift install cnv with ocs and external ceph</a></h1>
<p>本次测试的目标业务场景是，一个CS服务的虚机，用CNV跑在openshift上，虚机的镜像承载在ceph上，并测试虚机热迁移和虚机克隆场景。</p>
<p>由于测试环境所限，我们配置一个单节点的ceph，挂3个5.5T的盘，这个ceph节点，就用kvm，配置了16C32G，实际跑下来，感觉8C32G也是够的。</p>
<h2 id="部署架构图"><a class="header" href="#部署架构图">部署架构图</a></h2>
<p><img src="ocp4/4.5/./dia/4.5.ocp.ocs.cnv.ceph.drawio.svg" alt="diag" /></p>
<h2 id="视频讲解"><a class="header" href="#视频讲解">视频讲解</a></h2>
<h3 id="单节点ceph安装ocs安装对接外部ceph存储"><a class="header" href="#单节点ceph安装ocs安装对接外部ceph存储">单节点ceph安装，ocs安装，对接外部ceph存储</a></h3>
<p><a href="https://www.bilibili.com/video/BV1Dk4y1k7ky/"><kbd><img src="ocp4/4.5/imgs/2020-09-24-15-53-28.png" width="600"></kbd></a></p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1Dk4y1k7ky/">bilibili</a></li>
<li><a href="https://www.ixigua.com/6875967361250230798">xigua</a></li>
<li><a href="https://youtu.be/DLNsFRsZZ9s">youtube </a></li>
</ul>
<h3 id="cnv安装导入虚机镜像热迁移克隆"><a class="header" href="#cnv安装导入虚机镜像热迁移克隆">cnv安装，导入虚机镜像，热迁移，克隆</a></h3>
<p><a href="https://www.bilibili.com/video/BV1n541187iZ/"><kbd><img src="ocp4/4.5/imgs/2020-09-25-12-49-47.png" width="600"></kbd></a></p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1n541187iZ/">bilibili</a></li>
<li><a href="https://www.ixigua.com/6876282797774864904">xigua</a></li>
<li><a href="https://youtu.be/N0apXJSr3lY">youtube</a></li>
</ul>
<h2 id="install-ceph"><a class="header" href="#install-ceph">install ceph</a></h2>
<p>我们先安装这个ceph节点。</p>
<pre><code class="language-bash">
#####################################
## start to install ceph
cd /backup/wzh

lvremove -f ocp4/cephlv
lvcreate -y -L 230G -n cephlv ocp4

lvremove -f ocp4/cephdata01lv
lvcreate -y -L 3T -n cephdata01lv ocp4

lvremove -f ocp4/cephdata02lv
lvcreate -y -L 3T -n cephdata02lv ocp4

lvremove -f ocp4/cephdata03lv
lvcreate -y -L 3T -n cephdata03lv ocp4

virt-install --name=ocp4-ceph --vcpus=16 --ram=32768 \
--disk path=/dev/ocp4/cephlv,device=disk,bus=virtio,format=raw \
--disk path=/dev/ocp4/cephdata01lv,device=disk,bus=virtio,format=raw \
--disk path=/dev/ocp4/cephdata02lv,device=disk,bus=virtio,format=raw \
--disk path=/dev/ocp4/cephdata03lv,device=disk,bus=virtio,format=raw \
--os-variant centos7.0 --network network:openshift4,model=virtio \
--boot menu=on --location /home/data/openshift/ocp.4.3.21/rhel-server-7.8-x86_64-dvd.iso \
--initrd-inject rhel-ks-ceph.cfg --extra-args &quot;inst.ks=file:/rhel-ks-ceph.cfg&quot; 

#######################################
#  kvm's host bond and vlan

# https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/networking_guide/sec-configure_802_1q_vlan_tagging_using_the_command_line_tool_nmcli

# https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/networking_guide/sec-vlan_on_bond_and_bridge_using_the_networkmanager_command_line_tool_nmcli
nmcli con add type bond \
    con-name bond-24 \
    ifname bond-24 \
    mode 802.3ad ipv4.method disabled ipv6.method ignore
    
nmcli con mod id bond-24 bond.options \
    mode=802.3ad,miimon=100,lacp_rate=fast,xmit_hash_policy=layer2+3
    
nmcli con add type bond-slave ifname enp176s0f0 con-name enp176s0f0 master bond-24
nmcli con add type bond-slave ifname enp59s0f0 con-name enp59s0f0 master bond-24

nmcli con up bond-24

nmcli connection add type bridge con-name br-ceph ifname br-ceph ip4 192.168.18.200/24

nmcli con up br-ceph

nmcli con add type vlan con-name vlan-ceph ifname vlan-ceph dev bond-24 id 501 master br-ceph slave-type bridge

nmcli con up vlan-ceph

# no need below
# cat &lt;&lt; EOF &gt;  /backup/wzh/virt-net.xml
# &lt;network&gt;
#   &lt;name&gt;vm-br-ceph&lt;/name&gt;
#   &lt;forward mode='bridge'&gt;
#     &lt;bridge name='br-ceph'/&gt;
#   &lt;/forward&gt;
# &lt;/network&gt;
# EOF
# virsh net-define --file virt-net.xml
# virsh net-autostart br-ceph
# virsh net-start br-ceph
# virsh net-list

# # restore
# virsh net-undefine br-ceph
# virsh net-destroy br-ceph

cat &lt;&lt; EOF &gt; /root/.ssh/config
StrictHostKeyChecking no
UserKnownHostsFile=/dev/null

EOF

# restore
nmcli con del vlan-ceph
nmcli con del br-ceph
nmcli con del enp59s0f0
nmcli con del enp176s0f0
nmcli con del bond-24

########################################
# go to ceph vm

# https://www.cyberciti.biz/faq/linux-list-network-cards-command/
cat /proc/net/dev

nmcli con add type ethernet ifname eth1 con-name eth1
nmcli con modify eth1 ipv4.method manual ipv4.addresses 192.168.18.203/24
nmcli con modify eth1 connection.autoconnect yes
nmcli con reload
nmcli con up eth1

# restore
nmcli con del eth1

##########################################
# go to worker2 vm, to test the ceph vlan
nmcli con add type ethernet ifname ens9 con-name ens9
nmcli con modify ens9 ipv4.method manual ipv4.addresses 192.168.18.209/24
nmcli con modify ens9 connection.autoconnect yes
nmcli con reload
nmcli con up ens9

# restore
nmcli con del ens9
nmcli con del 'Wired connection 1'

##########################################
# go to worker1 vm, to test the ceph vlan
nmcli con add type ethernet ifname ens9 con-name ens9
nmcli con modify ens9 ipv4.method manual ipv4.addresses 192.168.18.208/24
nmcli con modify ens9 connection.autoconnect yes
nmcli con reload
nmcli con up ens9

# restore
nmcli con del ens9
nmcli con del 'Wired connection 1'

##########################################
# go to worker0 vm, to test the ceph vlan
nmcli con add type ethernet ifname ens9 con-name ens9
nmcli con modify ens9 ipv4.method manual ipv4.addresses 192.168.18.207/24
nmcli con modify ens9 connection.autoconnect yes
nmcli con reload
nmcli con up ens9

##########################################
# go to master2 vm, to test the ceph vlan
nmcli con add type ethernet ifname ens9 con-name ens9
nmcli con modify ens9 ipv4.method manual ipv4.addresses 192.168.18.206/24
nmcli con modify ens9 connection.autoconnect yes
nmcli con reload
nmcli con up ens9

# restore
nmcli con del ens9
nmcli con del 'Wired connection 1'

##########################################
# go to master1 vm, to test the ceph vlan
nmcli con add type ethernet ifname ens9 con-name ens9
nmcli con modify ens9 ipv4.method manual ipv4.addresses 192.168.18.205/24
nmcli con modify ens9 connection.autoconnect yes
nmcli con reload
nmcli con up ens9

# restore
nmcli con del ens9
nmcli con del 'Wired connection 1'

##########################################
# go to master0 vm, to test the ceph vlan
nmcli con add type ethernet ifname ens9 con-name ens9
nmcli con modify ens9 ipv4.method manual ipv4.addresses 192.168.18.204/24
nmcli con modify ens9 connection.autoconnect yes
nmcli con reload
nmcli con up ens9

# restore
nmcli con del ens9
nmcli con del 'Wired connection 1'

##########################################
# go to worker4 baremetal, to test the ceph vlan
nmcli con del 'Wired connection 1'
nmcli con del 'Wired connection 2'
nmcli con del 'Wired connection 3'
nmcli con del 'Wired connection 4'
nmcli con del 'Wired connection 5'
nmcli con del ens35f0.991
nmcli con del ens35f1

# https://access.redhat.com/solutions/1526613
nmcli con add type bond \
    con-name bond-24 \
    ifname bond-24 \
    mode 802.3ad ipv4.method disabled ipv6.method ignore
    
nmcli con mod id bond-24 bond.options \
    mode=802.3ad,miimon=100,lacp_rate=fast,xmit_hash_policy=layer2+3
    
nmcli con add type bond-slave ifname ens49f0 con-name ens49f0 master bond-24
nmcli con add type bond-slave ifname ens35f0 con-name ens35f0 master bond-24

nmcli con up bond-24

nmcli con add type vlan con-name vlan-ceph ifname vlan-ceph dev bond-24 id 501 ip4 192.168.18.211/24

nmcli con up vlan-ceph

# restore
nmcli con del vlan-ceph
nmcli con del ens49f0 ens35f0
nmcli con del bond-24

#############################################
# go to worker3 baremetal, to test the ceph vlan

nmcli con del 'Wired connection 1'
nmcli con del 'Wired connection 2'
nmcli con del 'Wired connection 3'
nmcli con del 'Wired connection 4'
nmcli con del 'Wired connection 5'

nmcli con add type bond \
    con-name bond-24 \
    ifname bond-24 \
    mode 802.3ad ipv4.method disabled ipv6.method ignore
    
nmcli con mod id bond-24 bond.options \
    mode=802.3ad,miimon=100,lacp_rate=fast,xmit_hash_policy=layer2+3
    
nmcli con add type bond-slave ifname ens49f0 con-name ens49f0 master bond-24
nmcli con add type bond-slave ifname ens35f0 con-name ens35f0 master bond-24

nmcli con up bond-24

nmcli con add type vlan con-name vlan-ceph ifname vlan-ceph dev bond-24 id 501  ip4 192.168.18.210/24

nmcli con up vlan-ceph

# restore
nmcli con del vlan-ceph
nmcli con del ens49f0 ens35f0
nmcli con del bond-24


#################################################
## for ceph vm
# install a 'fast' http proxy, then

subscription-manager --proxy=127.0.0.1:6666 register --username **** --password ********
# subscription-manager --proxy=127.0.0.1:6666 refresh

subscription-manager config --rhsm.baseurl=https://china.cdn.redhat.com
# subscription-manager config --rhsm.baseurl=https://cdn.redhat.com
subscription-manager --proxy=127.0.0.1:6666 refresh

# https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html-single/installation_guide/index
subscription-manager --proxy=127.0.0.1:6666 repos --disable=*
subscription-manager --proxy=127.0.0.1:6666 repos --enable=rhel-7-server-rpms \
--enable=rhel-7-server-extras-rpms \
--enable=rhel-7-server-supplementary-rpms \
--enable=rhel-7-server-optional-rpms \
--enable=rhel-7-server-rhceph-4-tools-rpms --enable=rhel-7-server-ansible-2.8-rpms \
--enable=rhel-7-server-rhceph-4-mon-rpms \
--enable=rhel-7-server-rhceph-4-osd-rpms \
--enable=rhel-7-server-rhceph-4-tools-rpms 


yum clean all
yum makecache

yum update -y

systemctl enable --now firewalld
systemctl start firewalld
systemctl status firewalld

firewall-cmd --zone=public --add-port=6789/tcp
firewall-cmd --zone=public --add-port=6789/tcp --permanent
firewall-cmd --zone=public --add-port=6800-7300/tcp
firewall-cmd --zone=public --add-port=6800-7300/tcp --permanent
firewall-cmd --zone=public --add-port=6800-7300/tcp
firewall-cmd --zone=public --add-port=6800-7300/tcp --permanent
firewall-cmd --zone=public --add-port=6800-7300/tcp
firewall-cmd --zone=public --add-port=6800-7300/tcp --permanent
firewall-cmd --zone=public --add-port=8080/tcp
firewall-cmd --zone=public --add-port=8080/tcp --permanent
firewall-cmd --zone=public --add-port=443/tcp
firewall-cmd --zone=public --add-port=443/tcp --permanent
# firewall-cmd --zone=public --add-port=9090/tcp
# firewall-cmd --zone=public --add-port=9090/tcp --permanent

ssh-keygen

sed -i 's/#UseDNS yes/UseDNS no/' /etc/ssh/sshd_config
systemctl restart sshd

ssh-copy-id root@ceph

yum install -y ceph-ansible docker

cd /usr/share/ceph-ansible

# yum install -y docker
systemctl enable --now docker

cd /usr/share/ceph-ansible
/bin/cp -f  group_vars/all.yml.sample group_vars/all.yml
/bin/cp -f  group_vars/osds.yml.sample group_vars/osds.yml
/bin/cp -f  site-docker.yml.sample site-docker.yml
/bin/cp -f  site.yml.sample site.yml
/bin/cp -f  group_vars/rgws.yml.sample group_vars/rgws.yml
/bin/cp -f  group_vars/mdss.yml.sample group_vars/mdss.yml

# remember to set the env
# https://access.redhat.com/RegistryAuthentication
# REGISTRY_USER_NAME=
# REGISTRY_TOKEN=

cat &lt;&lt; EOF &gt; ./group_vars/all.yml
fetch_directory: ~/ceph-ansible-keys
monitor_interface: eth1 
public_network: 192.168.18.0/24
# ceph_docker_image: rhceph/rhceph-4-rhel8
# ceph_docker_image_tag: &quot;latest&quot;
# containerized_deployment: true
ceph_docker_registry: registry.redhat.io
ceph_docker_registry_auth: true
ceph_docker_registry_username: ${REGISTRY_USER_NAME}
ceph_docker_registry_password: ${REGISTRY_TOKEN}
ceph_origin: repository
ceph_repository: rhcs
# ceph_repository_type: cdn
ceph_repository_type: iso
ceph_rhcs_iso_path: /root/rhceph-4.1-rhel-7-x86_64.iso
ceph_rhcs_version: 4
bootstrap_dirs_owner: &quot;167&quot;
bootstrap_dirs_group: &quot;167&quot;
dashboard_admin_user: admin
dashboard_admin_password: Redhat!23
node_exporter_container_image: registry.redhat.io/openshift4/ose-prometheus-node-exporter:v4.1
grafana_admin_user: admin
grafana_admin_password: Redhat!23
grafana_container_image: registry.redhat.io/rhceph/rhceph-4-dashboard-rhel8
prometheus_container_image: registry.redhat.io/openshift4/ose-prometheus:4.1
alertmanager_container_image: registry.redhat.io/openshift4/ose-prometheus-alertmanager:4.1
radosgw_interface: eth1
radosgw_address_block: 192.168.18.0/24
radosgw_civetweb_port: 8080
radosgw_civetweb_num_threads: 512
ceph_conf_overrides:
  global:
    osd_pool_default_size: 3
    osd_pool_default_min_size: 2
    osd_pool_default_pg_num: 32
    osd_pool_default_pgp_num: 32
  osd:
   osd_scrub_begin_hour: 22
   osd_scrub_end_hour: 7

EOF

cat &lt;&lt; EOF &gt; ./group_vars/osds.yml
devices:
  - /dev/vdb
  - /dev/vdc
  - /dev/vdd
EOF

cat &lt;&lt; EOF &gt; ./hosts
[grafana-server]
ceph
[mons]
ceph
[osds]
ceph
[mgrs]
ceph

EOF

sed -i &quot;s/#copy_admin_key: false/copy_admin_key: true/&quot; ./group_vars/rgws.yml

cd /usr/share/ceph-ansible

mkdir -p ~/ceph-ansible-keys
ansible all -m ping -i hosts

ansible-playbook -vv site.yml -i hosts

#  You can access your dashboard web UI at http://ceph:8443/ as an 'admin' user with 'Redhat!23' password

cd /root
ceph osd getcrushmap -o crushmap
crushtool -d crushmap -o crushmap.txt
sed -i 's/step chooseleaf firstn 0 type host/step chooseleaf firstn 0 type osd/' crushmap.txt
grep 'step chooseleaf' crushmap.txt
crushtool -c crushmap.txt -o crushmap-new
ceph osd setcrushmap -i crushmap-new
cd /usr/share/ceph-ansible

# test the result
ceph health detail
ceph osd pool create test 8
ceph osd pool set test pg_num 128
ceph osd pool set test pgp_num 128
ceph osd pool application enable test rbd
ceph -s
ceph osd tree
ceph osd pool ls
ceph pg dump
cat &lt;&lt; EOF &gt; hello-world.txt
wangzheng
EOF
rados --pool test put hello-world hello-world.txt
rados --pool test get hello-world fetch.txt
cat fetch.txt

# continue to install
cat &lt;&lt; EOF &gt;&gt; ./hosts
[rgws]
ceph
[mdss]
ceph

EOF

ansible-playbook -vv site.yml --limit mdss -i hosts

ansible-playbook -vv site.yml --limit rgws -i hosts

# change mon param for S3
# 416 (InvalidRange)
# https://www.cnblogs.com/flytor/p/11380026.html
# https://www.cnblogs.com/fuhai0815/p/12144214.html
# https://access.redhat.com/solutions/3328431
# add config line
vi /etc/ceph/ceph.conf
# mon_max_pg_per_osd = 300

systemctl restart ceph-mon@ceph.service

ceph tell mon.* injectargs '--mon_max_pg_per_osd=1000'

ceph --admin-daemon /var/run/ceph/ceph-mon.`hostname -s`.asok config show | grep mon_max_pg_per_osd

ceph --admin-daemon /var/run/ceph/ceph-mgr.`hostname -s`.asok config set mon_max_pg_per_osd 1000

ceph osd lspools
ceph osd dump | grep 'replicated size'

</code></pre>
<h2 id="install-ocs"><a class="header" href="#install-ocs">install ocs</a></h2>
<p>接下来在openshift4里面安装ocs组件，来对接之前安装的ceph节点。</p>
<pre><code class="language-bash"># check ceph versino
ceph tell osd.* version

python ceph-external-cluster-details-exporter.py --help

python ceph-external-cluster-details-exporter.py --rbd-data-pool-name test --rgw-endpoint 192.168.18.203:8080 --run-as-user client.ocs
# [{&quot;kind&quot;: &quot;ConfigMap&quot;, &quot;data&quot;: {&quot;maxMonId&quot;: &quot;0&quot;, &quot;data&quot;: &quot;ceph=192.168.18.203:6789&quot;, &quot;mapping&quot;: &quot;{}&quot;}, &quot;name&quot;: &quot;rook-ceph-mon-endpoints&quot;}, {&quot;kind&quot;: &quot;Secret&quot;, &quot;data&quot;: {&quot;mon-secret&quot;: &quot;mon-secret&quot;, &quot;fsid&quot;: &quot;bfaeb4fb-2f44-41e7-9539-1ca75bb394a8&quot;, &quot;cluster-name&quot;: &quot;openshift-storage&quot;, &quot;admin-secret&quot;: &quot;admin-secret&quot;}, &quot;name&quot;: &quot;rook-ceph-mon&quot;}, {&quot;kind&quot;: &quot;Secret&quot;, &quot;data&quot;: {&quot;userKey&quot;: &quot;AQBZUWdfavnEDBAA0qwn1WLRbFV+0bUY+8ZnMQ==&quot;, &quot;userID&quot;: &quot;client.ocs&quot;}, &quot;name&quot;: &quot;rook-ceph-operator-creds&quot;}, {&quot;kind&quot;: &quot;Secret&quot;, &quot;data&quot;: {&quot;userKey&quot;: &quot;AQBZUWdfC1EzDhAAjVV7+S3jKk8LcPUxxkIF9A==&quot;, &quot;userID&quot;: &quot;csi-rbd-node&quot;}, &quot;name&quot;: &quot;rook-csi-rbd-node&quot;}, {&quot;kind&quot;: &quot;StorageClass&quot;, &quot;data&quot;: {&quot;pool&quot;: &quot;test&quot;}, &quot;name&quot;: &quot;ceph-rbd&quot;}, {&quot;kind&quot;: &quot;Secret&quot;, &quot;data&quot;: {&quot;userKey&quot;: &quot;AQBZUWdfG8pvEBAAnldlqNj72gqBRvSxc8FB+g==&quot;, &quot;userID&quot;: &quot;csi-rbd-provisioner&quot;}, &quot;name&quot;: &quot;rook-csi-rbd-provisioner&quot;}, {&quot;kind&quot;: &quot;Secret&quot;, &quot;data&quot;: {&quot;adminID&quot;: &quot;csi-cephfs-provisioner&quot;, &quot;adminKey&quot;: &quot;AQBZUWdfCxXWExAAiiaU1KIyjFsBxZB6h9WVtw==&quot;}, &quot;name&quot;: &quot;rook-csi-cephfs-provisioner&quot;}, {&quot;kind&quot;: &quot;Secret&quot;, &quot;data&quot;: {&quot;adminID&quot;: &quot;csi-cephfs-node&quot;, &quot;adminKey&quot;: &quot;AQBZUWdf52L9ERAAXbK5upV2lO5phttDrwzJyg==&quot;}, &quot;name&quot;: &quot;rook-csi-cephfs-node&quot;}, {&quot;kind&quot;: &quot;StorageClass&quot;, &quot;data&quot;: {&quot;pool&quot;: &quot;cephfs_data&quot;, &quot;fsName&quot;: &quot;cephfs&quot;}, &quot;name&quot;: &quot;cephfs&quot;}, {&quot;kind&quot;: &quot;StorageClass&quot;, &quot;data&quot;: {&quot;endpoint&quot;: &quot;192.168.18.203:8080&quot;, &quot;poolPrefix&quot;: &quot;default&quot;}, &quot;name&quot;: &quot;ceph-rgw&quot;}]

oc get cephcluster -n openshift-storage

oc get storagecluster -n openshift-storage

# install chrome on kvm host
wget https://dl.google.com/linux/direct/google-chrome-stable_current_x86_64.rpm
yum install ./google-chrome-stable_current_*.rpm
google-chrome &amp;

</code></pre>
<h2 id="install-cnv"><a class="header" href="#install-cnv">install cnv</a></h2>
<pre><code class="language-bash"># upload win10.qcow2 to http server(helper)
scp win10.qcow2.gz root@192.168.8.202:/var/www/html/

# on helper
chmod 644 /var/www/html/win10.qcow2.gz

oc project demo
cat &lt;&lt; EOF &gt; win10.dv.yaml
apiVersion: cdi.kubevirt.io/v1alpha1
kind: DataVolume
metadata:
  name: &quot;example-import-dv-win10&quot;
spec:
  source:
      http:
         url: &quot;http://192.168.8.202:8080/win10.qcow2.gz&quot; 
  pvc:
    volumeMode: Block
    storageClassName: ocs-external-storagecluster-ceph-rbd
    accessModes:
      - ReadWriteMany
    resources:
      requests:
        storage: &quot;40Gi&quot;
EOF
oc apply -n demo -f win10.dv.yaml

oc get dv,pvc

# create a vm, and test the live migration

###############################################################
# network

#####################################
# worker4 baremetal, nic bond + vlan + bridge for business
nmcli con add type bond \
    con-name bond-13 \
    ifname bond-13 \
    mode 802.3ad ipv4.method disabled ipv6.method ignore
    
nmcli con mod id bond-13 bond.options \
    mode=802.3ad,miimon=100,lacp_rate=fast,xmit_hash_policy=layer2+3
    
nmcli con add type bond-slave ifname ens49f1 con-name ens49f1 master bond-13
nmcli con add type bond-slave ifname ens35f1 con-name ens35f1 master bond-13

nmcli con up bond-13

nmcli connection add type bridge con-name br-business ifname br-business ip4 172.17.4.211/24

nmcli con up br-business

nmcli con add type vlan con-name vlan-business ifname vlan-business dev bond-13 id 991 master br-business slave-type bridge

nmcli con up vlan-business

#####################################
# worker4 baremetal, nic bond + vlan + bridge for business
nmcli con add type bond \
    con-name bond-13 \
    ifname bond-13 \
    mode 802.3ad ipv4.method disabled ipv6.method ignore
    
nmcli con mod id bond-13 bond.options \
    mode=802.3ad,miimon=100,lacp_rate=fast,xmit_hash_policy=layer2+3
    
nmcli con add type bond-slave ifname ens49f1 con-name ens49f1 master bond-13
nmcli con add type bond-slave ifname ens35f1 con-name ens35f1 master bond-13

nmcli con up bond-13

nmcli connection add type bridge con-name br-business ifname br-business ip4 172.17.4.210/24

nmcli con up br-business

nmcli con add type vlan con-name vlan-business ifname vlan-business dev bond-13 id 991 master br-business slave-type bridge

nmcli con up vlan-business

###############################
# try to add 2nd nic
cat &lt;&lt; EOF &gt; nic.vm.yaml
apiVersion: &quot;k8s.cni.cncf.io/v1&quot;
kind: NetworkAttachmentDefinition
metadata:
  name: bridge-network-business
  annotations:
    k8s.v1.cni.cncf.io/resourceName: bridge.network.kubevirt.io/br-business 
spec:
  config: '{
    &quot;cniVersion&quot;: &quot;0.3.1&quot;,
    &quot;name&quot;: &quot;bridge-network-business&quot;, 
    &quot;plugins&quot;: [
      {
        &quot;type&quot;: &quot;cnv-bridge&quot;, 
        &quot;bridge&quot;: &quot;br-business&quot; 
      },
      {
        &quot;type&quot;: &quot;cnv-tuning&quot; 
      }
    ]
  }'
EOF

</code></pre>
<h2 id="cs-游戏业务场景测试"><a class="header" href="#cs-游戏业务场景测试">CS 游戏业务场景测试</a></h2>
<pre><code class="language-bash">
###################################
# add management vlan to kvm host

nmcli con add type vlan con-name vlan-management ifname vlan-management dev bond-24 id 500  ip4 1.41.0.124/27

nmcli con up vlan-management

#restore
nmcli con del vlan-management

# upload cs server image
# for python3
python -m http.server 7800
# for python2
python -m SimpleHTTPServer 7800

oc project demo
cat &lt;&lt; EOF &gt; cnv.cs.dv.yaml
apiVersion: cdi.kubevirt.io/v1alpha1
kind: DataVolume
metadata:
  name: &quot;import-dv-cs-yitu&quot;
spec:
  source:
      http:
         url: &quot;http://192.168.8.251:7800/yitu.raw&quot; 
  pvc:
    volumeMode: Block
    storageClassName: ocs-external-storagecluster-ceph-rbd
    accessModes:
      - ReadWriteMany
    resources:
      requests:
        storage: &quot;150Gi&quot;
EOF
oc apply -n demo -f cnv.cs.dv.yaml

oc get dv,pvc


</code></pre>
<p>业务测试服务器是一个CS业务，还是个ubuntu14，我们启动这个虚机，并且配置他的网络。
interface /etc/network/interfaces.d/eth0.cfg for cs server (ubuntu 14)</p>
<pre><code class="language-yaml"># The primary network interface
auto eth0
iface eth0 inet static
    address 172.17.4.215
    netmask 255.255.255.0
    gateway 172.17.4.254
    dns-nameservers 114.114.114.114
</code></pre>
<pre><code class="language-bash">ifdown eth0
ifup eth0
</code></pre>
<h2 id="cnv-live-migration"><a class="header" href="#cnv-live-migration">cnv live migration</a></h2>
<pre><code class="language-bash"># upload cs server image
# for python3
python -m http.server 7800
# for python2
python -m SimpleHTTPServer 7800

oc project demo
cat &lt;&lt; EOF &gt; cnv.cs.dv.yaml
apiVersion: cdi.kubevirt.io/v1alpha1
kind: DataVolume
metadata:
  name: &quot;import-dv-rhel-78&quot;
spec:
  source:
      http:
         url: &quot;http://192.168.8.251:7800/rhel7.8.img&quot; 
  pvc:
    volumeMode: Block
    storageClassName: ocs-external-storagecluster-ceph-rbd
    accessModes:
      - ReadWriteMany
    resources:
      requests:
        storage: &quot;10Gi&quot;
EOF
oc apply -n demo -f cnv.cs.dv.yaml

oc get dv,pvc

############################################
# try to debug the vm stuck after node failure, but find out this is not working.
# we try to decrease the pdb, but no use, vm still not move to another node. 
oc get pdb -n demo
# NAME                               MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
# kubevirt-disruption-budget-j5zlc   2               N/A               0                     12m
# kubevirt-disruption-budget-qsk9j   2               N/A               0                     12m

oc patch pdb kubevirt-disruption-budget-j5zlc -n demo --type=merge -p '{&quot;spec&quot;:{&quot;minAvailable&quot;:0}}'
oc patch pdb kubevirt-disruption-budget-qsk9j -n demo --type=merge -p '{&quot;spec&quot;:{&quot;minAvailable&quot;:0}}'

# Cannot evict pod as it would violate the pod's disruption budget.
oc adm drain worker-3.ocp4.redhat.ren --grace-period=10 --force --delete-local-data --ignore-daemonsets

oc adm uncordon worker-3.ocp4.redhat.ren

</code></pre>
<h2 id="debug-for-node-failure-senario"><a class="header" href="#debug-for-node-failure-senario">debug for node failure senario</a></h2>
<pre><code class="language-bash"># evictionStrategy: LiveMigrate
# power off and power on the VM 

# remove evictionStrategy: LiveMigrate settings, 
# and find out this doesn't work
oc patch -n demo vm/rhel78 --type json  -p '[{&quot;op&quot;: &quot;remove&quot;, &quot;path&quot;: &quot;/spec/template/spec/evictionStrategy&quot;}]'
oc get vm/rhel78 -o yaml | grep evictionStrategy

# restore evictionStrategy: LiveMigrate settings
oc patch -n demo vm/rhel78 --type=merge -p '{&quot;spec&quot;: {&quot;template&quot;: {&quot;spec&quot;: {&quot;evictionStrategy&quot;:&quot;LiveMigrate&quot;} } } }'

# oc delete pod -n openshift-storage noobaa-db-0 --force --grace-period=0
# oc get pod -n openshift-storage

# no result out for these 2 command
oc get pod/virt-launcher-rhel78-r6d9m -o yaml | grep -A2 finalizer
oc get vm/rhel78 -o yaml | grep -A2 finalizer

# we can see there are finalizers on vmi
oc get vmi/rhel78 -o yaml | grep -A2 finalizer
  # finalizers:
  # - foregroundDeleteVirtualMachine
  # generation: 20

# poweroff the node, to reproduce the issue
# when the node is notready, and pod is terminating
oc get node
# NAME                       STATUS     ROLES        AGE    VERSION
# master-0.ocp4.redhat.ren   Ready      master       102d   v1.18.3+6c42de8
# master-1.ocp4.redhat.ren   Ready      master       102d   v1.18.3+6c42de8
# master-2.ocp4.redhat.ren   Ready      master       102d   v1.18.3+6c42de8
# worker-0.ocp4.redhat.ren   Ready      worker       102d   v1.18.3+6c42de8
# worker-1.ocp4.redhat.ren   Ready      worker       102d   v1.18.3+6c42de8
# worker-2.ocp4.redhat.ren   Ready      worker       102d   v1.18.3+6c42de8
# worker-3.ocp4.redhat.ren   NotReady   cnv,worker   93d    v1.18.3+6c42de8
# worker-4.ocp4.redhat.ren   Ready      cnv,worker   91d    v1.18.3+6c42de8
oc get pod
# NAME                          READY   STATUS        RESTARTS   AGE
# v2v-vmware-568b875554-lsj57   1/1     Running       0          2d6h
# virt-launcher-rhel78-r6d9m    1/1     Terminating   0          44m
oc get vmi
# NAME     AGE   PHASE     IP               NODENAME
# rhel78   52m   Running   172.17.4.15/24   worker-3.ocp4.redhat.ren

# below is working
oc patch -n demo vmi/rhel78 --type=merge -p '{&quot;metadata&quot;: {&quot;finalizers&quot;:null}}'

# after node failure, delete vmi
oc delete vmi/rhel78
oc get pod
# NAME                          READY   STATUS        RESTARTS   AGE
# v2v-vmware-568b875554-lsj57   1/1     Running       0          2d6h
# virt-launcher-rhel78-f5ltc    1/1     Running       0          32s
# virt-launcher-rhel78-r6d9m    1/1     Terminating   0          46m

# no use below, because we are bare mental.
cat &lt;&lt; EOF &gt; healthcheck.yaml
apiVersion: machine.openshift.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: example 
  namespace: openshift-machine-api
spec:
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-machine-role: cnv
  unhealthyConditions:
  - type:    &quot;Ready&quot;
    timeout: &quot;300s&quot; 
    status: &quot;False&quot;
  - type:    &quot;Ready&quot;
    timeout: &quot;300s&quot; 
    status: &quot;Unknown&quot;
  maxUnhealthy: &quot;80%&quot; 
EOF
oc apply -f healthcheck.yaml
oc get MachineHealthCheck -n openshift-machine-api
# NAME      MAXUNHEALTHY   EXPECTEDMACHINES   CURRENTHEALTHY
# example   80%

</code></pre>
<h1 id="其他备忘-2"><a class="header" href="#其他备忘-2">其他备忘</a></h1>
<pre><code class="language-bash">oc get nns worker-4.ocp4.redhat.ren -o yaml
</code></pre>
<pre><code class="language-yaml">apiVersion: nmstate.io/v1alpha1
kind: NodeNetworkState
metadata:
  creationTimestamp: &quot;2020-09-16T03:15:51Z&quot;
  generation: 1
  managedFields:
  - apiVersion: nmstate.io/v1alpha1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:ownerReferences:
          .: {}
          k:{&quot;uid&quot;:&quot;135e4844-bf87-465a-8f6a-5fc1f85e5beb&quot;}:
            .: {}
            f:apiVersion: {}
            f:kind: {}
            f:name: {}
            f:uid: {}
      f:status:
        .: {}
        f:currentState:
          .: {}
          f:dns-resolver:
            .: {}
            f:config:
              .: {}
              f:search: {}
              f:server: {}
            f:running:
              .: {}
              f:search: {}
              f:server: {}
          f:interfaces: {}
          f:route-rules:
            .: {}
            f:config: {}
          f:routes:
            .: {}
            f:config: {}
            f:running: {}
        f:lastSuccessfulUpdateTime: {}
    manager: kubernetes-nmstate
    operation: Update
    time: &quot;2020-09-23T01:38:50Z&quot;
  name: worker-4.ocp4.redhat.ren
  ownerReferences:
  - apiVersion: v1
    kind: Node
    name: worker-4.ocp4.redhat.ren
    uid: 135e4844-bf87-465a-8f6a-5fc1f85e5beb
  resourceVersion: &quot;43763614&quot;
  selfLink: /apis/nmstate.io/v1alpha1/nodenetworkstates/worker-4.ocp4.redhat.ren
  uid: 095a8223-d139-4add-9fcf-0e0435191f78
status:
  currentState:
    dns-resolver:
      config:
        search: []
        server:
        - 192.168.8.202
      running:
        search: []
        server:
        - 192.168.8.202
    interfaces:
    - ipv4:
        dhcp: false
        enabled: false
      ipv6:
        autoconf: false
        dhcp: false
        enabled: false
      link-aggregation:
        mode: 802.3ad
        options:
          ad_actor_system: &quot;00:00:00:00:00:00&quot;
          lacp_rate: fast
          miimon: &quot;100&quot;
          xmit_hash_policy: layer2+3
        slaves:
        - ens49f1
        - ens35f1
      mac-address: B8:59:9F:EF:71:5D
      mtu: 1500
      name: bond-13
      state: up
      type: bond
    - ipv4:
        dhcp: false
        enabled: false
      ipv6:
        autoconf: false
        dhcp: false
        enabled: false
      link-aggregation:
        mode: 802.3ad
        options:
          ad_actor_system: &quot;00:00:00:00:00:00&quot;
          lacp_rate: fast
          miimon: &quot;100&quot;
          xmit_hash_policy: layer2+3
        slaves:
        - ens49f0
        - ens35f0
      mac-address: B8:59:9F:EF:71:5C
      mtu: 1500
      name: bond-24
      state: up
      type: bond
    - bridge:
        options:
          group-forward-mask: 0
          mac-ageing-time: 300
          multicast-snooping: true
          stp:
            enabled: true
            forward-delay: 15
            hello-time: 2
            max-age: 20
            priority: 32768
        port:
        - name: vlan-business
          stp-hairpin-mode: false
          stp-path-cost: 100
          stp-priority: 32
      ipv4:
        address:
        - ip: 172.17.4.211
          prefix-length: 24
        dhcp: false
        enabled: true
      ipv6:
        address:
        - ip: fe80::1a6a:4414:8fec:940e
          prefix-length: 64
        auto-dns: true
        auto-gateway: true
        auto-routes: true
        autoconf: true
        dhcp: true
        enabled: true
      mac-address: B8:59:9F:EF:71:5D
      mtu: 1500
      name: br-business
      state: up
      type: linux-bridge
    - ipv4:
        enabled: false
      ipv6:
        enabled: false
      mac-address: 1e:d4:cc:be:5e:49
      mtu: 1450
      name: br0
      state: down
      type: ovs-interface
    - ethernet:
        auto-negotiation: true
        duplex: full
        speed: 10000
        sr-iov:
          total-vfs: 0
          vfs: []
      ipv4:
        dhcp: false
        enabled: false
      ipv6:
        autoconf: false
        dhcp: false
        enabled: false
      mac-address: B8:59:9F:EF:71:5C
      mtu: 1500
      name: ens35f0
      state: up
      type: ethernet
    - ethernet:
        auto-negotiation: true
        duplex: full
        speed: 10000
        sr-iov:
          total-vfs: 0
          vfs: []
      ipv4:
        dhcp: false
        enabled: false
      ipv6:
        autoconf: false
        dhcp: false
        enabled: false
      mac-address: B8:59:9F:EF:71:5D
      mtu: 1500
      name: ens35f1
      state: up
      type: ethernet
    - ipv4:
        enabled: false
      ipv6:
        enabled: false
      mac-address: B4:96:91:67:2D:A4
      mtu: 1500
      name: ens47f0
      state: down
      type: ethernet
    - ethernet:
        auto-negotiation: true
        duplex: full
        speed: 1000
        sr-iov:
          total-vfs: 0
          vfs: []
      ipv4:
        address:
        - ip: 192.168.8.211
          prefix-length: 24
        dhcp: false
        enabled: true
      ipv6:
        address:
        - ip: fe80::b696:91ff:fe67:2da5
          prefix-length: 64
        autoconf: false
        dhcp: false
        enabled: true
      mac-address: B4:96:91:67:2D:A5
      mtu: 1500
      name: ens47f1
      state: up
      type: ethernet
    - ethernet:
        auto-negotiation: true
        duplex: full
        speed: 10000
        sr-iov:
          total-vfs: 0
          vfs: []
      ipv4:
        dhcp: false
        enabled: false
      ipv6:
        autoconf: false
        dhcp: false
        enabled: false
      mac-address: B8:59:9F:EF:71:5C
      mtu: 1500
      name: ens49f0
      state: up
      type: ethernet
    - ethernet:
        auto-negotiation: true
        duplex: full
        speed: 10000
        sr-iov:
          total-vfs: 0
          vfs: []
      ipv4:
        dhcp: false
        enabled: false
      ipv6:
        autoconf: false
        dhcp: false
        enabled: false
      mac-address: B8:59:9F:EF:71:5D
      mtu: 1500
      name: ens49f1
      state: up
      type: ethernet
    - ipv4:
        enabled: false
      ipv6:
        enabled: false
      mtu: 65536
      name: lo
      state: down
      type: unknown
    - ipv4:
        enabled: false
      ipv6:
        enabled: false
      mac-address: de:b2:ca:03:6b:fa
      mtu: 1450
      name: tun0
      state: down
      type: ovs-interface
    - ipv4:
        dhcp: false
        enabled: false
      ipv6:
        autoconf: false
        dhcp: false
        enabled: false
      mac-address: B8:59:9F:EF:71:5D
      mtu: 1500
      name: vlan-business
      state: up
      type: vlan
      vlan:
        base-iface: bond-13
        id: 991
    - ipv4:
        address:
        - ip: 192.168.18.211
          prefix-length: 24
        dhcp: false
        enabled: true
      ipv6:
        address:
        - ip: fe80::e852:70de:e7be:8f04
          prefix-length: 64
        auto-dns: true
        auto-gateway: true
        auto-routes: true
        autoconf: true
        dhcp: true
        enabled: true
      mac-address: B8:59:9F:EF:71:5C
      mtu: 1500
      name: vlan-ceph
      state: up
      type: vlan
      vlan:
        base-iface: bond-24
        id: 501
    - ipv4:
        enabled: false
      ipv6:
        enabled: false
      mac-address: C2:AE:59:84:C6:E0
      mtu: 65000
      name: vxlan_sys_4789
      state: down
      type: vxlan
      vxlan:
        base-iface: &quot;&quot;
        destination-port: 4789
        id: 0
        remote: &quot;&quot;
    route-rules:
      config: []
    routes:
      config:
      - destination: 0.0.0.0/0
        metric: -1
        next-hop-address: 192.168.8.1
        next-hop-interface: ens47f1
        table-id: 0
      running:
      - destination: 172.17.4.0/24
        metric: 425
        next-hop-address: &quot;&quot;
        next-hop-interface: br-business
        table-id: 254
      - destination: 0.0.0.0/0
        metric: 104
        next-hop-address: 192.168.8.1
        next-hop-interface: ens47f1
        table-id: 254
      - destination: 192.168.8.0/24
        metric: 104
        next-hop-address: &quot;&quot;
        next-hop-interface: ens47f1
        table-id: 254
      - destination: 192.168.18.0/24
        metric: 400
        next-hop-address: &quot;&quot;
        next-hop-interface: vlan-ceph
        table-id: 254
      - destination: fe80::/64
        metric: 425
        next-hop-address: &quot;&quot;
        next-hop-interface: br-business
        table-id: 254
      - destination: fe80::/64
        metric: 256
        next-hop-address: &quot;&quot;
        next-hop-interface: ens47f1
        table-id: 254
      - destination: fe80::/64
        metric: 400
        next-hop-address: &quot;&quot;
        next-hop-interface: vlan-ceph
        table-id: 254
      - destination: ff00::/8
        metric: 256
        next-hop-address: &quot;&quot;
        next-hop-interface: br-business
        table-id: 255
      - destination: ff00::/8
        metric: 256
        next-hop-address: &quot;&quot;
        next-hop-interface: ens47f1
        table-id: 255
      - destination: ff00::/8
        metric: 256
        next-hop-address: &quot;&quot;
        next-hop-interface: vlan-ceph
        table-id: 255
  lastSuccessfulUpdateTime: &quot;2020-09-23T01:38:50Z&quot;
</code></pre>
<h1 id="next-step"><a class="header" href="#next-step">next step</a></h1>
<p>Multi-Queue</p>
<ul>
<li>https://kubevirt.io/user-guide/#/creation/disks-and-volumes?id=virtio-block-multi-queue</li>
<li>https://kubevirt.io/user-guide/#/creation/interfaces-and-networks?id=virtio-net-multiqueue</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rhacs--stackrox"><a class="header" href="#rhacs--stackrox">RHACS / stackrox</a></h1>
<p>官方的安装文档，非常详细和准确，照着做就好。</p>
<ul>
<li>https://help.stackrox.com/docs/get-started/quick-start/</li>
</ul>
<p>视频讲解</p>
<p><a href="https://www.bilibili.com/video/BV1bo4y127EQ/"><kbd><img src="ocp4/4.7/imgs/2021-05-08-11-09-00.png" width="600"></kbd></a></p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1bo4y127EQ/">bilibili</a></li>
<li><a href="https://www.ixigua.com/6959765547965743646">xigua</a></li>
<li><a href="https://youtu.be/1WMJ_JQGY1o">youtube</a></li>
</ul>
<h2 id="install-rhacs"><a class="header" href="#install-rhacs">install rhacs</a></h2>
<pre><code class="language-bash"># below is no use for v3.0.59.1
cat &lt;&lt;EOF | oc apply -f -
apiVersion: helm.openshift.io/v1beta1
kind: HelmChartRepository
metadata:
  name: rhacs-repo
spec:
  name: rhacs-repo
  connectionConfig:
    url: http://registry.ocp4.redhat.ren:8080/rhacs-chart/
EOF

# restore
oc delete HelmChartRepository rhacs-repo

mkdir -p /data/install/rhacs
cd /data/install/rhacs

roxctl central generate interactive
# password: redhat

# Enter path to the backup bundle from which to restore keys and certificates (optional):
# Enter PEM cert bundle file (optional):
# Enter administrator password (default: autogenerated):
# Re-enter administrator password:
# Enter orchestrator (k8s, openshift): openshift
# Enter the directory to output the deployment bundle to (default: &quot;central-bundle&quot;):
# Enter the OpenShift major version (3 or 4) to deploy on (default: &quot;0&quot;): 4
# Enter Istio version when deploying into an Istio-enabled cluster (leave empty when not running Istio) (optional):
# Enter the method of exposing Central (route, lb, np, none) (default: &quot;none&quot;): route
# Enter main image to use (default: &quot;stackrox.io/main:3.0.59.1&quot;): registry.redhat.io/rh-acs/main:3.0.59.1
# Enter whether to run StackRox in offline mode, which avoids reaching out to the Internet (default: &quot;false&quot;): true
# Enter whether to enable telemetry (default: &quot;true&quot;):
# Enter the deployment tool to use (kubectl, helm, helm-values) (default: &quot;kubectl&quot;):
# Enter Scanner DB image to use (default: &quot;stackrox.io/scanner-db:2.13.0&quot;): registry.redhat.io/rh-acs/scanner-db:2.13.0
# Enter Scanner image to use (default: &quot;stackrox.io/scanner:2.13.0&quot;): registry.redhat.io/rh-acs/scanner:2.13.0
# Enter Central volume type (hostpath, pvc): pvc
# Enter external volume name (default: &quot;stackrox-db&quot;):
# Enter external volume size in Gi (default: &quot;100&quot;): 100
# Enter storage class name (optional if you have a default StorageClass configured):
# Generating deployment bundle...
# NOTE: Unless run in offline mode, StackRox Kubernetes Security Platform collects and transmits aggregated usage and system health information.  If you want to OPT OUT from this, re-generate the deployment bundle with the '--enable-telemetry=false' flag
# Done!

# Wrote central bundle to &quot;central-bundle&quot;

# To deploy:
#   - If you need to add additional trusted CAs, run central/scripts/ca-setup.sh.

#   - Deploy Central
#     - Run central/scripts/setup.sh
#     - Run oc create -R -f central

#   - Deploy Scanner
#      If you want to run the StackRox Scanner:
#      - Run scanner/scripts/setup.sh
#      - Run oc create -R -f scanner

# PLEASE NOTE: The recommended way to deploy StackRox is by using Helm. If you have
# Helm 3.1+ installed, please consider choosing this deployment route instead. For your
# convenience, all required files have been written to the helm/ subdirectory, along with
# a README file detailing the Helm-based deployment process.

# For administrator login, select the &quot;Login with username/password&quot; option on
# the login page, and log in with username &quot;admin&quot; and the password found in the
# &quot;password&quot; file located in the same directory as this README.

./central-bundle/central/scripts/setup.sh

oc -n stackrox get route central
# NAME      HOST/PORT                               PATH   SERVICES   PORT    TERMINATION   WILDCARD
# central   central-stackrox.apps.ocp4.redhat.ren          central    https   passthrough   None

cat central-bundle/password
# redhat

# open https://central-stackrox.apps.ocp4.redhat.ren 
# with admin / redhat

./central-bundle/scanner/scripts/setup.sh

oc create -R -f central-bundle/scanner
# serviceaccount/scanner created
# clusterrole.rbac.authorization.k8s.io/stackrox-scanner-psp created
# rolebinding.rbac.authorization.k8s.io/stackrox-scanner-psp created
# podsecuritypolicy.policy/stackrox-scanner created
# securitycontextconstraints.security.openshift.io/scanner created
# secret/scanner-db-password created
# secret/scanner-tls created
# secret/scanner-db-tls created
# configmap/scanner-config created
# networkpolicy.networking.k8s.io/scanner created
# networkpolicy.networking.k8s.io/scanner-db created
# deployment.apps/scanner created
# deployment.apps/scanner-db created
# service/scanner created
# service/scanner-db created
# horizontalpodautoscaler.autoscaling/scanner created
</code></pre>
<h2 id="install-sensor"><a class="header" href="#install-sensor">install sensor</a></h2>
<p>sensor是stackrox的runtime扫描器核心，本质上，是一个内核模块/ebpf注入，而且是从容器里面注入，这里面的原理，我会单独做一个视频解释一下。</p>
<p>为了装sensor，我们需要在central平台上，添加集群。登录到系统中，选择系统配置，集群，添加集群：
<img src="ocp4/4.7/imgs/2021-05-07-13-30-06.png" alt="" /></p>
<p>添加集群里面，有2个参数，是sensor的镜像地址，我们当然要用registry.redhat.io的这种不需要申请license的地址了，对应的栏位填写如下信息：</p>
<ul>
<li>registry.redhat.io/rh-acs/main</li>
<li>registry.redhat.io/rh-acs/collector</li>
</ul>
<p><img src="ocp4/4.7/imgs/2021-05-07-16-48-31.png" alt="" /></p>
<p>点击下一步以后，下载一个文件，然后到helper上继续。
<img src="ocp4/4.7/imgs/2021-05-07-17-18-36.png" alt="" /></p>
<pre><code class="language-bash">cd  /data/install/rhacs/

/bin/cp -f ~/Downloads/sensor-ocp4.zip /data/install/rhacs/
unzip -d sensor sensor-ocp4.zip

./sensor/sensor.sh
# namespace/stackrox annotated
# Now using project &quot;stackrox&quot; on server &quot;https://api.ocp4.redhat.ren:6443&quot;.
# Creating sensor secrets...
# secret/sensor-tls created
# Creating sensor RBAC roles...
# serviceaccount/sensor created
# clusterrole.rbac.authorization.k8s.io/stackrox:view-cluster created
# clusterrolebinding.rbac.authorization.k8s.io/stackrox:monitor-cluster created
# role.rbac.authorization.k8s.io/edit created
# rolebinding.rbac.authorization.k8s.io/manage-namespace created
# clusterrole.rbac.authorization.k8s.io/stackrox:edit-workloads created
# clusterrolebinding.rbac.authorization.k8s.io/stackrox:enforce-policies created
# clusterrole.rbac.authorization.k8s.io/stackrox:network-policies created
# clusterrolebinding.rbac.authorization.k8s.io/stackrox:network-policies-binding created
# clusterrole.rbac.authorization.k8s.io/stackrox:update-namespaces created
# clusterrolebinding.rbac.authorization.k8s.io/stackrox:update-namespaces-binding created
# clusterrole.rbac.authorization.k8s.io/stackrox:create-events created
# clusterrolebinding.rbac.authorization.k8s.io/stackrox:create-events-binding created
# clusterrole.rbac.authorization.k8s.io/stackrox:review-tokens created
# clusterrolebinding.rbac.authorization.k8s.io/stackrox:review-tokens-binding created
# Creating sensor security context constraints...
# securitycontextconstraints.security.openshift.io/sensor created
# Creating sensor network policies...
# networkpolicy.networking.k8s.io/sensor created
# Creating sensor pod security policies...
# clusterrole.rbac.authorization.k8s.io/stackrox-sensor-psp created
# rolebinding.rbac.authorization.k8s.io/stackrox-sensor-psp created
# podsecuritypolicy.policy/stackrox-sensor created
# Enter username for docker registry at registry.redhat.io: wandering.star
# Enter password for wandering.star @ registry.redhat.io: secret/collector-stackrox created
# Creating admission controller security context constraints...
# securitycontextconstraints.security.openshift.io/admission-control created
# Creating admission controller secrets...
# secret/admission-control-tls created
# Creating admission controller RBAC roles...
# serviceaccount/admission-control created
# role.rbac.authorization.k8s.io/watch-config created
# rolebinding.rbac.authorization.k8s.io/admission-control-watch-config created
# Creating admission controller network policies...
# networkpolicy.networking.k8s.io/admission-control-no-ingress created
# Creating admission controller pod security policies...
# podsecuritypolicy.policy/stackrox-admission-control created
# clusterrole.rbac.authorization.k8s.io/stackrox-admission-control-psp created
# rolebinding.rbac.authorization.k8s.io/stackrox-admission-control-psp created
# Creating admission controller deployment...
# deployment.apps/admission-control created
# service/admission-control created
# W0507 18:24:56.251769   13915 warnings.go:70] admissionregistration.k8s.io/v1beta1 ValidatingWebhookConfiguration is deprecated in v1.16+, unavailable in v1.22+; use admissionregistration.k8s.io/v1 ValidatingWebhookConfiguration
# W0507 18:24:56.272199   13915 warnings.go:70] admissionregistration.k8s.io/v1beta1 ValidatingWebhookConfiguration is deprecated in v1.16+, unavailable in v1.22+; use admissionregistration.k8s.io/v1 ValidatingWebhookConfiguration
# validatingwebhookconfiguration.admissionregistration.k8s.io/stackrox created
# Creating collector security context constraints...
# securitycontextconstraints.security.openshift.io/collector created
# Creating collector secrets...
# secret/collector-tls created
# Creating collector RBAC roles...
# serviceaccount/collector created
# Creating collector network policies...
# networkpolicy.networking.k8s.io/collector-no-ingress created
# Creating collector pod security policies...
# clusterrole.rbac.authorization.k8s.io/stackrox-collector-psp created
# rolebinding.rbac.authorization.k8s.io/stackrox-collector-psp created
# podsecuritypolicy.policy/stackrox-collector created
# Creating collector daemon set...
# daemonset.apps/collector created
# Creating sensor deployment...
# deployment.apps/sensor created
# service/sensor created
# service/sensor-webhook created
# secret/helm-effective-cluster-name created
# Creating upgrader service account
# serviceaccount/sensor-upgrader created
# clusterrolebinding.rbac.authorization.k8s.io/stackrox:upgrade-sensors created

</code></pre>
<p>我们来简单的窥探一下，装了sensor以后，master node上面dmesg的信息，可以看到有一个collector kmod加载了，并且还用到了CPU指令集的特性。
<img src="ocp4/4.7/imgs/2021-05-07-21-54-34.png" alt="" /></p>
<p>在master node上面，执行lsmod，也能看到这个collector kmod</p>
<pre><code class="language-bash">lsmod | grep coll
# collector             651264  22
</code></pre>
<h2 id="remove-sensor"><a class="header" href="#remove-sensor">remove sensor</a></h2>
<pre><code class="language-bash">cd /data/install/rhacs

# ./sensor/delete-sensor.sh

kubectl delete --raw /apis/security.openshift.io/v1/securitycontextconstraints/collector

./sensor/delete-sensor.sh


</code></pre>
<p>bugfix for https://access.redhat.com/solutions/5911951</p>
<pre><code class="language-bash">cd /data/install/rhacs


</code></pre>
<h2 id="upgrade"><a class="header" href="#upgrade">upgrade</a></h2>
<p>https://help.stackrox.com/docs/upgrade-stackrox/from-44/</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="为-rhacs-找个应用场景-安全合规测试云"><a class="header" href="#为-rhacs-找个应用场景-安全合规测试云">为 RHACS 找个应用场景： 安全合规测试云</a></h1>
<p>视频讲解</p>
<p><a href="https://www.bilibili.com/video/BV1M3411z7mC/"><kbd><img src="ocp4/4.7/imgs/2021-08-05-21-58-35.png" width="600"></kbd></a></p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1M3411z7mC/">bilibili</a></li>
<li><a href="https://youtu.be/sdFymWaaU90">youtube</a></li>
</ul>
<!-- - [xigua](https://www.ixigua.com/6992940047267791391) -->
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="mellanox-cx6-vdpa-硬件卸载-ovs-kernel-方式"><a class="header" href="#mellanox-cx6-vdpa-硬件卸载-ovs-kernel-方式">Mellanox CX6 vdpa 硬件卸载 ovs-kernel 方式</a></h1>
<p>本文来讲解，使用mellanox CX6 dx 网卡，实现vdpa硬件卸载。</p>
<p>视频讲解:</p>
<p><a href="https://www.bilibili.com/video/BV1734y1o7L5/"><kbd><img src="notes/2021/imgs/2021-11-22-08-44-32.png" width="600"></kbd></a></p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1734y1o7L5/">bilibili</a></li>
<li><a href="https://youtu.be/gGlAYXXXWgs">youtube</a></li>
</ul>
<h1 id="vdpa-硬件卸载介绍"><a class="header" href="#vdpa-硬件卸载介绍">vdpa 硬件卸载介绍</a></h1>
<p>既然说到了vdpa卸载，那么我们先简单介绍一下他是什么。</p>
<p>vDPA (virtio data path acceleration) 是一个内核框架，在2020年正式引入内核，NIC厂家会做vDPA网卡，意思是datapath遵循virtio规范，而控制面由厂家驱动提供。</p>
<p>以下是vDPA在虚拟机平台部署时的架构图：</p>
<p><a href="https://www.redhat.com/en/blog/introduction-vdpa-kernel-framework#:%7E:text=a%20VM%20running%20on%20the%20host%3A%C2%A0"><kbd><img src="notes/2021/imgs/2021-10-26-22-54-00.png" width="600"></kbd></a></p>
<p>以下是vDPA在k8s平台中部署是的架构图：</p>
<p><a href="https://www.redhat.com/en/blog/how-vdpa-can-help-network-service-providers-simplify-cnfvnf-certification#:%7E:text=decoupled%20from%20the%20vendor%E2%80%99s%20NIC%3A"><kbd><img src="notes/2021/imgs/2021-11-20-18-20-50.png" width="600"></kbd></a></p>
<p>上面的架构图，是借用红帽介绍vdpa背景的文章。我们这次的实验，是按照<a href="https://docs.mellanox.com/display/MLNXOFEDv53100143/Introduction">mellanox的文档</a>来做，从mellanox角度看，vdpa有2种方式来做</p>
<ol>
<li>配置ovs-dpdk, ovs配置vdpa端口，同时创建socket。vm通过socket挂载vdpa设备。</li>
<li>配置ovs-kernel，启动vdpa-dpdk程序，同时创建socket。vm通过socket挂载vdpa设备。</li>
</ol>
<p>第一种方法，由于ovs-dpdk，mellanox官方文档说只支持到rhel/centos 7 ， 我们的环境是rhel/rocky 8.4，所以我们用后面一种方法。</p>
<p>在这里，背景介绍的很简单，以下是参考链接，可以更深入的学习：</p>
<ul>
<li><a href="https://www.redhat.com/en/blog/introduction-vdpa-kernel-framework">Introduction to vDPA kernel framework</a>
<ul>
<li><a href="https://www.redhat.com/en/blog/vdpa-kernel-framework-part-1-vdpa-bus-abstracting-hardware">vDPA kernel framework part 1: vDPA bus for abstracting hardware</a></li>
<li><a href="https://www.redhat.com/en/blog/vdpa-kernel-framework-part-2-vdpa-bus-drivers-kernel-subsystem-interactions">vDPA kernel framework part 2: vDPA bus drivers for kernel subsystem interactions</a></li>
<li><a href="https://www.redhat.com/en/blog/vdpa-kernel-framework-part-3-usage-vms-and-containers">vDPA kernel framework part 3: usage for VMs and containers</a></li>
</ul>
</li>
<li><a href="https://www.redhat.com/en/blog/how-vhost-user-came-being-virtio-networking-and-dpdk">How vhost-user came into being: Virtio-networking and DPDK</a></li>
<li><a href="https://www.redhat.com/en/blog/journey-vhost-users-realm">A journey to the vhost-users realm</a></li>
<li><a href="https://www.redhat.com/en/blog/how-deep-does-vdpa-rabbit-hole-go">How deep does the vDPA rabbit hole go?</a></li>
<li><a href="https://www.redhat.com/en/blog/achieving-network-wirespeed-open-standard-manner-introducing-vdpa">Achieving network wirespeed in an open standard manner: introducing vDPA</a></li>
<li><a href="https://www.redhat.com/en/blog/vdpa-hands-proof-pudding">vDPA hands on: The proof is in the pudding</a></li>
<li><a href="https://github.com/redhat-nfvpe/vdpa-deployment/blob/master/README.md">vdpa-deployment from redhat-nfvpe</a></li>
<li><a href="https://www.redhat.com/en/virtio-networking-series">Virtio-networking series from redhat blog</a></li>
<li><a href="https://www.redhat.com/en/blog/how-vdpa-can-help-network-service-providers-simplify-cnfvnf-certification">How vDPA can help network service providers simplify CNF/VNF certification</a></li>
<li>vDPA : On the road to production
<ul>
<li><a href="https://static.sched.com/hosted_files/dpdkuserspace2020/ab/vDPA%20-%20DPDK%20Userspace%202020.pdf">pdf</a></li>
<li><a href="https://www.youtube.com/watch?v=H_z5gaIghos">youtube</a></li>
<li><a href="https://dpdkuserspace2020.sched.com/event/e47O/vdpa-integration-status-maxime-coquelin-red-hat">dpdk-userspace-2020</a></li>
</ul>
</li>
<li><a href="http://bonejim.blog.chinaunix.net/uid-28541347-id-5830937.html">vDPA原理和实现</a></li>
<li><a href="https://hackmd.io/@ebhFqyF8QryV_ZWAFURDhQ/HkQLV90yv">VirtIO and TC</a></li>
</ul>
<p>有一个dpdk特殊概念，vf representor，dpdk文档有说，简单理解，是给控制面准备的vf的分身。</p>
<ul>
<li>https://doc.dpdk.org/guides-18.11/prog_guide/switch_representation.html</li>
</ul>
<pre><code>   .-------------.                 .-------------. .-------------.
   | hypervisor  |                 |    VM 1     | |    VM 2     |
   | application |                 | application | | application |
   `--+---+---+--'                 `----------+--' `--+----------'
      |   |   |                               |       |
      |   |   `-------------------.           |       |
      |   `---------.             |           |       |
      |             |             |           |       |
.-----+-----. .-----+-----. .-----+-----.     |       |
| port_id 3 | | port_id 4 | | port_id 5 |     |       |
`-----+-----' `-----+-----' `-----+-----'     |       |
      |             |             |           |       |
    .-+--.    .-----+-----. .-----+-----. .---+--. .--+---.
    | PF |    | VF 1 rep. | | VF 2 rep. | | VF 1 | | VF 2 |
    `-+--'    `-----+-----' `-----+-----' `---+--' `--+---'
      |             |             |           |       |
      |             |   .---------'           |       |
      `-----.       |   |   .-----------------'       |
            |       |   |   |   .---------------------'
            |       |   |   |   |
         .--+-------+---+---+---+--.
         | managed interconnection |
         `------------+------------'
                      |
                 .----+-----.
                 | physical |
                 |  port 0  |
                 `----------'
</code></pre>
<p>本次实验的架构图如下：</p>
<p><img src="notes/2021/dia/2021.ovs.kernel.vdpa.drawio.svg" alt="" /></p>
<h1 id="系统安装"><a class="header" href="#系统安装">系统安装</a></h1>
<pre><code class="language-bash">
export VAR_HOST='rl_panlab105'

# 按照完了操作系统以后，添加kernel参数，主要是intel_iommu=on iommu=pt，然后重启
cp /etc/default/grub /etc/default/grub.bak
sed -i &quot;/GRUB_CMDLINE_LINUX/s/resume=[^[:space:]]*//&quot;  /etc/default/grub
sed -i &quot;/GRUB_CMDLINE_LINUX/s/rd.lvm.lv=${VAR_HOST}\\/swap//&quot;  /etc/default/grub
# https://unix.stackexchange.com/questions/403706/sed-insert-text-after-nth-character-preceding-following-a-given-string
sed -i '/GRUB_CMDLINE_LINUX/s/&quot;/ intel_iommu=on iommu=pt  default_hugepagesz=1G hugepagesz=1G hugepages=16 rdblacklist=nouveau&quot;/2' /etc/default/grub

grub2-mkconfig -o /boot/efi/EFI/rocky/grub.cfg

grub2-mkconfig -o /boot/grub2/grub.cfg

# 添加kvm cpu host mode模式的支持，可以不做
cat &lt;&lt; EOF &gt; /etc/modprobe.d/kvm-nested.conf
options kvm_intel nested=1  
options kvm-intel enable_shadow_vmcs=1   
options kvm-intel enable_apicv=1         
options kvm-intel ept=1                  
EOF

# 默认的操作系统安装，有swap, home分区，我们是测试系统，全都删了吧。
umount /home
swapoff  /dev/$VAR_HOST/swap

cp /etc/fstab /etc/fstab.bak
sed -i 's/^[^#]*home/#&amp;/' /etc/fstab
sed -i 's/^[^#]*swap/#&amp;/' /etc/fstab

lvremove -f /dev/$VAR_HOST/home
lvremove -f /dev/$VAR_HOST/swap

lvextend -l +100%FREE /dev/$VAR_HOST/root
xfs_growfs /dev/$VAR_HOST/root

# 至此，开始安装网卡驱动
# 103 driver install
# https://www.mellanox.com/products/infiniband-drivers/linux/mlnx_ofed
mkdir -p /data/down/
cd /data/down/
dnf groupinstall -y 'Development Tools'
dnf groupinstall -y &quot;Server with GUI&quot;

wget https://www.mellanox.com/downloads/ofed/MLNX_OFED-5.4-3.0.3.0/MLNX_OFED_LINUX-5.4-3.0.3.0-rhel8.4-x86_64.tgz
tar zvxf *.tgz
cd /data/down/MLNX_OFED_LINUX-5.4-3.0.3.0-rhel8.4-x86_64
dnf install -y tcl tk kernel-modules-extra python36 make gcc-gfortran tcsh unbound
./mlnxofedinstall --all --force --distro rhel8.4
# ./mlnxofedinstall --dpdk --ovs-dpdk --upstream-libs --add-kernel-support --force --distro rhel8.4

reboot

systemctl enable --now mst
systemctl enable --now openibd

cat &lt;&lt; EOF &gt; /etc/yum.repos.d/mlx.repo
[mlnx_ofed]
name=MLNX_OFED Repository
baseurl=file:///data/down/MLNX_OFED_LINUX-5.4-3.0.3.0-rhel8.4-x86_64/RPMS
enabled=1
gpgcheck=0
EOF

dnf makecache 

# 开始安装dpdk相关的软件
mkdir -p /data/soft
cd /data/soft

dnf config-manager --set-enabled powertools
dnf install -y ninja-build meson

# 装mlnx版本的dpdk组件和ovs软件
# dnf group list
# dnf groupinstall -y 'Development Tools'
# install dpdk
dnf install -y mlnx-dpdk mlnx-dpdk-devel numactl-devel openvswitch  openvswitch-selinux-policy libnl3-devel openssl-devel zlib-devel libpcap-devel elfutils-libelf-devel 
# https://doc.dpdk.org/guides/linux_gsg/sys_reqs.html#compilation-of-the-dpdk
pip3 install --user pyelftools

systemctl enable --now openvswitch

export PATH=$PATH:/opt/mellanox/dpdk/bin/
echo 'export PATH=$PATH:/opt/mellanox/dpdk/bin/' &gt;&gt; ~/.bash_profile

# 编译上游的dpdk软件包，因为我们要用里面的vdpa sample程序
cd /data/soft/
wget https://fast.dpdk.org/rel/dpdk-20.11.3.tar.xz
tar vxf dpdk-20.11.3.tar.xz
# https://core.dpdk.org/doc/quick-start/
cd /data/soft/dpdk-stable-20.11.3/
# meson -Dexamples=all build
meson --reconfigure -Dexamples=all build
ninja -C build

export PKG_CONFIG_PATH=/opt/mellanox/dpdk/lib64/pkgconfig/
cd /data/soft/dpdk-stable-20.11.3/examples/vdpa
make -j 

# 按照kvm相关软件包
# install kvm with qemu
# dnf -y groupinstall &quot;Server with GUI&quot;

dnf -y install qemu-kvm libvirt libguestfs-tools virt-install virt-viewer virt-manager tigervnc-server

systemctl disable --now firewalld
systemctl enable --now libvirtd

# 最后，设置mlx网卡参数，激活sriov
# pci地址，使用 lspci -D | grep -i mell 或者 lshw -c network -businfo 得到
lspci -D | grep -i mell
# 0000:04:00.0 Ethernet controller: Mellanox Technologies MT2892 Family [ConnectX-6 Dx]
# 0000:04:00.1 Ethernet controller: Mellanox Technologies MT2892 Family [ConnectX-6 Dx]

lshw -c network -businfo
# Bus info          Device     Class          Description
# =======================================================
# pci@0000:02:00.0  eno3       network        NetXtreme BCM5720 2-port Gigabit Ethernet PCIe
# pci@0000:02:00.1  eno4       network        NetXtreme BCM5720 2-port Gigabit Ethernet PCIe
# pci@0000:01:00.0  eno1       network        NetXtreme BCM5720 2-port Gigabit Ethernet PCIe
# pci@0000:01:00.1  eno2       network        NetXtreme BCM5720 2-port Gigabit Ethernet PCIe
# pci@0000:04:00.0  enp4s0f0   network        MT2892 Family [ConnectX-6 Dx]
# pci@0000:04:00.1  enp4s0f1   network        MT2892 Family [ConnectX-6 Dx]

# UCTX_EN is for enable DevX
# DevX allows to access firmware objects
mlxconfig -y -d 0000:04:00.0 set SRIOV_EN=1 UCTX_EN=1 NUM_OF_VFS=8
</code></pre>
<h1 id="ovs-kernel-方案"><a class="header" href="#ovs-kernel-方案">ovs-kernel 方案</a></h1>
<p>网卡设置脚本</p>
<pre><code class="language-bash"># mlx默认的ovs，缺少一些selinux的配置，在此补上
# 项目上，可以根据需要，自行补充缺少的selinux配置
semodule -i wzh-mellanox-ovs-dpdk.pp

# 这里做了一个配置和启动ovs的脚步，逻辑是先清空ovs配置，再配置网卡模式，然后启动ovs
cat &lt;&lt; 'EOF' &gt; /data/ovs-offload-env.sh
#!/usr/bin/env bash

set -e
set -x

systemctl restart openvswitch
ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-init=try
systemctl restart openvswitch

ip link set dev ${IFNAME} down || true
ip link set dev ${IFNAME}_0 down || true
ip link set dev ${IFNAME}_1 down || true

ip link set dev ${IFNAME}v0 down || true
ip link set dev ${IFNAME}v1 down || true

ovs-vsctl del-port ovs-sriov ${IFNAME} || true
ovs-vsctl del-port ovs-sriov ${IFNAME}_0 || true
ovs-vsctl del-port ovs-sriov ${IFNAME}_1 || true
ovs-vsctl del-br ovs-sriov || true

ovs-vsctl del-port br0-ovs pf0vf0 || true
ovs-vsctl del-port br0-ovs pf0vf1 || true
ovs-vsctl del-port br0-ovs pf0 || true
ovs-vsctl del-br br0-ovs || true

ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-init=false
ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-extra=&quot; &quot;
ovs-vsctl --no-wait set Open_vSwitch . other_config={}

# Turn off SR-IOV on the PF device. 
echo 0 &gt; /sys/class/net/$IFNAME/device/sriov_numvfs
cat /sys/class/net/$IFNAME/device/sriov_numvfs
# 0

systemctl restart openvswitch

# Turn ON SR-IOV on the PF device. 
echo 2 &gt; /sys/class/net/$IFNAME/device/sriov_numvfs
cat /sys/class/net/$IFNAME/device/sriov_numvfs
# 2

ip link set $IFNAME vf 0 mac ${VF1MAC}
ip link set $IFNAME vf 1 mac ${VF2MAC}

echo ${PCINUM%%.*}.2 &gt; /sys/bus/pci/drivers/mlx5_core/unbind || true
echo ${PCINUM%%.*}.3 &gt; /sys/bus/pci/drivers/mlx5_core/unbind || true

devlink dev eswitch set pci/$PCINUM mode switchdev
devlink dev eswitch show pci/$PCINUM
# # pci/0000:43:00.0: mode switchdev inline-mode none encap-mode basic

echo ${PCINUM%%.*}.2 &gt; /sys/bus/pci/drivers/mlx5_core/bind
echo ${PCINUM%%.*}.3 &gt; /sys/bus/pci/drivers/mlx5_core/bind

# systemctl enable --now openvswitch
# systemctl restart openvswitch

# Create an OVS bridge (here it's named ovs-sriov). 
ovs-vsctl add-br ovs-sriov

ovs-vsctl set Open_vSwitch . other_config:hw-offload=true

systemctl restart openvswitch

ovs-vsctl add-port ovs-sriov ${IFNAME}
ovs-vsctl add-port ovs-sriov ${IFNAME}_0
ovs-vsctl add-port ovs-sriov ${IFNAME}_1

ip link set dev ${IFNAME} up
ip link set dev ${IFNAME}_0 up
ip link set dev ${IFNAME}_1 up

ip link set dev ${IFNAME}v0 up
ip link set dev ${IFNAME}v1 up

# systemctl restart openvswitch

# ip addr add ${VF1IP} dev ${IFNAME}v0
# ip addr add ${VF2IP} dev ${IFNAME}v1

EOF

# for 103
# export IFNAME=enp4s0f0
# export PCINUM=0000:04:00.0
# export VF1MAC=e4:11:22:33:44:50
# export VF2MAC=e4:11:22:33:44:51
# export VF1IP=192.168.55.21/24
# export VF2IP=192.168.55.22/24
# bash /data/ovs-offload-env.sh

# 设置一下环境变量，就可以执行脚本，启动ovs了。
# for 105
export IFNAME=enp67s0f0
export PCINUM=0000:43:00.0
export VF1MAC=e4:11:22:33:55:60
export VF2MAC=e4:11:22:33:55:61
# export VF1IP=192.168.55.31/24
# export VF2IP=192.168.55.32/24
bash /data/ovs-offload-env.sh

# 我们还需要启动一个DPDK的程序，做vdpa的功能，并接到vf上去。
/data/soft/dpdk-stable-20.11.3/examples/vdpa/build/vdpa -w ${PCINUM%%.*}.2,class=vdpa --log-level=pmd,info -- -i
create /tmp/sock-virtio0 0000:43:00.2
# EAL: Detected 24 lcore(s)
# EAL: Detected 2 NUMA nodes
# Option -w, --pci-whitelist is deprecated, use -a, --allow option instead
# EAL: Detected shared linkage of DPDK
# EAL: Multi-process socket /var/run/dpdk/rte/mp_socket
# EAL: Selected IOVA mode 'VA'
# EAL: No available hugepages reported in hugepages-2048kB
# EAL: Probing VFIO support...
# EAL: Probe PCI driver: mlx5_pci (15b3:101e) device: 0000:43:00.2 (socket 1)
# mlx5_vdpa: ROCE is disabled by Netlink successfully.
# EAL: No legacy callbacks, legacy socket not created
# Interactive-mode selected
# vdpa&gt; create /tmp/sock-virtio0 0000:43:00.2
# VHOST_CONFIG: vhost-user server: socket created, fd: 112
# VHOST_CONFIG: bind to /tmp/sock-virtio0
# vdpa&gt;

vdpa&gt; list
# device name     queue num       supported features
# 0000:43:00.2            256             0x114c60180b

vdpa&gt; stats 0000:43:00.2 0
# Device 0000:43:00.2:
#         Virtq 0:
#                 received_descriptors                                             1024
#                 completed_descriptors                                            39
#                 bad descriptor errors                                            0
#                 exceed max chain                                                 0
#                 invalid buffer                                                   0
#                 completion errors                                                0

</code></pre>
<h1 id="kvm"><a class="header" href="#kvm">kvm</a></h1>
<p>接下来，我们就要创建一个kvm，来使用我们的vdpa通道。</p>
<p>由于我们创建了一个socket，需要qemu有权限读取这个socket，所以我们需要把qemu的用户改为root。</p>
<pre><code class="language-bash">sed -i.bak 's/#user = &quot;root&quot;/user = &quot;root&quot;/' /etc/libvirt/qemu.conf

# 我们还需要创建一个网桥，让kvm能接住宿主机的网口能上网。方便访问和管理。
mkdir -p /data/kvm
cat &lt;&lt; 'EOF' &gt; /data/kvm/bridge.sh
#!/usr/bin/env bash

PUB_CONN='eno1'
PUB_IP='172.21.6.103/24'
PUB_GW='172.21.6.254'
PUB_DNS='172.21.1.1'

nmcli con down &quot;$PUB_CONN&quot;
nmcli con delete &quot;$PUB_CONN&quot;
nmcli con down baremetal
nmcli con delete baremetal
# RHEL 8.1 appends the word &quot;System&quot; in front of the connection,delete in case it exists
nmcli con down &quot;System $PUB_CONN&quot;
nmcli con delete &quot;System $PUB_CONN&quot;
nmcli connection add ifname baremetal type bridge con-name baremetal ipv4.method 'manual' \
    ipv4.address &quot;$PUB_IP&quot; \
    ipv4.gateway &quot;$PUB_GW&quot; \
    ipv4.dns &quot;$PUB_DNS&quot;
    
nmcli con add type bridge-slave ifname &quot;$PUB_CONN&quot; master baremetal
nmcli con down &quot;$PUB_CONN&quot;;pkill dhclient;dhclient baremetal
nmcli con up baremetal
EOF
bash /data/kvm/bridge.sh

# 我们先用标准的方法，创建，启动和安装一个kvm
cd /data/kvm
export DOMAIN=cx6.1

virt-install --name=&quot;${DOMAIN}&quot; --vcpus=2 --ram=8192 \
--cputune vcpupin0.vcpu=14,vcpupin1.vcpu=16 \
--memorybacking hugepages.page0.size=1,hugepages.page0.unit=GiB \
--cpu host-model \
--disk path=/data/kvm/${DOMAIN}.qcow2,bus=virtio,size=30 \
--os-variant rhel8.4 \
--network bridge=baremetal,model=virtio \
--graphics vnc,port=59000 \
--boot menu=on --location /data/kvm/Rocky-8.4-x86_64-minimal.iso \
--initrd-inject helper-ks-rocky.cfg --extra-args &quot;inst.ks=file:/helper-ks-rocky.cfg&quot; 

# 接下来，配置这个kvm，把vdpa的通道加入到kvm里面。
# https://unix.stackexchange.com/questions/235414/libvirt-how-to-pass-qemu-command-line-args
# virt-xml $DOMAIN --edit --confirm --qemu-commandline 'env=MY-ENV=1234'
virt-xml $DOMAIN --edit --qemu-commandline='-chardev socket,id=charnet1,path=/tmp/sock-virtio0'
virt-xml $DOMAIN --edit --qemu-commandline='-netdev vhost-user,chardev=charnet1,queues=16,id=hostnet1'
virt-xml $DOMAIN --edit --qemu-commandline='-device virtio-net-pci,mq=on,vectors=6,netdev=hostnet1,id=net1,mac=e4:11:c6:d3:45:f2,bus=pcie.0,addr=0x6,page-per-vq=on,rx_queue_size=1024,tx_queue_size=1024'
</code></pre>
<p>接下来，要手动修改如下的配置配置，注意这里cpu binding的核，都应该在一个numa上面。</p>
<pre><code class="language-bash">virsh edit cx6.1
</code></pre>
<pre><code class="language-xml">
  &lt;cputune&gt;
    &lt;vcpupin vcpu='0' cpuset='14'/&gt;
    &lt;vcpupin vcpu='1' cpuset='16'/&gt;
  &lt;/cputune&gt;

  &lt;cpu mode='host-model' check='partial'&gt;
    &lt;numa&gt;
      &lt;cell id='0' cpus='0-1' memory='8388608' unit='KiB' memAccess='shared'/&gt;
    &lt;/numa&gt;
  &lt;/cpu&gt;
  
</code></pre>
<p>最后的配置样例如下，项目中，可以根据以下例子排错。</p>
<pre><code class="language-bash">virsh dumpxml cx6.1
</code></pre>
<pre><code class="language-xml">
&lt;domain type='kvm' id='11' xmlns:qemu='http://libvirt.org/schemas/domain/qemu/1.0'&gt;
  &lt;name&gt;cx6.1&lt;/name&gt;
  &lt;uuid&gt;5cbb6f7c-7122-4fc4-9706-ff46aed3bf25&lt;/uuid&gt;
  &lt;metadata&gt;
    &lt;libosinfo:libosinfo xmlns:libosinfo=&quot;http://libosinfo.org/xmlns/libvirt/domain/1.0&quot;&gt;
      &lt;libosinfo:os id=&quot;http://redhat.com/rhel/8.4&quot;/&gt;
    &lt;/libosinfo:libosinfo&gt;
  &lt;/metadata&gt;
  &lt;memory unit='KiB'&gt;8388608&lt;/memory&gt;
  &lt;currentMemory unit='KiB'&gt;8388608&lt;/currentMemory&gt;
  &lt;memoryBacking&gt;
    &lt;hugepages&gt;
      &lt;page size='1048576' unit='KiB'/&gt;
    &lt;/hugepages&gt;
  &lt;/memoryBacking&gt;
  &lt;vcpu placement='static'&gt;2&lt;/vcpu&gt;
  &lt;cputune&gt;
    &lt;vcpupin vcpu='0' cpuset='14'/&gt;
    &lt;vcpupin vcpu='1' cpuset='16'/&gt;
  &lt;/cputune&gt;
  &lt;resource&gt;
    &lt;partition&gt;/machine&lt;/partition&gt;
  &lt;/resource&gt;
  &lt;os&gt;
    &lt;type arch='x86_64' machine='pc-q35-rhel8.2.0'&gt;hvm&lt;/type&gt;
    &lt;boot dev='hd'/&gt;
    &lt;bootmenu enable='yes'/&gt;
  &lt;/os&gt;
  &lt;features&gt;
    &lt;acpi/&gt;
    &lt;apic/&gt;
  &lt;/features&gt;
  &lt;cpu mode='custom' match='exact' check='full'&gt;
    &lt;model fallback='forbid'&gt;IvyBridge-IBRS&lt;/model&gt;
    &lt;vendor&gt;Intel&lt;/vendor&gt;
    &lt;feature policy='require' name='ss'/&gt;
    &lt;feature policy='require' name='vmx'/&gt;
    &lt;feature policy='require' name='pdcm'/&gt;
    &lt;feature policy='require' name='pcid'/&gt;
    &lt;feature policy='require' name='hypervisor'/&gt;
    &lt;feature policy='require' name='arat'/&gt;
    &lt;feature policy='require' name='tsc_adjust'/&gt;
    &lt;feature policy='require' name='umip'/&gt;
    &lt;feature policy='require' name='md-clear'/&gt;
    &lt;feature policy='require' name='stibp'/&gt;
    &lt;feature policy='require' name='arch-capabilities'/&gt;
    &lt;feature policy='require' name='ssbd'/&gt;
    &lt;feature policy='require' name='xsaveopt'/&gt;
    &lt;feature policy='require' name='pdpe1gb'/&gt;
    &lt;feature policy='require' name='ibpb'/&gt;
    &lt;feature policy='require' name='ibrs'/&gt;
    &lt;feature policy='require' name='amd-stibp'/&gt;
    &lt;feature policy='require' name='amd-ssbd'/&gt;
    &lt;feature policy='require' name='skip-l1dfl-vmentry'/&gt;
    &lt;feature policy='require' name='pschange-mc-no'/&gt;
    &lt;numa&gt;
      &lt;cell id='0' cpus='0-1' memory='8388608' unit='KiB' memAccess='shared'/&gt;
    &lt;/numa&gt;
  &lt;/cpu&gt;
  &lt;clock offset='utc'&gt;
    &lt;timer name='rtc' tickpolicy='catchup'/&gt;
    &lt;timer name='pit' tickpolicy='delay'/&gt;
    &lt;timer name='hpet' present='no'/&gt;
  &lt;/clock&gt;
  &lt;on_poweroff&gt;destroy&lt;/on_poweroff&gt;
  &lt;on_reboot&gt;restart&lt;/on_reboot&gt;
  &lt;on_crash&gt;destroy&lt;/on_crash&gt;
  &lt;pm&gt;
    &lt;suspend-to-mem enabled='no'/&gt;
    &lt;suspend-to-disk enabled='no'/&gt;
  &lt;/pm&gt;
  &lt;devices&gt;
    &lt;emulator&gt;/usr/libexec/qemu-kvm&lt;/emulator&gt;
    &lt;disk type='file' device='disk'&gt;
      &lt;driver name='qemu' type='qcow2'/&gt;
      &lt;source file='/data/kvm/cx6.1.qcow2' index='2'/&gt;
      &lt;backingStore/&gt;
      &lt;target dev='vda' bus='virtio'/&gt;
      &lt;alias name='virtio-disk0'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x04' slot='0x00' function='0x0'/&gt;
    &lt;/disk&gt;
    &lt;disk type='file' device='cdrom'&gt;
      &lt;driver name='qemu'/&gt;
      &lt;target dev='sda' bus='sata'/&gt;
      &lt;readonly/&gt;
      &lt;alias name='sata0-0-0'/&gt;
      &lt;address type='drive' controller='0' bus='0' target='0' unit='0'/&gt;
    &lt;/disk&gt;
    &lt;controller type='usb' index='0' model='qemu-xhci' ports='15'&gt;
      &lt;alias name='usb'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x02' slot='0x00' function='0x0'/&gt;
    &lt;/controller&gt;
    &lt;controller type='sata' index='0'&gt;
      &lt;alias name='ide'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x1f' function='0x2'/&gt;
    &lt;/controller&gt;
    &lt;controller type='pci' index='0' model='pcie-root'&gt;
      &lt;alias name='pcie.0'/&gt;
    &lt;/controller&gt;
    &lt;controller type='pci' index='1' model='pcie-root-port'&gt;
      &lt;model name='pcie-root-port'/&gt;
      &lt;target chassis='1' port='0x10'/&gt;
      &lt;alias name='pci.1'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0' multifunction='on'/&gt;
    &lt;/controller&gt;
    &lt;controller type='pci' index='2' model='pcie-root-port'&gt;
      &lt;model name='pcie-root-port'/&gt;
      &lt;target chassis='2' port='0x11'/&gt;
      &lt;alias name='pci.2'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x1'/&gt;
    &lt;/controller&gt;
    &lt;controller type='pci' index='3' model='pcie-root-port'&gt;
      &lt;model name='pcie-root-port'/&gt;
      &lt;target chassis='3' port='0x12'/&gt;
      &lt;alias name='pci.3'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x2'/&gt;
    &lt;/controller&gt;
    &lt;controller type='pci' index='4' model='pcie-root-port'&gt;
      &lt;model name='pcie-root-port'/&gt;
      &lt;target chassis='4' port='0x13'/&gt;
      &lt;alias name='pci.4'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x3'/&gt;
    &lt;/controller&gt;
    &lt;controller type='pci' index='5' model='pcie-root-port'&gt;
      &lt;model name='pcie-root-port'/&gt;
      &lt;target chassis='5' port='0x14'/&gt;
      &lt;alias name='pci.5'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x4'/&gt;
    &lt;/controller&gt;
    &lt;controller type='pci' index='6' model='pcie-root-port'&gt;
      &lt;model name='pcie-root-port'/&gt;
      &lt;target chassis='6' port='0x15'/&gt;
      &lt;alias name='pci.6'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x5'/&gt;
    &lt;/controller&gt;
    &lt;controller type='pci' index='7' model='pcie-root-port'&gt;
      &lt;model name='pcie-root-port'/&gt;
      &lt;target chassis='7' port='0x16'/&gt;
      &lt;alias name='pci.7'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x6'/&gt;
    &lt;/controller&gt;
    &lt;controller type='virtio-serial' index='0'&gt;
      &lt;alias name='virtio-serial0'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x03' slot='0x00' function='0x0'/&gt;
    &lt;/controller&gt;
    &lt;interface type='bridge'&gt;
      &lt;mac address='52:54:00:8d:b6:8e'/&gt;
      &lt;source bridge='baremetal'/&gt;
      &lt;target dev='vnet2'/&gt;
      &lt;model type='virtio'/&gt;
      &lt;alias name='net0'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x01' slot='0x00' function='0x0'/&gt;
    &lt;/interface&gt;
    &lt;serial type='pty'&gt;
      &lt;source path='/dev/pts/6'/&gt;
      &lt;target type='isa-serial' port='0'&gt;
        &lt;model name='isa-serial'/&gt;
      &lt;/target&gt;
      &lt;alias name='serial0'/&gt;
    &lt;/serial&gt;
    &lt;console type='pty' tty='/dev/pts/6'&gt;
      &lt;source path='/dev/pts/6'/&gt;
      &lt;target type='serial' port='0'/&gt;
      &lt;alias name='serial0'/&gt;
    &lt;/console&gt;
    &lt;channel type='unix'&gt;
      &lt;source mode='bind' path='/var/lib/libvirt/qemu/channel/target/domain-11-cx6.1/org.qemu.guest_agent.0'/&gt;
      &lt;target type='virtio' name='org.qemu.guest_agent.0' state='disconnected'/&gt;
      &lt;alias name='channel0'/&gt;
      &lt;address type='virtio-serial' controller='0' bus='0' port='1'/&gt;
    &lt;/channel&gt;
    &lt;input type='tablet' bus='usb'&gt;
      &lt;alias name='input0'/&gt;
      &lt;address type='usb' bus='0' port='1'/&gt;
    &lt;/input&gt;
    &lt;input type='mouse' bus='ps2'&gt;
      &lt;alias name='input1'/&gt;
    &lt;/input&gt;
    &lt;input type='keyboard' bus='ps2'&gt;
      &lt;alias name='input2'/&gt;
    &lt;/input&gt;
    &lt;graphics type='vnc' port='59000' autoport='no' listen='127.0.0.1'&gt;
      &lt;listen type='address' address='127.0.0.1'/&gt;
    &lt;/graphics&gt;
    &lt;video&gt;
      &lt;model type='qxl' ram='65536' vram='65536' vgamem='16384' heads='1' primary='yes'/&gt;
      &lt;alias name='video0'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x0'/&gt;
    &lt;/video&gt;
    &lt;memballoon model='virtio'&gt;
      &lt;stats period='5'/&gt;
      &lt;alias name='balloon0'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x05' slot='0x00' function='0x0'/&gt;
    &lt;/memballoon&gt;
    &lt;rng model='virtio'&gt;
      &lt;backend model='random'&gt;/dev/urandom&lt;/backend&gt;
      &lt;alias name='rng0'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x06' slot='0x00' function='0x0'/&gt;
    &lt;/rng&gt;
  &lt;/devices&gt;
  &lt;seclabel type='dynamic' model='selinux' relabel='yes'&gt;
    &lt;label&gt;system_u:system_r:svirt_t:s0:c46,c926&lt;/label&gt;
    &lt;imagelabel&gt;system_u:object_r:svirt_image_t:s0:c46,c926&lt;/imagelabel&gt;
  &lt;/seclabel&gt;
  &lt;seclabel type='dynamic' model='dac' relabel='yes'&gt;
    &lt;label&gt;+0:+0&lt;/label&gt;
    &lt;imagelabel&gt;+0:+0&lt;/imagelabel&gt;
  &lt;/seclabel&gt;
  &lt;qemu:commandline&gt;
    &lt;qemu:arg value='-chardev'/&gt;
    &lt;qemu:arg value='socket,id=charnet1,path=/tmp/sock-virtio0'/&gt;
    &lt;qemu:arg value='-netdev'/&gt;
    &lt;qemu:arg value='vhost-user,chardev=charnet1,queues=16,id=hostnet1'/&gt;
    &lt;qemu:arg value='-device'/&gt;
    &lt;qemu:arg value='virtio-net-pci,mq=on,vectors=6,netdev=hostnet1,id=net1,mac=e4:11:c6:d3:45:f2,bus=pcie.0,addr=0x6,page-per-vq=on,rx_queue_size=1024,tx_queue_size=1024'/&gt;
  &lt;/qemu:commandline&gt;
&lt;/domain&gt;

</code></pre>
<h1 id="赶紧试试吧"><a class="header" href="#赶紧试试吧">赶紧试试吧</a></h1>
<p>接下来就进入测试和体验环节。</p>
<pre><code class="language-bash"># in cx6.1 kvm
# nmcli dev connect enp0s6
nmcli con modify enp0s6 ipv4.method manual ipv4.addresses 192.168.99.11/24
# nmcli con modify enp0s6 ipv4.method manual ipv4.addresses 192.168.55.91/24
nmcli con up enp0s6

# on peer machine (102)
nmcli con modify enp66s0f0 ipv4.method manual ipv4.addresses 192.168.99.21/24
# nmcli con modify enp66s0f0 ipv4.method manual ipv4.addresses 192.168.55.92/24
# nmcli dev connect enp66s0f0
nmcli con up enp66s0f0

# run after the tcpdump is running
ping 192.168.99.21
# PING 192.168.99.21 (192.168.99.21) 56(84) bytes of data.
# 64 bytes from 192.168.99.21: icmp_seq=1 ttl=64 time=0.089 ms
# 64 bytes from 192.168.99.21: icmp_seq=2 ttl=64 time=0.044 ms
# 64 bytes from 192.168.99.21: icmp_seq=3 ttl=64 time=0.046 ms
# ....

# on 105
tcpdump -i enp67s0f0_0 -w dump.test
# dropped privs to tcpdump
# tcpdump: listening on enp67s0f0_0, link-type EN10MB (Ethernet), capture size 262144 bytes
# ^C2 packets captured
# 2 packets received by filter
# 0 packets dropped by kernel

tcpdump -i enp67s0f0 -w dump.test
# dropped privs to tcpdump
# tcpdump: listening on enp67s0f0, link-type EN10MB (Ethernet), capture size 262144 bytes
# ^C4 packets captured
# 4 packets received by filter
# 0 packets dropped by kernel

</code></pre>
<p>用 wireshark 打开，可以看到是标准的icmp包，说明我们构建的是数据通路，而不是协议封装。另外，我们会发现，ping了很多包，但是我们只是抓到了1个，这是因为，网卡offload了，我们只能抓到第一个进入内核查流表的包，后面的都网卡offload了，就抓不到了。</p>
<p><img src="notes/2021/imgs/2021-11-19-20-22-47.png" alt="" /></p>
<p>以下是在pf上抓的包，抓到了4个。都是流的第一个包，后面的就都offload啦。</p>
<p><img src="notes/2021/imgs/2021-11-20-20-30-55.png" alt="" /></p>
<pre><code class="language-bash"># ovs-dpctl dump-flows

# on 105
# 看看ovs的流表，可以看到有2个arp(0x0806)的流表(0x0806)，正向和方向
# 还有2个ip(0x0800)的流表，正向和反向
ovs-appctl dpctl/dump-flows type=offloaded
# recirc_id(0),in_port(2),eth(src=0c:42:a1:fa:18:8e,dst=e4:11:c6:d3:45:f2),eth_type(0x0800),ipv4(frag=no), packets:149, bytes:15198, used:0.510s, actions:3
# recirc_id(0),in_port(2),eth(src=0c:42:a1:fa:18:8e,dst=e4:11:c6:d3:45:f2),eth_type(0x0806), packets:0, bytes:0, used:8.700s, actions:3
# recirc_id(0),in_port(3),eth(src=e4:11:c6:d3:45:f2,dst=0c:42:a1:fa:18:8e),eth_type(0x0800),ipv4(frag=no), packets:149, bytes:14602, used:0.510s, actions:2
# recirc_id(0),in_port(3),eth(src=e4:11:c6:d3:45:f2,dst=0c:42:a1:fa:18:8e),eth_type(0x0806), packets:0, bytes:0, used:8.701s, actions:2

# 我们再看看tc的配置，可以看到ovs把配置下发给了tc
# 这里是vf的入流量，可以看到它把流量镜像给了父端口，并且规则由硬件实现
tc -s filter show dev enp67s0f0_0 ingress
# filter protocol ip pref 2 flower chain 0
# filter protocol ip pref 2 flower chain 0 handle 0x1
#   dst_mac 0c:42:a1:fa:18:8e
#   src_mac e4:11:c6:d3:45:f2
#   eth_type ipv4
#   ip_flags nofrag
#   in_hw in_hw_count 1
#         action order 1: mirred (Egress Redirect to device enp67s0f0) stolen
#         index 4 ref 1 bind 1 installed 318 sec used 0 sec
#         Action statistics:
#         Sent 30380 bytes 310 pkt (dropped 0, overlimits 0 requeues 0)
#         Sent software 0 bytes 0 pkt
#         Sent hardware 30380 bytes 310 pkt
#         backlog 0b 0p requeues 0
#         cookie 8be6df4d7d4c33fce08f01a46fa10a4a
#         no_percpu
#         used_hw_stats delayed

# 我们再看看vf的出流量
# 有2个规则，一个是arp，一个是ip
# 都会把流量镜像给了父端口，并且规则由硬件实现
tc -s filter show dev enp67s0f0_0 egress
# filter ingress protocol ip pref 2 flower chain 0
# filter ingress protocol ip pref 2 flower chain 0 handle 0x1
#   dst_mac 0c:42:a1:fa:18:8e
#   src_mac e4:11:c6:d3:45:f2
#   eth_type ipv4
#   ip_flags nofrag
#   in_hw in_hw_count 1
#         action order 1: mirred (Egress Redirect to device enp67s0f0) stolen
#         index 4 ref 1 bind 1 installed 379 sec used 0 sec
#         Action statistics:
#         Sent 36260 bytes 370 pkt (dropped 0, overlimits 0 requeues 0)
#         Sent software 0 bytes 0 pkt
#         Sent hardware 36260 bytes 370 pkt
#         backlog 0b 0p requeues 0
#         cookie 8be6df4d7d4c33fce08f01a46fa10a4a
#         no_percpu
#         used_hw_stats delayed

# filter ingress protocol arp pref 4 flower chain 0
# filter ingress protocol arp pref 4 flower chain 0 handle 0x1
#   dst_mac 0c:42:a1:fa:18:8e
#   src_mac e4:11:c6:d3:45:f2
#   eth_type arp
#   in_hw in_hw_count 1
#         action order 1: mirred (Egress Redirect to device enp67s0f0) stolen
#         index 3 ref 1 bind 1 installed 13 sec used 6 sec
#         Action statistics:
#         Sent 60 bytes 1 pkt (dropped 0, overlimits 0 requeues 0)
#         Sent software 0 bytes 0 pkt
#         Sent hardware 60 bytes 1 pkt
#         backlog 0b 0p requeues 0
#         cookie 1fbfd56eae42f9dbe71bf99bd800cd6d
#         no_percpu
#         used_hw_stats delayed

tc qdisc show dev enp67s0f0_0
# qdisc mq 0: root
# qdisc fq_codel 0: parent :1 limit 10240p flows 1024 quantum 1514 target 5ms interval 100ms memory_limit 32Mb ecn drop_batch 64
# qdisc ingress ffff: parent ffff:fff1 ----------------

# 最后，我们把系统环境记录一下，方便回忆和项目上对比。
# on 105
ip link
# 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
#     link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
# 2: eno1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq master baremetal state UP mode DEFAULT group default qlen 1000
#     link/ether 90:b1:1c:40:59:27 brd ff:ff:ff:ff:ff:ff
# 3: eno2: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc mq state DOWN mode DEFAULT group default qlen 1000
#     link/ether 90:b1:1c:40:59:28 brd ff:ff:ff:ff:ff:ff
# 4: eno3: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc mq state DOWN mode DEFAULT group default qlen 1000
#     link/ether 90:b1:1c:40:59:29 brd ff:ff:ff:ff:ff:ff
# 5: eno4: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc mq state DOWN mode DEFAULT group default qlen 1000
#     link/ether 90:b1:1c:40:59:2a brd ff:ff:ff:ff:ff:ff
# 6: enp67s0f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq master ovs-system state UP mode DEFAULT group default qlen 1000
#     link/ether 0c:42:a1:fa:18:a2 brd ff:ff:ff:ff:ff:ff
#     vf 0     link/ether e4:11:22:33:55:60 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state disable, trust off, query_rss off
#     vf 1     link/ether e4:11:22:33:55:61 brd ff:ff:ff:ff:ff:ff, spoof checking off, link-state disable, trust off, query_rss off
# 7: enp67s0f1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
#     link/ether 0c:42:a1:fa:18:a3 brd ff:ff:ff:ff:ff:ff
# 8: ib0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 4092 qdisc mq state DOWN mode DEFAULT group default qlen 256
#     link/infiniband 00:00:10:28:fe:80:00:00:00:00:00:00:98:03:9b:03:00:cc:71:2c brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff
# 9: baremetal: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000
#     link/ether 90:b1:1c:40:59:27 brd ff:ff:ff:ff:ff:ff
# 10: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default qlen 1000
#     link/ether 52:54:00:8f:4a:bc brd ff:ff:ff:ff:ff:ff
# 11: virbr0-nic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc fq_codel master virbr0 state DOWN mode DEFAULT group default qlen 1000
#     link/ether 52:54:00:8f:4a:bc brd ff:ff:ff:ff:ff:ff
# 16: enp67s0f0_0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq master ovs-system state UP mode DEFAULT group default qlen 1000
#     link/ether fa:cf:0f:6a:ec:45 brd ff:ff:ff:ff:ff:ff
# 17: enp67s0f0_1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq master ovs-system state UP mode DEFAULT group default qlen 1000
#     link/ether 76:65:93:70:96:ac brd ff:ff:ff:ff:ff:ff
# 18: enp67s0f0v0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
#     link/ether e4:11:22:33:55:60 brd ff:ff:ff:ff:ff:ff
# 19: enp67s0f0v1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
#     link/ether e4:11:22:33:55:61 brd ff:ff:ff:ff:ff:ff
# 20: ovs-system: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
#     link/ether f6:e9:fd:16:8a:ea brd ff:ff:ff:ff:ff:ff
# 21: ovs-sriov: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
#     link/ether 0c:42:a1:fa:18:a2 brd ff:ff:ff:ff:ff:ff
# 22: vnet0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel master baremetal state UNKNOWN mode DEFAULT group default qlen 1000
#     link/ether fe:54:00:8d:b6:8e brd ff:ff:ff:ff:ff:ff

ip a
# 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
#     link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
#     inet 127.0.0.1/8 scope host lo
#        valid_lft forever preferred_lft forever
#     inet6 ::1/128 scope host
#        valid_lft forever preferred_lft forever
# 2: eno1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq master baremetal state UP group default qlen 1000
#     link/ether 90:b1:1c:40:59:27 brd ff:ff:ff:ff:ff:ff
# 3: eno2: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc mq state DOWN group default qlen 1000
#     link/ether 90:b1:1c:40:59:28 brd ff:ff:ff:ff:ff:ff
# 4: eno3: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc mq state DOWN group default qlen 1000
#     link/ether 90:b1:1c:40:59:29 brd ff:ff:ff:ff:ff:ff
# 5: eno4: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc mq state DOWN group default qlen 1000
#     link/ether 90:b1:1c:40:59:2a brd ff:ff:ff:ff:ff:ff
# 6: enp67s0f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq master ovs-system state UP group default qlen 1000
#     link/ether 0c:42:a1:fa:18:a2 brd ff:ff:ff:ff:ff:ff
# 7: enp67s0f1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000
#     link/ether 0c:42:a1:fa:18:a3 brd ff:ff:ff:ff:ff:ff
# 8: ib0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 4092 qdisc mq state DOWN group default qlen 256
#     link/infiniband 00:00:10:28:fe:80:00:00:00:00:00:00:98:03:9b:03:00:cc:71:2c brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff
# 9: baremetal: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
#     link/ether 90:b1:1c:40:59:27 brd ff:ff:ff:ff:ff:ff
#     inet 172.21.6.105/24 brd 172.21.6.255 scope global noprefixroute baremetal
#        valid_lft forever preferred_lft forever
#     inet6 fe80::12a7:202d:c70b:be14/64 scope link noprefixroute
#        valid_lft forever preferred_lft forever
# 10: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default qlen 1000
#     link/ether 52:54:00:8f:4a:bc brd ff:ff:ff:ff:ff:ff
#     inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0
#        valid_lft forever preferred_lft forever
# 11: virbr0-nic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc fq_codel master virbr0 state DOWN group default qlen 1000
#     link/ether 52:54:00:8f:4a:bc brd ff:ff:ff:ff:ff:ff
# 16: enp67s0f0_0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq master ovs-system state UP group default qlen 1000
#     link/ether fa:cf:0f:6a:ec:45 brd ff:ff:ff:ff:ff:ff
# 17: enp67s0f0_1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq master ovs-system state UP group default qlen 1000
#     link/ether 76:65:93:70:96:ac brd ff:ff:ff:ff:ff:ff
# 18: enp67s0f0v0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000
#     link/ether e4:11:22:33:55:60 brd ff:ff:ff:ff:ff:ff
#     inet 192.168.55.31/24 scope global enp67s0f0v0
#        valid_lft forever preferred_lft forever
# 19: enp67s0f0v1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000
#     link/ether e4:11:22:33:55:61 brd ff:ff:ff:ff:ff:ff
#     inet 192.168.55.32/24 scope global enp67s0f0v1
#        valid_lft forever preferred_lft forever
# 20: ovs-system: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000
#     link/ether f6:e9:fd:16:8a:ea brd ff:ff:ff:ff:ff:ff
# 21: ovs-sriov: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000
#     link/ether 0c:42:a1:fa:18:a2 brd ff:ff:ff:ff:ff:ff
# 22: vnet0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel master baremetal state UNKNOWN group default qlen 1000
#     link/ether fe:54:00:8d:b6:8e brd ff:ff:ff:ff:ff:ff
#     inet6 fe80::fc54:ff:fe8d:b68e/64 scope link
#        valid_lft forever preferred_lft forever

ovs-vsctl show
# 8f3eddeb-c42c-4af4-9dc8-a46169d91a7c
#     Bridge ovs-sriov
#         Port enp67s0f0_1
#             Interface enp67s0f0_1
#         Port ovs-sriov
#             Interface ovs-sriov
#                 type: internal
#         Port enp67s0f0
#             Interface enp67s0f0
#         Port enp67s0f0_0
#             Interface enp67s0f0_0
#     ovs_version: &quot;2.14.1&quot;

# on kvm
ip link
# 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
#     link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
# 2: enp0s6: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
#     link/ether e4:11:c6:d3:45:f2 brd ff:ff:ff:ff:ff:ff
# 3: enp1s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000
#     link/ether 52:54:00:8d:b6:8e brd ff:ff:ff:ff:ff:ff

ip a
# 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
#     link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
#     inet 127.0.0.1/8 scope host lo
#        valid_lft forever preferred_lft forever
#     inet6 ::1/128 scope host
#        valid_lft forever preferred_lft forever
# 2: enp0s6: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000
#     link/ether e4:11:c6:d3:45:f2 brd ff:ff:ff:ff:ff:ff
#     inet 192.168.99.11/24 brd 192.168.99.255 scope global noprefixroute enp0s6
#        valid_lft forever preferred_lft forever
#     inet6 fe80::f3c:b686:1739:a748/64 scope link noprefixroute
#        valid_lft forever preferred_lft forever
# 3: enp1s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000
#     link/ether 52:54:00:8d:b6:8e brd ff:ff:ff:ff:ff:ff
#     inet 172.21.6.11/24 brd 172.21.6.255 scope global noprefixroute enp1s0
#        valid_lft forever preferred_lft forever
#     inet6 fe80::5054:ff:fe8d:b68e/64 scope link noprefixroute
#        valid_lft forever preferred_lft forever

</code></pre>
<h1 id="性能测试"><a class="header" href="#性能测试">性能测试</a></h1>
<pre><code class="language-bash"># on 102
dnf install -y iperf3
systemctl disable --now firewalld

iperf3 -s -p 6666

# on 11
dnf install -y iperf3

iperf3 -t 20 -p 6666 -c 192.168.99.21
Connecting to host 192.168.99.21, port 6666
[  5] local 192.168.99.11 port 50960 connected to 192.168.99.21 port 6666
[ ID] Interval           Transfer     Bitrate         Retr  Cwnd
[  5]   0.00-1.00   sec  1.40 GBytes  12.0 Gbits/sec    0    594 KBytes
[  5]   1.00-2.00   sec  1.39 GBytes  12.0 Gbits/sec    0    594 KBytes
[  5]   2.00-3.00   sec  1.39 GBytes  12.0 Gbits/sec    0    594 KBytes
[  5]   3.00-4.00   sec  1.40 GBytes  12.0 Gbits/sec    0    624 KBytes
[  5]   4.00-5.00   sec  1.40 GBytes  12.0 Gbits/sec    0    659 KBytes
[  5]   5.00-6.00   sec  1.40 GBytes  12.0 Gbits/sec    0    659 KBytes
[  5]   6.00-7.00   sec  1.40 GBytes  12.0 Gbits/sec    0    659 KBytes
[  5]   7.00-8.00   sec  1.40 GBytes  12.0 Gbits/sec    0   1.03 MBytes
[  5]   8.00-9.00   sec  1.40 GBytes  12.0 Gbits/sec    0   1.03 MBytes
[  5]   9.00-10.00  sec  1.40 GBytes  12.0 Gbits/sec    0   1.03 MBytes
[  5]  10.00-11.00  sec  1.39 GBytes  12.0 Gbits/sec    0   1.03 MBytes
[  5]  11.00-12.00  sec  1.39 GBytes  12.0 Gbits/sec    0   1.03 MBytes
[  5]  12.00-13.00  sec  1.39 GBytes  11.9 Gbits/sec    0   1.03 MBytes
[  5]  13.00-14.00  sec  1.39 GBytes  11.9 Gbits/sec    0   1.03 MBytes
[  5]  14.00-15.00  sec  1.39 GBytes  11.9 Gbits/sec    0   1.03 MBytes
[  5]  15.00-16.00  sec  1.39 GBytes  11.9 Gbits/sec    0   1.03 MBytes
[  5]  16.00-17.00  sec  1.39 GBytes  12.0 Gbits/sec    0   1.03 MBytes
[  5]  17.00-18.00  sec  1.39 GBytes  11.9 Gbits/sec    0   1.03 MBytes
[  5]  18.00-19.00  sec  1.39 GBytes  11.9 Gbits/sec    0   1.03 MBytes
[  5]  19.00-20.00  sec  1.39 GBytes  11.9 Gbits/sec    0   1.03 MBytes
- - - - - - - - - - - - - - - - - - - - - - - - -
[ ID] Interval           Transfer     Bitrate         Retr
[  5]   0.00-20.00  sec  27.9 GBytes  12.0 Gbits/sec    0             sender
[  5]   0.00-20.04  sec  27.9 GBytes  11.9 Gbits/sec                  receiver

iperf Done.

# on 105
systemctl disable --now irqbalance.service
mlnx_affinity start

# on 102
systemctl disable --now irqbalance.service
mlnx_affinity start

# on 102
dnf install -y qperf
qperf

# on 105
qperf 192.168.88.21 tcp_bw
tcp_bw:
    bw  =  2.8 GB/sec

# on 101
qperf 192.168.99.21 tcp_bw
tcp_bw:
    bw  =  1.48 GB/sec

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rhelcentos-8-build-kernel"><a class="header" href="#rhelcentos-8-build-kernel">RHEL/centos 8 build kernel</a></h1>
<p>本文描述如何在rhel8上编译自定义的内核。</p>
<p>业务背景是，客户需要使用mellanox网卡高级功能，需要kernel打开相应的选项，才可以使用，所以我们就编译一个新的内核出来。</p>
<h2 id="讲解视频"><a class="header" href="#讲解视频">讲解视频</a></h2>
<p><a href="https://www.bilibili.com/video/BV1ya4y1j7R3/"><kbd><img src="rhel/imgs/2020-09-28-10-33-03.png" width="600"></kbd></a></p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1ya4y1j7R3/">bilibili</a></li>
<li><a href="https://www.ixigua.com/6877362547456999943">xigua</a></li>
<li><a href="https://youtu.be/jYCUVSv4Faw">youtube</a></li>
</ul>
<h2 id="实验步骤"><a class="header" href="#实验步骤">实验步骤</a></h2>
<pre><code class="language-bash"># https://access.redhat.com/articles/3938081
# grubby --info=ALL | grep title

# https://blog.packagecloud.io/eng/2015/04/20/working-with-source-rpms/

# 由于需要rhel8.3，而当前8.3还是beta状态，我们需要注册特殊的订阅。
subscription-manager --proxy=192.168.253.1:5084 register --username **** --password ********

# subscription-manager config --rhsm.baseurl=https://china.cdn.redhat.com
# subscription-manager config --rhsm.baseurl=https://cdn.redhat.com
subscription-manager --proxy=192.168.253.1:5084 refresh

subscription-manager --proxy=192.168.253.1:5084 repos --help

subscription-manager --proxy=192.168.253.1:5084 repos --list &gt; list

cat list | grep 'Repo ID' | grep -v source | grep -v debug

subscription-manager --proxy=192.168.253.1:5084 repos --disable=&quot;*&quot;

subscription-manager --proxy=192.168.253.1:5084 repos \
    --enable=&quot;rhel-8-for-x86_64-baseos-beta-rpms&quot; \
    --enable=&quot;rhel-8-for-x86_64-appstream-beta-rpms&quot; \
    --enable=&quot;rhel-8-for-x86_64-supplementary-beta-rpms&quot; \
    --enable=&quot;rhel-8-for-x86_64-rt-beta-rpms&quot; \
    --enable=&quot;rhel-8-for-x86_64-highavailability-beta-rpms&quot; \
    --enable=&quot;rhel-8-for-x86_64-nfv-beta-rpms&quot; \
    --enable=&quot;fast-datapath-beta-for-rhel-8-x86_64-rpms&quot; \
    --enable=&quot;codeready-builder-beta-for-rhel-8-x86_64-rpms&quot; \
    # --enable=&quot;dirsrv-beta-for-rhel-8-x86_64-rpms&quot; \
    # ansible-2.9-for-rhel-8-x86_64-rpms

cat &lt;&lt; EOF &gt;&gt; /etc/dnf/dnf.conf
proxy=http://192.168.253.1:5084
EOF

# 编译内核，需要rhel7, rhel8里面的epel的包
yum -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
dnf -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm

yum -y install yum-utils rpm-build

yum list kernel.x86_64

# 下载内核源码包
yumdownloader --source kernel.x86_64

# 安装源码包
rpm -ivh /root/kernel-4.18.0-221.el8.src.rpm

cd /root/rpmbuild/SPECS
# https://stackoverflow.com/questions/13227162/automatically-install-build-dependencies-prior-to-building-an-rpm-package
# 安装辅助包
yum-builddep kernel.spec

# 生成配置
rpmbuild -bp --target=x86_64 kernel.spec

# libbabeltrace-devel

# https://www.cnblogs.com/luohaixian/p/9313863.html
KERNELVERION=`uname -r | sed &quot;s/.$(uname -m)//&quot;`
KERNELRV=$(uname -r)
/bin/cp -f /root/rpmbuild/BUILD/kernel-${KERNELVERION}/linux-${KERNELRV}/configs/* /root/rpmbuild/SOURCES/

cd /root/rpmbuild/BUILD/kernel-${KERNELVERION}/linux-${KERNELRV}/

/bin/cp -f configs/kernel-4.18.0-`uname -m`.config .config
# cp /boot/config-`uname -r`   .config

make oldconfig
# 自定义配置，请观看视频
make menuconfig

# vi .config

# CONFIG_MLX5_TC_CT=y
# CONFIG_NET_ACT_CT=m
# CONFIG_SKB_EXTENSIONS=y
# CONFIG_NET_TC_SKB_EXT=y
# CONFIG_NF_FLOW_TABLE=m
# CONFIG_NF_FLOW_TABLE_IPV4=m  x
# CONFIG_NF_FLOW_TABLE_IPV6=m  x
# CONFIG_NF_FLOW_TABLE_INET=m
# CONFIG_NET_ACT_CONNMARK=m x
# CONFIG_NET_ACT_IPT=m  x
# CONFIG_NET_EMATCH_IPT=m   x
# CONFIG_NET_ACT_IFE=m  x

# 指明编译x86
# x86_64
sed -i '1s/^/# x86_64\n/' .config

/bin/cp -f .config configs/kernel-4.18.0-`uname -m`.config
/bin/cp -f .config configs/kernel-x86_64.config

/bin/cp -f configs/* /root/rpmbuild/SOURCES/

cd /root/rpmbuild/SPECS

# cp kernel.spec kernel.spec.orig
# https://fedoraproject.org/wiki/Building_a_custom_kernel

# 自定义内核名称
sed -i &quot;s/# define buildid \\.local/%define buildid \\.wzh/&quot; kernel.spec

# rpmbuild -bb --target=`uname -m` --without kabichk  kernel.spec 2&gt; build-err.log | tee build-out.log

# rpmbuild -bb --target=`uname -m` --without debug --without debuginfo --without kabichk kernel.spec 2&gt; build-err.log | tee build-out.log

rpmbuild -bb --target=`uname -m` --with baseonly --without debug --without debuginfo --without kabichk kernel.spec 2&gt; build-err.log | tee build-out.log

cd /root/rpmbuild/RPMS/x86_64/

# 安装编译的内核
INSTALLKV=4.18.0-221.el8.wzh

yum install ./kernel-$INSTALLKV.x86_64.rpm ./kernel-core-$INSTALLKV.x86_64.rpm ./kernel-modules-$INSTALLKV.x86_64.rpm

# 重启以后，检查内核模块激活
grep -R --include=Makefile CONFIG_NET_ACT_IFE
# rpmbuild/BUILD/kernel-4.18.0-221.el8/linux-4.18.0-221.el8.wzh.x86_64/net/sched/Makefile:obj-$(CONFIG_NET_ACT_IFE)	+= act_ife.o
modprobe act_ife
lsmod | grep act_ife

</code></pre>
<p>本次实验编译完成的rhel kernel的包，在这里下载：</p>
<p>链接: https://pan.baidu.com/s/1AG07HxpXy9hoCLMq9qXi0Q  密码: 7hkt
--来自百度网盘超级会员V3的分享</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="检查是否在虚拟机上以及主机基本情况"><a class="header" href="#检查是否在虚拟机上以及主机基本情况">检查是否在虚拟机上以及主机基本情况</a></h1>
<h2 id="以下是虚拟机的输出"><a class="header" href="#以下是虚拟机的输出">以下是虚拟机的输出</a></h2>
<p>https://www.cnblogs.com/klb561/p/10527197.html</p>
<pre><code class="language-bash">dmidecode -s system-product-name
# OpenStack Nova

lshw -class system
# sz-mec-dev02
#     description: Computer
#     product: OpenStack Nova
#     vendor: OpenStack Foundation
#     version: 13.2.1-20190604220711
#     serial: 261977f6-fc7a-49f3-954e-cf9feb70fc2c
#     width: 64 bits
#     capabilities: smbios-2.8 dmi-2.8 smp vsyscall32
#     configuration: boot=normal family=Virtual Machine uuid=8C0EE55A-5F37-554D-8300-313E29EF58B0
#   *-pnp00:00
#        product: PnP device PNP0b00
#        physical id: 1
#        capabilities: pnp
#        configuration: driver=rtc_cmos

dmesg |grep -i virtual
# [    0.145659] Booting paravirtualized kernel on KVM
# [    1.177345] input: VirtualPS/2 VMware VMMouse as /devices/platform/i8042/serio1/input/input4
# [    1.178356] input: VirtualPS/2 VMware VMMouse as /devices/platform/i8042/serio1/input/input3
# [    1.223866] systemd[1]: Detected virtualization kvm.
</code></pre>
<h2 id="check-core"><a class="header" href="#check-core">check core</a></h2>
<p>https://www.cyberciti.biz/faq/check-how-many-cpus-are-there-in-linux-system/</p>
<pre><code class="language-bash">echo &quot;Number of CPU/cores online at $HOSTNAME: $(getconf _NPROCESSORS_ONLN)&quot;
</code></pre>
<h2 id="check-memory"><a class="header" href="#check-memory">check memory</a></h2>
<p>https://www.networkworld.com/article/3336174/how-much-memory-is-installed-and-being-used-on-your-linux-systems.html</p>
<pre><code class="language-bash">dmidecode -t 17 | grep &quot;Size.*MB&quot; | awk '{s+=$2} END {print s / 1024 &quot;GB&quot;}'

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-4-kvmovs-install"><a class="header" href="#openshift-4-kvmovs-install">openshift 4 kvm+ovs install</a></h1>
<p>openshift4在日常的安装场景中，有这样一个情况，就是需要在多台配置小一些的主机上，实现跨主机的集群安装，这就需要多个kvm跨主机通讯，本来使用bridge方式，搭配可直连的ip地址，是可以满足的，但是由于ip地址管理的限制，我们没有可以直连的ip地址，那么我们就需要ovs+vxlan的方式，来解决这个问题。</p>
<p>本文针对2台主机，讲述如何配置ovs，以及如何启动kvm。</p>
<p>参考资料：</p>
<ul>
<li>https://stackoverflow.com/questions/30622680/kvm-ovs-bridged-network-how-to-configure</li>
<li>https://stackoverflow.com/questions/31566658/setup-private-networking-between-two-hosts-and-two-vms-with-libvirt-openvswitc</li>
<li>https://blog.csdn.net/wuliangtianzu/article/details/81870551</li>
<li>https://pinrojas.com/2017/05/03/how-to-use-virt-install-to-connect-at-openvswitch-bridges/</li>
<li>https://www.jianshu.com/p/658332deac99</li>
<li>https://developer.gnome.org/NetworkManager/stable/nm-openvswitch.html</li>
</ul>
<p>mtu 调整：</p>
<ul>
<li>https://www.cnblogs.com/JacZhu/p/11006738.html</li>
<li>https://stackoom.com/question/3gFcR/%E6%97%A0%E6%B3%95%E5%9C%A8OVS%E9%9A%A7%E9%81%93%E4%B8%AD%E6%8D%95%E8%8E%B7%E5%A4%A7%E4%BA%8EMTU-%E7%9A%84%E6%B5%81%E9%87%8F</li>
<li>https://serverfault.com/questions/680635/mtu-on-open-vswitch-bridge-port</li>
<li>https://stackoverflow.com/questions/54398827/unable-to-capture-traffic-greater-than-mtu-1500-in-ovs-tunnel</li>
</ul>
<p>vxlan</p>
<ul>
<li>https://blog.csdn.net/a363344923/article/details/98033856</li>
<li>https://prolinuxhub.com/configure-start-up-scripts-for-ovs-on-centos-and-red-hat/</li>
</ul>
<p>nat</p>
<ul>
<li>https://www.sdnlab.com/19842.html</li>
<li>https://www.sdnlab.com/19802.html</li>
<li>https://www.sdnlab.com/19765.html</li>
</ul>
<p>基于本文的ocp4安装实践，见笔记： https://github.com/wangzheng422/docker_env/blob/master/redhat/prepare/cmri/lab.md</p>
<h2 id="on-redhat-01"><a class="header" href="#on-redhat-01">on redhat-01</a></h2>
<pre><code class="language-bash">
yum -y install openvswitch2.11 NetworkManager-ovs
# install pkg for vnc and kvm

systemctl enable --now openvswitch
systemctl status openvswitch

systemctl enable --now libvirtd

cat &lt;&lt; 'EOF' &gt; /etc/sysconfig/network-scripts/ifcfg-br-int 
DEVICE=br-int
ONBOOT=yes
DEVICETYPE=ovs
TYPE=OVSBridge
BOOTPROTO=static
HOTPLUG=no
IPADDR=192.168.7.1
PREFIX=24
MTU=1450
EOF

cat &lt;&lt; 'EOF' &gt; /etc/sysconfig/network-scripts/ifcfg-vxlan1
DEVICE=vxlan1
ONBOOT=yes
DEVICETYPE=ovs
TYPE=OVSTunnel
OVS_BRIDGE=br-int
OVS_TUNNEL_TYPE=vxlan
OVS_TUNNEL_OPTIONS=&quot;options:remote_ip=172.29.159.100&quot;
BOOTPROTO=static
HOTPLUG=no
EOF

systemctl restart network

ovs-vsctl show

# ovs-vsctl set int br-int mtu_request=1450
# ovs-vsctl set int br-int mtu_request=[]

mkdir -p /data/kvm
cd /data/kvm

# bridge mode
cat &lt;&lt; 'EOF' &gt; ovsnet.xml
&lt;network&gt;
  &lt;name&gt;br-int&lt;/name&gt;
  &lt;forward mode='bridge'/&gt;
  &lt;bridge name='br-int'/&gt;
  &lt;virtualport type='openvswitch'/&gt;
&lt;/network&gt;
EOF

virsh net-define ovsnet.xml
virsh net-start br-int
virsh net-autostart br-int

# restore
virsh net-destroy br-int
virsh net-undefine br-int
/bin/rm -f /etc/sysconfig/network-scripts/ifcfg-br-int 
/bin/rm -f /etc/sysconfig/network-scripts/ifcfg-vxlan1
systemctl restart network


</code></pre>
<h2 id="on-redhat-02"><a class="header" href="#on-redhat-02">on redhat-02</a></h2>
<pre><code class="language-bash"> 
yum -y install openvswitch2.11 NetworkManager-ovs
# install pkg for vnc and kvm

systemctl enable --now openvswitch
systemctl status openvswitch

systemctl enable --now libvirtd

ovs-vsctl show

cat &lt;&lt; 'EOF' &gt; /etc/sysconfig/network-scripts/ifcfg-br-int 
DEVICE=br-int
ONBOOT=yes
DEVICETYPE=ovs
TYPE=OVSBridge
BOOTPROTO=static
HOTPLUG=no
IPADDR=192.168.7.2
PREFIX=24
MTU=1450
EOF

cat &lt;&lt; 'EOF' &gt; /etc/sysconfig/network-scripts/ifcfg-vxlan1
DEVICE=vxlan1
ONBOOT=yes
DEVICETYPE=ovs
TYPE=OVSTunnel
OVS_BRIDGE=br-int
OVS_TUNNEL_TYPE=vxlan
OVS_TUNNEL_OPTIONS=&quot;options:remote_ip=172.29.159.99&quot;
BOOTPROTO=static
HOTPLUG=no
EOF

systemctl restart network

ovs-vsctl show

# ovs-vsctl set int br-int mtu_request=1450

mkdir -p /data/kvm
cd /data/kvm

# bridge mode
cat &lt;&lt; 'EOF' &gt; ovsnet.xml
&lt;network&gt;
  &lt;name&gt;br-int&lt;/name&gt;
  &lt;forward mode='bridge'/&gt;
  &lt;bridge name='br-int'/&gt;
  &lt;virtualport type='openvswitch'/&gt;
&lt;/network&gt;
EOF

virsh net-define ovsnet.xml
virsh net-start br-int
virsh net-autostart br-int

# restore
virsh net-destroy br-int
virsh net-undefine br-int



</code></pre>
<h2 id="创建虚拟机"><a class="header" href="#创建虚拟机">创建虚拟机</a></h2>
<p>虚机创建，注意调整每个虚机的mtu，关键在虚拟机里面，操作系统对网卡mtu的设置，这个其实是kernel安装的时候，启动参数的问题，请参考这里：
https://www.man7.org/linux/man-pages/man7/dracut.cmdline.7.html</p>
<pre><code class="language-bash">
mkdir -p /data/kvm
cd /data/kvm

lvremove -f datavg/helperlv
lvcreate -y -L 230G -n helperlv datavg

# 230G
virt-install --name=&quot;ocp4-aHelper&quot; --vcpus=2 --ram=4096 \
--disk path=/dev/datavg/helperlv,device=disk,bus=virtio,format=raw \
--os-variant centos7.0 --network network:br-int,model=virtio \
--boot menu=on --location /data/kvm/rhel-server-7.8-x86_64-dvd.iso \
--initrd-inject /data/kvm/helper-ks.cfg --extra-args &quot;inst.ks=file:/helper-ks.cfg&quot; 

</code></pre>
<h2 id="弯路-1"><a class="header" href="#弯路-1">弯路</a></h2>
<p>ovs上的虚拟机，要开启mtu调整</p>
<pre><code class="language-bash">
sysctl -w net.ipv4.tcp_mtu_probing=1

cat &lt;&lt; 'EOF' &gt; /etc/sysctl.d/99-sysctl-wzh.conf
net.ipv4.tcp_mtu_probing = 1
EOF

sysctl --system

ovs-vsctl add-port br-int vxlan1 -- \
  set Interface vxlan1 type=vxlan options:remote_ip=172.29.159.99

ovs-vsctl set int br-int mtu_request=1450

nmcli connection add type vxlan id 100 remote 172.29.159.99 ipv4.addresses 192.168.77.2/24 ipv4.method manual ifname vxlan1 connection.id vxlan1 vxlan.parent enp2s0f0 
nmcli conn up vxlan1

nmcli conn del vxlan1

ovs-vsctl add-port br-int vxlan1 -- \
  set Interface vxlan1 type=vxlan options:remote_ip=172.29.159.100

ovs-vsctl set int br-int mtu_request=1450
ovs-vsctl set int br-int mtu_request=[]

systemctl restart network

# restore
ovs-vsctl del-port br-int vxlan1
ovs-vsctl del-br br-int
rm -f /etc/sysconfig/network-scripts/ifcfg-br-int 
systemctl restart network

man nm-openvswitch

nmcli con add type ovs-bridge \
    con-name br-private \
    ifname br-private \
    ipv4.method 'manual' \
    ipv4.address '192.168.7.1/24' 

nmcli connection modify br-private ipv4.addresses 192.168.7.1/24
nmcli connection modify eno2 ipv4.gateway 192.168.39.254
nmcli connection modify eno2 ipv4.dns 192.168.39.129
nmcli connection modify br-private ipv4.method manual
nmcli connection modify br-private connection.autoconnect yes
nmcli connection modify br-private connection.autoconnect yes
nmcli connection reload

nmcli con del br-private

nmcli connection add type vxlan id 100 remote 172.29.159.100 ipv4.addresses 192.168.77.1/24 ipv4.method manual ifname vxlan1 connection.id vxlan1 vxlan.parent enp2s0f0 
nmcli conn up vxlan1

nmcli conn del vxlan1

nmcli conn add type ovs-bridge conn.interface bridge0
nmcli conn add type ovs-port conn.interface port0 master bridge0
nmcli conn add type ovs-interface conn.interface iface0 master port0 \
             ipv4.method manual ipv4.address 192.168.7.1/24

nmcli conn del ovs-slave-iface0
nmcli conn del ovs-slave-port0
nmcli conn del ovs-bridge-bridge0

ovs-vsctl add-br br-private

ovs-dpctl show
ovs-ofctl show br0


</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="内网隔离情况下使用ssh正向和反向代理实现连通外网http-proxy"><a class="header" href="#内网隔离情况下使用ssh正向和反向代理实现连通外网http-proxy">内网隔离情况下，使用SSH正向和反向代理，实现连通外网http proxy</a></h1>
<p>我们的实验室环境是一个内网隔离的环境，但是我们做实验，需要从外网下载一些依赖，那么我们可以使用SSH正向和反向代理相结合的办法，实现连接外网http proxy。</p>
<p><img src="notes/2021/dia/2021.ssh.tunnel.drawio.svg" alt="" /></p>
<p>从图里面，我们可看到，我们有一个跳板机，我们就利用这个跳板机来完成内网，外网的互通：</p>
<ul>
<li>第一步，是公网连接到跳板机，这个过程可能涉及vpn拨号，或者多次ssh到跳板机上，但是最终目的是，公网的主机和跳板机ssh可达，并能ssh到跳板机上。</li>
<li>第二步，ssh正向forward，实际上，这是ssh到跳板机，然后让跳板机上的ssh服务器打开一个到内网主机的tcp通道，并且在公网主机上监听一个端口，以后访问公网主机的端口，流量就直接转发到内网主机上。</li>
<li>第三步，通过第二步打开的正向tcp通道，建立一个ssh反向forward的连接，作用是公网主机直接ssh到内网主机上，然后在内网主机上打开一个监听端口，所有到这个端口上的流量，直接转发到一个指定的公网ip和端口上。</li>
<li>第四步，就是我们可以在内网应用上，配置http代理，指向内网主机和监听的端口，实际效果是，这些流量被转发到了公网ip和端口上。</li>
</ul>
<pre><code class="language-bash">
# gateway port for ssh -R
sed -i 's/#GatewayPorts no/GatewayPorts yes/g' /etc/ssh/sshd_config
systemctl restart sshd

# 第二步的命令范例
/usr/local/bin/autossh -M 0 -N -D 8801 -i /Users/wzh/.ssh/id_rsa root@ocp.pan.redhat.ren

# 第三部的命令范例
/usr/local/bin/autossh -M 0 -N -R 0.0.0.0:18801:127.0.0.1:5085 -i /Users/wzh/.ssh/id_rsa root@ocp.pan.redhat.ren

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="openshift-and-container-storage-for-administrators"><a class="header" href="#openshift-and-container-storage-for-administrators">OpenShift and Container Storage for Administrators</a></h1>
<p>本文讲述openshift4的管理员上手培训，主要亮点是openshift的存储模块ocs，openshift的集中日志，和openshift的计量计费，这几个模块需要的底层资源比较多，平时难得有环境可以尝试。</p>
<p>workshop upstream github: 
https://github.com/openshift/openshift-cns-testdrive</p>
<h2 id="workshop-modules"><a class="header" href="#workshop-modules">WORKSHOP MODULES</a></h2>
<p>以下是培训的各个模块的教材。</p>
<ul>
<li><a href="https://1drv.ms/b/s!AqLmU5b8zhHEo0sRBCH5BMaZddhi?e=KEudMv">Environment Overview</a></li>
<li><a href="https://1drv.ms/b/s!AqLmU5b8zhHEo0wqS3v9yTSoklce?e=pQYdyU">Installation and Verification</a></li>
<li><a href="https://1drv.ms/b/s!AqLmU5b8zhHEo009M1dtOK0jjxdF?e=IjteVg">Application Management Basics</a></li>
<li><a href="https://1drv.ms/b/s!AqLmU5b8zhHEo05w6aqcqsFj_kLi?e=vUcPbL">Application Storage Basics</a></li>
<li><a href="https://1drv.ms/b/s!AqLmU5b8zhHEo09Atba1-eE6uVNL?e=ibXzyT">MachineSets, Machines, and Nodes</a></li>
<li><a href="https://1drv.ms/b/s!AqLmU5b8zhHEo0wqS3v9yTSoklce?e=oUbqDA">Infrastructure Nodes and Operators</a></li>
<li><a href="https://1drv.ms/b/s!AqLmU5b8zhHEo1GVX4TQfnsXwdsG?e=YaOWod">Deploying and Managing OpenShift Container Storage</a></li>
<li><a href="https://1drv.ms/b/s!AqLmU5b8zhHEo1I_-Mb3Jt6ayMVe?e=AB8QQJ">OpenShift Log Aggregation</a></li>
<li><a href="https://1drv.ms/b/s!AqLmU5b8zhHEo1NIj7lEctuhyDKE?e=d5yt0w">External (LDAP) Authentication Providers, Users, and Groups</a></li>
<li><a href="https://1drv.ms/b/s!AqLmU5b8zhHEo1SAeYG1yZtBxoC0?e=iNCUQj">OpenShift Monitoring with Prometheus</a></li>
<li><a href="https://1drv.ms/b/s!AqLmU5b8zhHEo1Xcm6-ZJBlFpThB?e=EXgqJM">Project Template, Quota, and Limits</a></li>
<li><a href="https://1drv.ms/b/s!AqLmU5b8zhHEo1bOfjTiirI_WSI4?e=o42hIP">OpenShift Networking and NetworkPolicy</a></li>
<li><a href="https://1drv.ms/b/s!AqLmU5b8zhHEo1cXtp9n45kxGyJh?e=GGz6UB">Disabling Project Self-Provisioning</a></li>
<li><a href="https://1drv.ms/b/s!AqLmU5b8zhHEo1hqTna_NJ5iVYCL?e=jo6zxm">Cluster Resource Quotas</a></li>
<li><a href="https://1drv.ms/b/s!AqLmU5b8zhHEo1mtc9lSRdzwp0uQ?e=1QD43U">Cluster Metering</a></li>
<li><a href="https://1drv.ms/b/s!AqLmU5b8zhHEo1pb5Czs_S8_pM5b?e=tN1uwZ">Taints and Tolerations</a></li>
</ul>
<h2 id="ocs-openshift-container-storage"><a class="header" href="#ocs-openshift-container-storage">ocs (openshift container storage)</a></h2>
<p><a href="https://www.bilibili.com/video/BV1Ta4y1j7Bk/"><kbd><img src="ocp4/4.5/imgs/2020-09-17-14-34-10.png" width="600"></kbd></a></p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1Ta4y1j7Bk/">bilibili</a></li>
<li><a href="https://www.ixigua.com/6873344400114582030">xigua</a></li>
<li><a href="https://www.youtube.com/watch?v=BQ46mIkDZjo">youtube</a></li>
</ul>
<h2 id="集中日志"><a class="header" href="#集中日志">集中日志</a></h2>
<p><a href="https://www.bilibili.com/video/BV11h411X7h5/"><kbd><img src="ocp4/4.5/imgs/2020-09-23-06-33-45.png" width="600"></kbd></a></p>
<ul>
<li><a href="https://www.bilibili.com/video/BV11h411X7h5/">bilibili</a></li>
<li><a href="https://www.ixigua.com/6875209638690849294">xigua</a></li>
<li><a href="https://www.youtube.com/watch?v=xZ1pDMx_gho">youtube</a></li>
</ul>
<h2 id="计量计费"><a class="header" href="#计量计费">计量计费</a></h2>
<p><a href="https://www.bilibili.com/video/BV1AZ4y1K7GE/"><kbd><img src="ocp4/4.5/imgs/2020-09-23-12-18-19.png" width="600"></kbd></a></p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1AZ4y1K7GE/">bilibili</a></li>
<li><a href="https://www.ixigua.com/6875534619349877261">xigua</a></li>
<li><a href="https://youtu.be/jMrz_Rvd89U">youtube</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="poc-for-sc"><a class="header" href="#poc-for-sc">poc for sc</a></h1>
<ul>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#poc-for-sc">poc for sc</a>
<ul>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#rhel-host-maintain">rhel host maintain</a>
<ul>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#aliyun-host">aliyun host</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#helper-host">helper host</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#helper-host-day-2">helper host day 2</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#bootstrap-host">bootstrap host</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#master0-host">master0 host</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#master1-host">master1 host</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#master2-host">master2 host</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#infra0-host">infra0 host</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#infra1-host">infra1 host</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#worker-0-host">worker-0 host</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#worker-0-disk">worker-0 disk</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#worker-1-host">worker-1 host</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#worker-1-disk">worker-1 disk</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#worker-1-nic-bond">worker-1 nic bond</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#worker-2-host">worker-2 host</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#worker-2-disk">worker-2 disk</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#worker-2-disk-tunning">worker-2 disk tunning</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#worker-2-nic-bond">worker-2 nic bond</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#worker-3-host">worker-3 host</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#worker-3-disk">worker-3 disk</a></li>
</ul>
</li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#install-ocp">install ocp</a>
<ul>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#helper-node-day1">helper node day1</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#helper-node-day1-oper">helper node day1 oper</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#helper-node-day-2-sec">helper node day 2 sec</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#helper-node-quay">helper node quay</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#helper-node-zte-oper">helper node zte oper</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#helper-host-add-vm-router">helper host add vm-router</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#helper-node-zte-tcp-router">helper node zte tcp-router</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#helper-node-cluster-tunning">helper node cluster tunning</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#helper-node-local-storage">helper node local storage</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#bootstrap-node-day1">bootstrap node day1</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#master1-node-day1">master1 node day1</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#master0-node-day1">master0 node day1</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#master2-node-day1">master2 node day1</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#infra0-node-day1">infra0 node day1</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#infra1-node-day1">infra1 node day1</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#worker-0-day2-oper">worker-0 day2 oper</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#worker-1-day2-oper">worker-1 day2 oper</a></li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#worker-2-day2-oper">worker-2 day2 oper</a></li>
</ul>
</li>
<li><a href="ocp4/4.3/poc.sc/install.poc.sc.html#tips">tips</a></li>
</ul>
</li>
</ul>
<h2 id="rhel-host-maintain"><a class="header" href="#rhel-host-maintain">rhel host maintain</a></h2>
<h3 id="aliyun-host"><a class="header" href="#aliyun-host">aliyun host</a></h3>
<pre><code class="language-bash">
ssh-copy-id root@

cat &lt;&lt; EOF &gt; /root/.ssh/config
StrictHostKeyChecking no
UserKnownHostsFile=/dev/null

EOF

export VULTR_HOST=helper.hsc.redhat.ren

rsync -e ssh --info=progress2 -P --delete -arz /data/rhel-data/data ${VULTR_HOST}:/data/rhel-data

rsync -e ssh --info=progress2 -P --delete -arz /data/registry ${VULTR_HOST}:/data/

rsync -e ssh --info=progress2 -P --delete -arz /data/ocp4 ${VULTR_HOST}:/data/

rsync -e ssh --info=progress2 -P --delete -arz /data/is.samples ${VULTR_HOST}:/data/

cd /data
tar -cvf - registry/ | pigz -c &gt; registry.tgz
tar -cvf - ocp4/ | pigz -c &gt; ocp4.tgz
tar -cvf - data/ | pigz -c &gt; rhel-data.tgz
tar -cvf - is.samples/ | pigz -c &gt; /data_hdd/down/is.samples.tgz

</code></pre>
<h3 id="helper-host"><a class="header" href="#helper-host">helper host</a></h3>
<pre><code class="language-bash">######################################################
# on helper

find . -name vsftp*
yum -y install ./data/rhel-7-server-rpms/Packages/vsftpd-3.0.2-25.el7.x86_64.rpm
systemctl start vsftpd
systemctl restart vsftpd
systemctl enable vsftpd

firewall-cmd --permanent --add-service=ftp
firewall-cmd --reload

mv data /var/ftp/
chcon -R -t public_content_t /var/ftp/data

mkdir /etc/yum.repos.d.bak
mv /etc/yum.repos.d/* /etc/yum.repos.d.bak

cat &lt;&lt; EOF &gt; /etc/yum.repos.d/remote.repo
[remote]
name=RHEL FTP
baseurl=ftp://117.177.241.16/data
enabled=1
gpgcheck=0

EOF

yum clean all
yum repolist

yum -y update

hostnamectl set-hostname helper.hsc.redhat.ren
nmcli connection modify em1 ipv4.dns 114.114.114.114
nmcli connection reload
nmcli connection up em1

yum -y install fail2ban

cat &lt;&lt; EOF &gt; /etc/fail2ban/jail.d/wzh.conf
[sshd]
enabled = true

[recidive]
enabled = true

EOF

systemctl enable fail2ban
systemctl restart fail2ban

fail2ban-client status sshd
fail2ban-client status recidive
systemctl status fail2ban
tail -F /var/log/fail2ban.log

cp /etc/ssh/sshd_config /etc/ssh/sshd_config.BAK
sed -i 's/#UseDNS yes/UseDNS no/g' /etc/ssh/sshd_config

diff /etc/ssh/sshd_config /etc/ssh/sshd_config.BAK

systemctl restart sshd

passwd

useradd -m wzh

lsblk | grep 446 | awk '{print $1}' | xargs -I DEMO echo -n &quot;/dev/DEMO &quot;
# /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm
lsblk | grep 446 | awk '{print $1}' | wc -l
# 12

# https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_logical_volumes/assembly_configure-mange-raid-configuring-and-managing-logical-volumes
yum install -y lvm2

pvcreate -y /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm

vgcreate datavg /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm

vgs

lvcreate --type raid10 -l 100%FREE --stripes 6 -n datalv datavg

umount /data_hdd
lvremove /dev/datavg/datalv

mkfs.xfs /dev/datavg/datalv

lvdisplay /dev/datavg/datalv -m

mkdir -p /data

cp /etc/fstab /etc/fstab.bak

cat &lt;&lt; EOF &gt;&gt; /etc/fstab
/dev/datavg/datalv /data                   xfs     defaults        0 0

EOF

mount -a

yum install -y sysstat
lsblk | grep disk | awk '{print $1}' | xargs -I DEMO echo -n &quot;DEMO &quot;
# sda sdb sdc sdd sde sdf sdg sdh sdi sdj sdk sdl sdm
iostat -h -m -x sda sdb sdc sdd sde sdf sdg sdh sdi sdj sdk sdl sdm 5
iostat -m -x dm-24 5

yum install -y chrony
systemctl enable chronyd
systemctl restart chronyd
systemctl status chronyd
chronyc tracking

firewall-cmd --get-zones
# block dmz drop external home internal public trusted work
firewall-cmd --zone=public --list-all

firewall-cmd --permanent --zone=public --remove-port=2049/tcp

firewall-cmd --permanent --zone=public --add-rich-rule='rule family=&quot;ipv4&quot; port port=&quot;2049&quot; protocol=&quot;tcp&quot; source address=&quot;117.177.241.0/24&quot; accept'
firewall-cmd --permanent --zone=public --add-rich-rule='rule family=&quot;ipv4&quot; port port=&quot;2049&quot; protocol=&quot;tcp&quot; source address=&quot;39.137.101.0/24&quot; accept'

# firewall-cmd --permanent --zone=public --add-port=4443/tcp

firewall-cmd --reload

showmount -a
exportfs -s

cd /data_ssd/
scp *.tgz root@117.177.241.17:/data_hdd/down/

# https://access.redhat.com/solutions/3341191
# subscription-manager register --org=ORG ID --activationkey= Key Name
cat /var/log/rhsm/rhsm.log

subscription-manager config --rhsm.manage_repos=0
cp /etc/yum/pluginconf.d/subscription-manager.conf /etc/yum/pluginconf.d/subscription-manager.conf.orig
cat &lt;&lt; EOF  &gt; /etc/yum/pluginconf.d/subscription-manager.conf
[main]
enabled=0
EOF

# https://access.redhat.com/products/red-hat-insights/#getstarted
subscription-manager register --auto-attach
yum --disableplugin=subscription-manager install insights-client
insights-client --register

yum --disableplugin=subscription-manager install ncdu

</code></pre>
<h3 id="helper-host-day-2"><a class="header" href="#helper-host-day-2">helper host day 2</a></h3>
<pre><code class="language-bash">####################################
# anti scan
firewall-cmd --permanent --zone=public --remove-rich-rule='rule family=&quot;ipv4&quot; port port=&quot;2049&quot; protocol=&quot;tcp&quot; source address=&quot;117.177.241.0/24&quot; accept'
firewall-cmd --permanent --zone=public --remove-rich-rule='rule family=&quot;ipv4&quot; port port=&quot;2049&quot; protocol=&quot;tcp&quot; source address=&quot;39.137.101.0/24&quot; accept'

firewall-cmd --permanent --new-ipset=my-allow-list --type=hash:net
firewall-cmd --permanent --get-ipsets

cat &gt; /root/iplist.txt &lt;&lt;EOL
127.0.0.1/32
223.87.20.0/24
117.177.241.0/24
39.134.200.0/24
39.134.201.0/24
39.137.101.0/24
192.168.7.0/24
112.44.102.224/27
47.93.86.113/32
221.226.0.75/32
210.21.236.182/32
61.132.54.0/24
112.44.102.228/32
223.87.20.7/32
10.88.0.0/16
223.86.0.14/32
39.134.204.0/24
EOL

firewall-cmd --permanent --ipset=my-allow-list --add-entries-from-file=iplist.txt

firewall-cmd --permanent --ipset=my-allow-list --get-entries

firewall-cmd --permanent --zone=trusted --add-source=ipset:my-allow-list 
firewall-cmd --reload

firewall-cmd --list-all
firewall-cmd --get-active-zones

firewall-cmd --zone=block --change-interface=em1

firewall-cmd --set-default-zone=block
firewall-cmd --runtime-to-permanent
firewall-cmd --reload

# setup time server
/bin/cp -f /etc/chrony.conf /etc/chrony.conf.bak

cat &lt;&lt; EOF &gt; /etc/chrony.conf
server 0.rhel.pool.ntp.org iburst
server 1.rhel.pool.ntp.org iburst
server 2.rhel.pool.ntp.org iburst
server 3.rhel.pool.ntp.org iburst
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
logdir /var/log/chrony
allow 39.134.0.0/16
EOF

systemctl restart chronyd
systemctl status chronyd
chronyc tracking

useradd -m zte

groupadd docker
usermod -aG docker zte

# https://github.com/containers/libpod/issues/5049
loginctl enable-linger zte
su -l zte

# https://www.redhat.com/en/blog/preview-running-containers-without-root-rhel-76
echo 10000 &gt; /proc/sys/user/max_user_namespaces

####################################
## trust podman
firewall-cmd --permanent --zone=trusted --add-interface=cni0
firewall-cmd --permanent --zone=trusted --remove-interface=cni0

firewall-cmd --reload

# update ntp
cat &lt;&lt; EOF &gt; /etc/chrony.conf
server 223.87.20.100 iburst
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
logdir /var/log/chrony
allow 39.134.0.0/16
EOF

systemctl restart chronyd
systemctl status chronyd
chronyc tracking
</code></pre>
<h3 id="bootstrap-host"><a class="header" href="#bootstrap-host">bootstrap host</a></h3>
<pre><code class="language-bash">######################################################
# bootstrap

mkdir /etc/yum.repos.d.bak
mv /etc/yum.repos.d/* /etc/yum.repos.d.bak

cat &lt;&lt; EOF &gt; /etc/yum.repos.d/remote.repo
[remote]
name=RHEL FTP
baseurl=ftp://117.177.241.16/data
enabled=1
gpgcheck=0

EOF

yum clean all
yum repolist

yum -y update

hostnamectl set-hostname bootstrap.hsc.redhat.ren

nmcli connection modify em1 ipv4.dns 117.177.241.16
nmcli connection reload
nmcli connection up em1

yum -y install fail2ban

cat &lt;&lt; EOF &gt; /etc/fail2ban/jail.d/wzh.conf
[sshd]
enabled = true

[recidive]
enabled = true

EOF

systemctl enable fail2ban
systemctl restart fail2ban

fail2ban-client status sshd
fail2ban-client status recidive
systemctl status fail2ban
tail -F /var/log/fail2ban.log

cp /etc/ssh/sshd_config /etc/ssh/sshd_config.BAK
sed -i 's/#UseDNS yes/UseDNS no/g' /etc/ssh/sshd_config

diff /etc/ssh/sshd_config /etc/ssh/sshd_config.BAK

systemctl restart sshd

passwd

useradd -m wzh

lsblk | grep 446 | awk '{print $1}' | xargs -I DEMO echo -n &quot;/dev/DEMO &quot;
# /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm
lsblk | grep 446 | awk '{print $1}' | wc -l
# 12

# https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_logical_volumes/assembly_configure-mange-raid-configuring-and-managing-logical-volumes
yum install -y lvm2

pvcreate -y /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm

vgcreate datavg /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm

vgs

lvcreate --type raid10 -l 100%FREE --stripes 6 -n datalv datavg

mkfs.xfs /dev/datavg/datalv

lvdisplay /dev/datavg/datalv -m

mkdir -p /data

cp /etc/fstab /etc/fstab.bak

cat &lt;&lt; EOF &gt;&gt; /etc/fstab
/dev/datavg/datalv /data                   xfs     defaults        0 0

EOF

mount -a

yum install -y sysstat
lsblk | grep disk | awk '{print $1}' | xargs -I DEMO echo -n &quot;DEMO &quot;
# sda sdb sdc sdd sde sdf sdg sdh sdi sdj sdk sdl sdm
iostat -h -m -x sda sdb sdc sdd sde sdf sdg sdh sdi sdj sdk sdl sdm 5
iostat -m -x dm-24 5

yum install -y chrony
systemctl enable chronyd
systemctl restart chronyd
systemctl status chronyd
chronyc tracking

# update ntp
cat &lt;&lt; EOF &gt; /etc/chrony.conf
server 223.87.20.100 iburst
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
logdir /var/log/chrony
EOF

systemctl restart chronyd
systemctl status chronyd
chronyc tracking
</code></pre>
<h3 id="master0-host"><a class="header" href="#master0-host">master0 host</a></h3>
<pre><code class="language-bash">#####################################################
# master0

mkdir /etc/yum.repos.d.bak
mv /etc/yum.repos.d/* /etc/yum.repos.d.bak

cat &lt;&lt; EOF &gt; /etc/yum.repos.d/remote.repo
[remote]
name=RHEL FTP
baseurl=ftp://117.177.241.16/data
enabled=1
gpgcheck=0

EOF

yum clean all
yum repolist

yum -y update

hostnamectl set-hostname master0.hsc.redhat.ren

nmcli connection modify em1 ipv4.dns 117.177.241.16
nmcli connection reload
nmcli connection up em1

yum -y install fail2ban

cat &lt;&lt; EOF &gt; /etc/fail2ban/jail.d/wzh.conf
[sshd]
enabled = true

EOF

systemctl enable fail2ban
systemctl restart fail2ban

fail2ban-client status sshd
fail2ban-client status recidive
systemctl status fail2ban
tail -F /var/log/fail2ban.log

cat &lt;&lt; EOF &gt; /etc/fail2ban/jail.d/wzh.conf
[sshd]
enabled = true

[recidive]
enabled = true

EOF

cp /etc/ssh/sshd_config /etc/ssh/sshd_config.BAK
sed -i 's/#UseDNS yes/UseDNS no/g' /etc/ssh/sshd_config

diff /etc/ssh/sshd_config /etc/ssh/sshd_config.BAK

systemctl restart sshd

passwd

useradd -m wzh

yum install -y chrony
systemctl enable chronyd
systemctl restart chronyd
systemctl status chronyd
chronyc tracking

lsblk | grep 446 | awk '{print $1}' | xargs -I DEMO echo -n &quot;/dev/DEMO &quot;
# /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm
lsblk | grep 446 | awk '{print $1}' | wc -l
# 12

# https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_logical_volumes/assembly_configure-mange-raid-configuring-and-managing-logical-volumes
yum install -y lvm2

pvcreate -y /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm

vgcreate datavg /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm

vgs

lvcreate --type raid0 -l 100%FREE --stripes 12 -n datalv datavg

mkfs.xfs /dev/datavg/datalv

lvdisplay /dev/datavg/datalv -m

mkdir -p /data
mkdir -p /data_hdd

cp /etc/fstab /etc/fstab.bak

cat &lt;&lt; EOF &gt;&gt; /etc/fstab
/dev/datavg/datalv /data_hdd                  xfs     defaults        0 0

EOF

mount -a

# update ntp
cat &lt;&lt; EOF &gt; /etc/chrony.conf
server 223.87.20.100 iburst
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
logdir /var/log/chrony
EOF

systemctl restart chronyd
systemctl status chronyd
chronyc tracking
</code></pre>
<h3 id="master1-host"><a class="header" href="#master1-host">master1 host</a></h3>
<pre><code class="language-bash">######################################################
# master1

mkdir /etc/yum.repos.d.bak
mv /etc/yum.repos.d/* /etc/yum.repos.d.bak

cat &lt;&lt; EOF &gt; /etc/yum.repos.d/remote.repo
[remote]
name=RHEL FTP
baseurl=ftp://117.177.241.16/data
enabled=1
gpgcheck=0

EOF

yum clean all
yum repolist

yum -y update

hostnamectl set-hostname master1.hsc.redhat.ren

nmcli connection modify em1 ipv4.dns 117.177.241.16
nmcli connection reload
nmcli connection up em1

yum -y install fail2ban

cat &lt;&lt; EOF &gt; /etc/fail2ban/jail.d/wzh.conf
[sshd]
enabled = true

[recidive]
enabled = true

EOF

systemctl enable fail2ban
systemctl restart fail2ban

fail2ban-client status sshd
fail2ban-client status recidive
systemctl status fail2ban
tail -F /var/log/fail2ban.log

cp /etc/ssh/sshd_config /etc/ssh/sshd_config.BAK
sed -i 's/#UseDNS yes/UseDNS no/g' /etc/ssh/sshd_config

diff /etc/ssh/sshd_config /etc/ssh/sshd_config.BAK

systemctl restart sshd

passwd

useradd -m wzh

yum install -y chrony
systemctl enable chronyd
systemctl restart chronyd
systemctl status chronyd
chronyc tracking

mkdir -p /data_hdd
mkfs.xfs -f /dev/sdb

cat &lt;&lt; EOF &gt;&gt; /etc/fstab
/dev/sdb /data_hdd                   xfs     defaults        0 0
EOF

mount -a

# update ntp
cat &lt;&lt; EOF &gt; /etc/chrony.conf
server 223.87.20.100 iburst
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
logdir /var/log/chrony
EOF

systemctl restart chronyd
systemctl status chronyd
chronyc tracking
</code></pre>
<h3 id="master2-host"><a class="header" href="#master2-host">master2 host</a></h3>
<pre><code class="language-bash">######################################################
# master2

mkdir /etc/yum.repos.d.bak
mv /etc/yum.repos.d/* /etc/yum.repos.d.bak

cat &lt;&lt; EOF &gt; /etc/yum.repos.d/remote.repo
[remote]
name=RHEL FTP
baseurl=ftp://117.177.241.16/data
enabled=1
gpgcheck=0

EOF

yum clean all
yum repolist

yum -y update

hostnamectl set-hostname master2.hsc.redhat.ren

nmcli connection modify em1 ipv4.dns 117.177.241.16
nmcli connection reload
nmcli connection up em1

yum -y install fail2ban

cat &lt;&lt; EOF &gt; /etc/fail2ban/jail.d/wzh.conf
[sshd]
enabled = true
EOF

systemctl enable fail2ban
systemctl restart fail2ban

cat &lt;&lt; EOF &gt; /etc/fail2ban/jail.d/wzh.conf
[sshd]
enabled = true

[recidive]
enabled = true

EOF

fail2ban-client status
systemctl status fail2ban
tail -F /var/log/fail2ban.log

cp /etc/ssh/sshd_config /etc/ssh/sshd_config.BAK
sed -i 's/#UseDNS yes/UseDNS no/g' /etc/ssh/sshd_config

diff /etc/ssh/sshd_config /etc/ssh/sshd_config.BAK

systemctl restart sshd

passwd

useradd -m wzh

lsblk | grep 446 | awk '{print $1}' | xargs -I DEMO echo -n &quot;/dev/DEMO &quot;
# /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm
lsblk | grep 446 | awk '{print $1}' | wc -l
# 12

# https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_logical_volumes/assembly_configure-mange-raid-configuring-and-managing-logical-volumes
yum install -y lvm2

pvcreate -y /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm

vgcreate datavg /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm

vgs

lvcreate --type raid0 -l 100%FREE --stripes 12 -n datalv datavg

mkfs.xfs /dev/datavg/datalv

lvdisplay /dev/datavg/datalv -m

mkdir -p /data
mkdir -p /data_hdd

cp /etc/fstab /etc/fstab.bak

cat &lt;&lt; EOF &gt;&gt; /etc/fstab
/dev/datavg/datalv /data_hdd                   xfs     defaults        0 0

EOF

mount -a

yum install -y sysstat
lsblk | grep disk | awk '{print $1}' | xargs -I DEMO echo -n &quot;DEMO &quot;
# sda sdb sdc sdd sde sdf sdg sdh sdi sdj sdk sdl sdm
iostat -m -x sda sdb sdc sdd sde sdf sdg sdh sdi sdj sdk sdl sdm 5
iostat -m -x dm-12 5

yum install -y chrony
systemctl enable chronyd
systemctl restart chronyd
systemctl status chronyd
chronyc tracking

# update ntp
cat &lt;&lt; EOF &gt; /etc/chrony.conf
server 223.87.20.100 iburst
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
logdir /var/log/chrony
EOF

systemctl restart chronyd
systemctl status chronyd
chronyc tracking
</code></pre>
<h3 id="infra0-host"><a class="header" href="#infra0-host">infra0 host</a></h3>
<pre><code class="language-bash">######################################################
# infra0

mkdir /etc/yum.repos.d.bak
mv /etc/yum.repos.d/* /etc/yum.repos.d.bak

cat &lt;&lt; EOF &gt; /etc/yum.repos.d/remote.repo
[remote]
name=RHEL FTP
baseurl=ftp://117.177.241.16/data
enabled=1
gpgcheck=0

EOF

yum clean all
yum repolist

yum -y update

hostnamectl set-hostname infra0.hsc.redhat.ren

nmcli connection modify em1 ipv4.dns 117.177.241.16
nmcli connection reload
nmcli connection up em1

yum -y install fail2ban

cat &lt;&lt; EOF &gt; /etc/fail2ban/jail.d/wzh.conf
[sshd]
enabled = true

[recidive]
enabled = true

EOF

systemctl enable fail2ban
systemctl restart fail2ban

fail2ban-client status sshd
fail2ban-client status recidive
systemctl status fail2ban
tail -F /var/log/fail2ban.log

cp /etc/ssh/sshd_config /etc/ssh/sshd_config.BAK
sed -i 's/#UseDNS yes/UseDNS no/g' /etc/ssh/sshd_config

diff /etc/ssh/sshd_config /etc/ssh/sshd_config.BAK

systemctl restart sshd

passwd

useradd -m wzh

lsblk | grep 446 | awk '{print $1}' | xargs -I DEMO echo -n &quot;/dev/DEMO &quot;
# /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm
lsblk | grep 446 | awk '{print $1}' | wc -l
# 12

# https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_logical_volumes/assembly_configure-mange-raid-configuring-and-managing-logical-volumes
yum install -y lvm2

pvcreate -y /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm

vgcreate datavg /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm

vgs

lvcreate --type raid0 -l 100%FREE --stripes 12 -n datalv datavg

mkfs.xfs /dev/datavg/datalv

lvdisplay /dev/datavg/datalv -m

mkdir -p /data
mkdir -p /data_hdd

cp /etc/fstab /etc/fstab.bak

cat &lt;&lt; EOF &gt;&gt; /etc/fstab
/dev/datavg/datalv /data                   xfs     defaults        0 0

EOF

mount -a

# https://access.redhat.com/solutions/769403
fuser -km /data
lvremove -f datavg/datalv
vgremove datavg
pvremove /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm

pvcreate -y /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm
vgcreate datavg /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm
lvcreate --type raid0 -L 400G --stripes 12 -n monitorlv datavg

yum install -y sysstat
lsblk | grep disk | awk '{print $1}' | xargs -I DEMO echo -n &quot;DEMO &quot;
# sda sdb sdc sdd sde sdf sdg sdh sdi sdj sdk sdl sdm
iostat -m -x sda sdb sdc sdd sde sdf sdg sdh sdi sdj sdk sdl sdm 5
iostat -m -x dm-12 5

yum install -y chrony
systemctl enable chronyd
systemctl restart chronyd
systemctl status chronyd
chronyc tracking

# update ntp
cat &lt;&lt; EOF &gt; /etc/chrony.conf
server 223.87.20.100 iburst
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
logdir /var/log/chrony
EOF

systemctl restart chronyd
systemctl status chronyd
chronyc tracking
</code></pre>
<h3 id="infra1-host"><a class="header" href="#infra1-host">infra1 host</a></h3>
<pre><code class="language-bash">######################################################
# infra1

mkdir /etc/yum.repos.d.bak
mv /etc/yum.repos.d/* /etc/yum.repos.d.bak

cat &lt;&lt; EOF &gt; /etc/yum.repos.d/remote.repo
[remote]
name=RHEL FTP
baseurl=ftp://117.177.241.16/data
enabled=1
gpgcheck=0

EOF

yum clean all
yum repolist

yum -y update

hostnamectl set-hostname infra1.hsc.redhat.ren

nmcli connection modify em1 ipv4.dns 117.177.241.16
nmcli connection reload
nmcli connection up em1

yum -y install fail2ban

cat &lt;&lt; EOF &gt; /etc/fail2ban/jail.d/wzh.conf
[sshd]
enabled = true

[recidive]
enabled = true

EOF

systemctl enable fail2ban
systemctl restart fail2ban

fail2ban-client status sshd
fail2ban-client status recidive
systemctl status fail2ban
tail -F /var/log/fail2ban.log

cp /etc/ssh/sshd_config /etc/ssh/sshd_config.BAK
sed -i 's/#UseDNS yes/UseDNS no/g' /etc/ssh/sshd_config

diff /etc/ssh/sshd_config /etc/ssh/sshd_config.BAK

systemctl restart sshd

passwd

useradd -m wzh

lsblk | grep 446 | awk '{print $1}' | xargs -I DEMO echo -n &quot;/dev/DEMO &quot;
# /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm
lsblk | grep 446 | awk '{print $1}' | wc -l
# 12

# https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_logical_volumes/assembly_configure-mange-raid-configuring-and-managing-logical-volumes
yum install -y lvm2

pvcreate -y /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm

vgcreate datavg /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm

vgs

lvcreate --type raid0 -l 100%FREE --stripes 12 -n datalv datavg

mkfs.xfs /dev/datavg/datalv

lvdisplay /dev/datavg/datalv -m

mkdir -p /data
mkdir -p /data_hdd

cp /etc/fstab /etc/fstab.bak

cat &lt;&lt; EOF &gt;&gt; /etc/fstab
/dev/datavg/datalv /data                   xfs     defaults        0 0

EOF

mount -a

# https://access.redhat.com/solutions/769403
fuser -km /data
lvremove -f datavg/datalv
vgremove datavg
pvremove /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm

pvcreate -y /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm
vgcreate datavg /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm
lvcreate --type raid0 -L 400G --stripes 12 -n monitorlv datavg

yum install -y sysstat
lsblk | grep disk | awk '{print $1}' | xargs -I DEMO echo -n &quot;DEMO &quot;
# sda sdb sdc sdd sde sdf sdg sdh sdi sdj sdk sdl sdm
iostat -m -x sda sdb sdc sdd sde sdf sdg sdh sdi sdj sdk sdl sdm 5
iostat -m -x dm-12 5

yum install -y chrony
systemctl enable chronyd
systemctl restart chronyd
systemctl status chronyd
chronyc tracking

# update ntp
cat &lt;&lt; EOF &gt; /etc/chrony.conf
server 223.87.20.100 iburst
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
logdir /var/log/chrony
EOF

systemctl restart chronyd
systemctl status chronyd
chronyc tracking

</code></pre>
<h3 id="worker-0-host"><a class="header" href="#worker-0-host">worker-0 host</a></h3>
<pre><code class="language-bash">
mkdir /etc/yum.repos.d.bak
mv /etc/yum.repos.d/* /etc/yum.repos.d.bak

cat &lt;&lt; EOF &gt; /etc/yum.repos.d/remote.repo
[remote]
name=RHEL FTP
baseurl=ftp://117.177.241.16/data
enabled=1
gpgcheck=0

EOF

yum clean all
yum --disableplugin=subscription-manager  repolist

yum -y update

hostnamectl set-hostname worker-0.ocpsc.redhat.ren

nmcli connection modify enp3s0f0 ipv4.dns 117.177.241.16
nmcli connection reload
nmcli connection up enp3s0f0

yum -y install fail2ban

cat &lt;&lt; EOF &gt; /etc/fail2ban/jail.d/wzh.conf
[sshd]
enabled = true

EOF

systemctl enable fail2ban
systemctl restart fail2ban

cat &lt;&lt; EOF &gt; /etc/fail2ban/jail.d/wzh.conf
[sshd]
enabled = true

[recidive]
enabled = true

EOF

systemctl restart fail2ban

fail2ban-client status sshd
fail2ban-client status recidive
systemctl status fail2ban
tail -F /var/log/fail2ban.log

cp /etc/ssh/sshd_config /etc/ssh/sshd_config.BAK
sed -i 's/#UseDNS yes/UseDNS no/g' /etc/ssh/sshd_config

diff /etc/ssh/sshd_config /etc/ssh/sshd_config.BAK

systemctl restart sshd

passwd

useradd -m wzh

lsblk | grep 446 | awk '{print $1}' | xargs -I DEMO echo -n &quot;/dev/DEMO &quot;
# /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk
lsblk | grep 446 | awk '{print $1}' | wc -l
# 11

yum install -y lvm2

pvcreate -y /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk 

vgcreate datavg /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk

vgs

lvcreate --type raid0 -l 100%FREE --stripes 10 -n datalv datavg

mkfs.xfs /dev/datavg/datalv

lvdisplay /dev/datavg/datalv -m

mkdir -p /data

cp /etc/fstab /etc/fstab.bak

cat &lt;&lt; EOF &gt;&gt; /etc/fstab
/dev/datavg/datalv /data                  xfs     defaults        0 0

EOF

mount -a

yum install -y sysstat
lsblk | grep disk | awk '{print $1}' | xargs -I DEMO echo -n &quot;DEMO &quot;
# sda sdb sdc sdd sde sdf sdg sdh sdi sdj sdk sdl sdm
iostat -m -x sda sdb sdc sdd sde sdf sdg sdh sdi sdj sdk 5
iostat -m -x dm-10 5



####################################
# ntp
yum install -y chrony
systemctl enable chronyd
systemctl restart chronyd
systemctl status chronyd
chronyc tracking

systemctl disable --now firewalld.service

# update ntp
cat &lt;&lt; EOF &gt; /etc/chrony.conf
server 223.87.20.100 iburst
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
logdir /var/log/chrony
EOF

systemctl restart chronyd
systemctl status chronyd
chronyc tracking

#######################################
# nic bond
cat &lt;&lt; EOF &gt; /root/nic.bond.sh
#!/bin/bash

# delete all connection 
nmcli -g uuid con | while read i ; do nmcli c delete uuid ${i} ; done 

nmcli con add type bond \
    con-name bond0 \
    ifname bond0 \
    mode 802.3ad \
    ipv4.method 'manual' \
    ipv4.address '39.137.101.28/25' \
    ipv4.gateway '39.137.101.126' \
    ipv4.dns '117.177.241.16'
    
nmcli con mod id bond0 bond.options \
    mode=802.3ad,miimon=100,lacp_rate=fast,xmit_hash_policy=layer2+3
    
nmcli con add type bond-slave ifname enp3s0f0 con-name enp3s0f0 master bond0
nmcli con add type bond-slave ifname enp3s0f1 con-name enp3s0f1 master bond0

# nmcli con down enp3s0f0 &amp;&amp; nmcli con start enp3s0f0
# nmcli con down enp3s0f1 &amp;&amp; nmcli con start enp3s0f1
# nmcli con down bond0 &amp;&amp; nmcli con start bond0

systemctl restart network

EOF

cat &gt; /root/nic.restore.sh &lt;&lt; 'EOF'
#!/bin/bash

# delete all connection 
nmcli -g uuid con | while read i ; do nmcli c delete uuid ${i} ; done 

# re-create primary connection 
nmcli con add type ethernet \
    con-name enp3s0f0 \
    ifname enp3s0f0 \
    ipv4.method 'manual' \
    ipv4.address '39.137.101.28/25' \
    ipv4.gateway '39.137.101.126' \
    ipv4.dns '117.177.241.16'

# restart interface
# nmcli con down enp3s0f0 &amp;&amp; nmcli con up enp3s0f0

systemctl restart network

exit 0
EOF

chmod +x /root/nic.restore.sh

cat &gt; ~/cron-network-con-recreate &lt;&lt; EOF
*/2 * * * * /bin/bash /root/nic.restore.sh
EOF

crontab ~/cron-network-con-recreate

bash /root/nic.bond.sh

</code></pre>
<h3 id="worker-0-disk"><a class="header" href="#worker-0-disk">worker-0 disk</a></h3>
<pre><code class="language-bash">
#########################################
# ssd cache + hdd
# https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/logical_volume_manager_administration/index#lvm_cache_volume_creation
umount /data
lsblk -d -o name,rota

lvremove  /dev/datavg/datalv

pvcreate /dev/nvme0n1

# https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/logical_volume_manager_administration/vg_grow
vgextend datavg /dev/nvme0n1

## raid5 + cache
lvcreate --type raid5 -L 1G --stripes 9 -n hddlv datavg /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk

lvcreate --type raid5 -L 3.8T --stripes 9 -n mixlv datavg /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk

lvcreate -L 1G -n ssdlv datavg /dev/nvme0n1

# lvcreate --type cache-pool -L 300G -n cache1 datavg /dev/nvme0n1

lvcreate -L 1.4T -n cache1 datavg /dev/nvme0n1

lvcreate -L 14G -n cache1meta datavg /dev/nvme0n1

lvconvert --type cache-pool --poolmetadata datavg/cache1meta datavg/cache1

lvconvert --type cache --cachepool datavg/cache1 datavg/mixlv

# lvcreate --type raid5 --stripes 9 -L 1T -I 16M -R 4096K -n hddlv datavg /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk

# lvcreate --type raid5 --stripes 9 -L 1T -I 16M -R 4096K -n datalv datavg /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk

# lvcreate --type raid5 --stripes 9 -L 1T -n datalv datavg /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk

## raid0 + cache

lvcreate --type raid0 -L 4T --stripes 10 -n hddlv datavg /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk










lvcreate --type raid0 -L 1T --stripes 10 -n mixlv datavg /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk

lvcreate -L 300G -n ssdlv datavg /dev/nvme0n1

lvcreate --type cache-pool -L 300G -n cpool datavg /dev/nvme0n1

lvs -a -o name,size,attr,devices datavg

# lvconvert --type cache --cachepool cpool datavg/datalv

lvconvert --type cache --cachepool cpool datavg/mixlv

# lvconvert --type cache --cachepool cpool --cachemode writeback datavg/datalv

# lvs -a -o name,size,attr,devices datavg
# lvs -o+cache_mode datavg

# mkfs.xfs /dev/datavg/datalv
mkfs.xfs /dev/datavg/hddlv
mkfs.xfs /dev/datavg/ssdlv
mkfs.xfs /dev/datavg/mixlv

mkdir -p /data/
mkdir -p /data_ssd/
mkdir -p /data_mix/

cat /etc/fstab

cat &lt;&lt; EOF &gt;&gt; /etc/fstab
/dev/datavg/hddlv /data                  xfs     defaults        0 0
/dev/datavg/ssdlv /data_ssd                  xfs     defaults        0 0
/dev/datavg/mixlv /data_mix                  xfs     defaults        0 0
EOF

mount -a
df -h | grep \/data

# cleanup
umount /data/
umount /data_ssd/
umount /data_mix/
lvremove -f /dev/datavg/hddlv
lvremove -f /dev/datavg/ssdlv
lvremove -f /dev/datavg/mixlv

fio --rw=rw --rwmixread=99 --bsrange=4k-256k --name=vdo \
    --directory=./ --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=0 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g 

fio --rw=rw --rwmixread=99 --bsrange=4k-256k --name=vdo \
    --directory=./ --ioengine=sync --size=100g 

blktrace /dev/datavg/mixlv /dev/nvme0n1 /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk

blkparse -o /dev/null -i dm-42 -d dm-42.bin
btt -i dm-42.blktrace.bin

blkparse -o /dev/null -i nvme0n1 -d nvme0n1.bin
btt -i nvme0n1.bin | less

blkparse -o /dev/null -i sdb -d sdb.bin
btt -i sdb.bin | less


dstat -D /dev/mapper/datavg-hddlv,sdd,nvme0n1 -N enp3s0f0

dstat -D /dev/mapper/datavg-hddlv,sdd,nvme0n1 --disk-util 

bmon -p ens8f0,ens8f1,enp3s0f0,enp3s0f1

lvs -o+lv_all datavg/mixlv_corig

lvs -o+Layout datavg/mixlv_corig

lvs -o+CacheReadHits,CacheReadMisses

lvs -o+Layout

blockdev --report 
# https://access.redhat.com/solutions/3588841
/sbin/blockdev --setra 262144 /dev/mapper/datavg-hddlv
/sbin/blockdev --setra 8192 /dev/mapper/datavg-hddlv
/sbin/blockdev --setra 0 /dev/mapper/datavg-hddlv


hdparm -t /dev/mapper/datavg-hddlv

/sbin/blockdev --setra 4096 /dev/mapper/datavg-hddlv
/sbin/blockdev --setra 8192 /dev/mapper/datavg-hddlv
/sbin/blockdev --setra 16384 /dev/mapper/datavg-hddlv
/sbin/blockdev --setra 32768 /dev/mapper/datavg-hddlv
/sbin/blockdev --setra 65536 /dev/mapper/datavg-hddlv
/sbin/blockdev --setra 131072 /dev/mapper/datavg-hddlv

for f in /dev/mapper/datavg-hddlv_rimage_*; do /sbin/blockdev --setra 65536 $f ; done

for f in /dev/mapper/datavg-hddlv_rimage_*; do /sbin/blockdev --setra 131072 $f ; done

blktrace /dev/datavg/hddlv /dev/nvme0n1 /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk

# Generate distribution of file sizes from the command prompt
# https://superuser.com/questions/565443/generate-distribution-of-file-sizes-from-the-command-prompt
find /data/mnt/ -type f &gt; list
cat list | xargs ls -l &gt; list.size
cat list.size | awk '{ n=int(log($5)/log(2));                         \
          if (n&lt;10) n=10;                                               \
          size[n]++ }                                                   \
      END { for (i in size) printf(&quot;%d %d\n&quot;, 2^i, size[i]) }'          \
 | sort -n                                                              \
 | awk 'function human(x) { x[1]/=1024;                                 \
                            if (x[1]&gt;=1024) { x[2]++;                   \
                                              human(x) } }              \
        { a[1]=$1;                                                      \
          a[2]=0;                                                       \
          human(a);                                                     \
          printf(&quot;%3d%s: %6d\n&quot;, a[1],substr(&quot;kMGTEPYZ&quot;,a[2]+1,1),$2) }' 
#   1k:      2
#  16k: 18875840
#  64k: 7393088
# 128k: 5093147
# 512k: 1968632
#   1M: 914486

cat list.size | awk '{size[int(log($5)/log(2))]++}END{for (i in size) printf(&quot;%10d %3d\n&quot;, 2^i, size[i])}' | sort -n

# 5.5
var_basedir=&quot;/data_ssd/mnt&quot;
find $var_basedir -type f -size -16k  &gt; list.16k
find $var_basedir -type f -size -128k  -size +16k &gt; list.128k
find $var_basedir -type f -size +128k &gt; list.+128k
find $var_basedir -type f &gt; list


dstat --output /root/dstat.csv -D /dev/mapper/datavg-mixlv,/dev/mapper/datavg-mixlv_corig,sdh,sdab -N bond0

dstat -D /dev/mapper/datavg-hddlv,/dev/datavg/ext4lv,sdh,sdab -N bond0

i=0
while read f; do
  /bin/cp -f $f /data_mix/mnt/$i
  ((i++))
done &lt; list

find /data_mix/mnt/ -type f &gt; list

cat list | shuf &gt; list.shuf.all

cat list.16k | shuf &gt; list.shuf.16k
cat list.128k | shuf &gt; list.shuf.128k
cat list.+128k | shuf &gt; list.shuf.+128k
cat list.128k list.+128k | shuf &gt; list.shuf.+16k

# zte use 1800
var_total=10
rm -f split.list.*


split -n l/$var_total list.shuf.all split.list.all.

split -n l/$var_total list.shuf.16k split.list.16k.
split -n l/$var_total list.shuf.128k split.list.128k.
split -n l/$var_total list.shuf.+128k split.list.+128k.
split -n l/$var_total list.shuf.+16k split.list.+16k.


for f in split.list.16k.*; do 
    cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
done
# for f in split.list.+16k.*; do 
#     cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
# done
for f in split.list.128k.*; do 
    cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
done
for f in split.list.+128k.*; do 
    cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
done

for f in split.list.all.*; do 
    cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
done

ps -ef | grep /data_ssd/mnt | grep cat | awk '{print $2}' | xargs -I DEMO kill DEMO

echo &quot;wait to finish&quot;
wait
# while true; do
#   for f in split.list.all.*; do 
#       cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
#   done
#   echo &quot;wait to finish&quot;
#   wait
# done
kill -9 $(jobs -p)

jobs -p  | xargs kill

ps -ef | grep /mnt/zxdfs | grep cat | awk '{print $2}' | xargs -I DEMO kill DEMO

ps -ef | grep /data_mix/mnt | grep cat | awk '{print $2}' | xargs -I DEMO kill DEMO




</code></pre>
<h3 id="worker-1-host"><a class="header" href="#worker-1-host">worker-1 host</a></h3>
<pre><code class="language-bash">
mkdir /etc/yum.repos.d.bak
mv /etc/yum.repos.d/* /etc/yum.repos.d.bak

cat &lt;&lt; EOF &gt; /etc/yum.repos.d/remote.repo
[remote]
name=RHEL FTP
baseurl=ftp://117.177.241.16/data
enabled=1
gpgcheck=0

EOF

yum clean all
yum --disableplugin=subscription-manager  repolist

yum install -y byobu htop iostat

yum -y update

hostnamectl set-hostname worker-2.ocpsc.redhat.ren

nmcli connection modify eno1 ipv4.dns 117.177.241.16
nmcli connection reload
nmcli connection up eno1

yum -y install fail2ban

cat &lt;&lt; EOF &gt; /etc/fail2ban/jail.d/wzh.conf
[sshd]
enabled = true

EOF

systemctl enable fail2ban
systemctl restart fail2ban

cat &lt;&lt; EOF &gt; /etc/fail2ban/jail.d/wzh.conf
[sshd]
enabled = true

[recidive]
enabled = true

EOF

systemctl restart fail2ban

fail2ban-client status sshd
fail2ban-client status recidive
systemctl status fail2ban
tail -F /var/log/fail2ban.log

cp /etc/ssh/sshd_config /etc/ssh/sshd_config.BAK
sed -i 's/#UseDNS yes/UseDNS no/g' /etc/ssh/sshd_config

diff /etc/ssh/sshd_config /etc/ssh/sshd_config.BAK

systemctl restart sshd

passwd

useradd -m wzh

lsblk | grep 5.5 | awk '{print $1}' | xargs -I DEMO echo -n &quot;/dev/DEMO &quot;
# /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk
lsblk | grep 5.5 | awk '{print $1}' | wc -l
# 24

yum install -y lvm2

pvcreate -y /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

vgcreate datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

vgs

lvcreate --type raid0 -l 100%FREE --stripes 24 -n datalv datavg

mkfs.xfs /dev/datavg/datalv

lvdisplay /dev/datavg/datalv -m

mkdir -p /data

cp /etc/fstab /etc/fstab.bak

cat &lt;&lt; EOF &gt;&gt; /etc/fstab
/dev/datavg/datalv /data                  xfs     defaults        0 0

EOF

mount -a

yum install -y sysstat
lsblk | grep disk | awk '{print $1}' | xargs -I DEMO echo -n &quot;DEMO &quot;
# sda sdb sdc sdd sde sdf sdg sdh sdi sdj sdk sdl sdm
iostat -m -x sda sdb sdc sdd sde sdf sdg sdh sdi sdj sdk 5
iostat -m -x dm-10 5


########################################
# ntp
yum install -y chrony
systemctl enable chronyd
systemctl restart chronyd
systemctl status chronyd
chronyc tracking

systemctl disable --now firewalld.service

# setup time server
/bin/cp -f /etc/chrony.conf /etc/chrony.conf.bak

cat &lt;&lt; EOF &gt; /etc/chrony.conf
server 117.177.241.16 iburst
server 0.rhel.pool.ntp.org iburst
server 1.rhel.pool.ntp.org iburst
server 2.rhel.pool.ntp.org iburst
server 3.rhel.pool.ntp.org iburst
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
logdir /var/log/chrony
EOF

systemctl restart chronyd
systemctl status chronyd
chronyc tracking
chronyc sources -v

# update ntp
cat &lt;&lt; EOF &gt; /etc/chrony.conf
server 223.87.20.100 iburst
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
logdir /var/log/chrony
EOF

systemctl restart chronyd
systemctl status chronyd
chronyc tracking

</code></pre>
<h3 id="worker-1-disk"><a class="header" href="#worker-1-disk">worker-1 disk</a></h3>
<pre><code class="language-bash">##################################
## config
mkdir -p /app_conf/zxcdn


#########################################
# ssd cache + hdd
# https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/logical_volume_manager_administration/index#lvm_cache_volume_creation
umount /data
lsblk -d -o name,rota

lvremove  /dev/datavg/datalv

# lsblk | grep 894 | awk '{print $1}'

pvcreate /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

# https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/logical_volume_manager_administration/vg_grow
vgextend datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

## raid5

lvcreate --type raid5 -L 3T --stripes 23 -n hddlv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

lvcreate --type raid0 -L 1G --stripes 10 -n ssdlv datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvcreate --type raid5 -L 3T --stripes 23 -n mixlv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

lvcreate --type raid5 -L 1T --stripes 9 -n cache1 datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvcreate --type raid5 -L 10G --stripes 9 -n cache1meta datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvconvert --type cache-pool --poolmetadata datavg/cache1meta datavg/cache1

lvconvert --type cache --cachepool datavg/cache1 datavg/mixlv

# lvcreate --type raid5 --stripes 9 -L 1T -I 16M -R 4096K -n hddlv datavg /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk



lvcreate --type raid5 -L 12T --stripes 23 -n mix0lv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

lvcreate --type raid0 -L 4T --stripes 10 -n cachemix0 datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvcreate --type raid0 -L 40G --stripes 10 -n cachemix0meta datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvconvert --type cache-pool --poolmetadata datavg/cachemix0meta datavg/cachemix0

lvconvert --type cache --cachepool datavg/cachemix0 datavg/mix0lv


lvcreate --type raid5 -L 1T --stripes 23 -n mix0weblv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

lvcreate --type raid0 -L 162G --stripes 10 -n cachemix0web datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvcreate --type raid0 -L 2G --stripes 10 -n cachemix0webmeta datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvconvert --type cache-pool --poolmetadata datavg/cachemix0webmeta datavg/cachemix0web

lvconvert --type cache --cachepool datavg/cachemix0web datavg/mix0weblv


# lvcreate --type raid0 -L 200G --stripes 10 -n ssd0lv datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvcreate --type raid0 -L 200G --stripes 4 -n ssd0lv datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac

fio --rw=rw --rwmixread=99 --bsrange=4k-256k --name=vdo \
    --filename=/dev/datavg/ssd0lv --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=1 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g

lvremove -f datavg/ssd0lv

## raid0 + stripe

lvcreate --type raid0 -L 130T --stripes 24 -n hddlv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx







lvcreate --type raid0 -L 900G --stripesize 128k --stripes 24 -n testfslv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

mkfs.ext4 /dev/datavg/testfslv
mount /dev/datavg/testfslv /data_mix






lvcreate --type raid0 -L 5T --stripes 10 -n ssdlv datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvcreate --type raid5 -L 5T --stripes 9 -n ssdlv datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

mkfs.ext4 /dev/datavg/ssdlv
mount /dev/datavg/ssdlv /data_ssd

rsync -e ssh --info=progress2 -P --delete -ar --files-from=list.20k / 39.134.201.65:/data_ssd/mnt/

rsync -e ssh --info=progress2 -P --delete -ar /data/mnt/ 39.134.201.65:/data_ssd/mnt/

rsync -e ssh --info=progress2 -P --delete -ar /data/mnt/zxdfs/webcache-011/   39.134.201.65:/data_ssd/mnt/zxdfs/webcache-011/

rsync -e ssh --info=progress2 -P --delete -ar /data/mnt/zxdfs/webcache-012/   39.134.201.65:/data_ssd/mnt/zxdfs/webcache-012/







# slow
lvcreate --type raid0 -L 400G --stripesize 128k --stripes 12 -n testfslv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl 

# Generate distribution of file sizes from the command prompt
# https://superuser.com/questions/565443/generate-distribution-of-file-sizes-from-the-command-prompt
cat list | xargs ls -l &gt; list.size
cat list.size | awk '{ n=int(log($5)/log(2));                         \
          if (n&lt;10) n=10;                                               \
          size[n]++ }                                                   \
      END { for (i in size) printf(&quot;%d %d\n&quot;, 2^i, size[i]) }'          \
 | sort -n                                                              \
 | awk 'function human(x) { x[1]/=1024;                                 \
                            if (x[1]&gt;=1024) { x[2]++;                   \
                                              human(x) } }              \
        { a[1]=$1;                                                      \
          a[2]=0;                                                       \
          human(a);                                                     \
          printf(&quot;%3d%s: %6d\n&quot;, a[1],substr(&quot;kMGTEPYZ&quot;,a[2]+1,1),$2) }' 




lvcreate --type raid0 -L 1T --stripes 24 -n mixlv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

lvcreate --type raid0 -L 300G --stripes 10 -n ssdlv datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvcreate --type raid0 -L 300G --stripes 10 -n cache1 datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvcreate --type raid0 -L 3G --stripes 10 -n cache1meta datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvconvert --type cache-pool --poolmetadata datavg/cache1meta datavg/cache1

# lvs -a -o name,size,attr,devices datavg

lvconvert --type cache --cachepool datavg/cache1 datavg/mixlv

# lvs -a -o name,size,attr,devices datavg
# lvs -o+cache_mode datavg

mkfs.xfs /dev/datavg/hddlv
mkfs.xfs /dev/datavg/ssdlv
mkfs.xfs /dev/datavg/mixlv
mkfs.xfs /dev/datavg/mix0lv
mkfs.xfs /dev/datavg/mix0weblv

mkdir -p /data/
mkdir -p /data_ssd/
mkdir -p /data_mix/
mkdir -p /data_mix0
mkdir -p /data_mix0_web/

cat /etc/fstab

cat &lt;&lt; EOF &gt;&gt; /etc/fstab
/dev/datavg/hddlv /data                  xfs     defaults        0 0
# /dev/datavg/ssdlv /data_ssd                  xfs     defaults        0 0
# /dev/datavg/mixlv /data_mix                  xfs     defaults        0 0
# /dev/datavg/mix0lv  /data_mix0                  xfs     defaults        0 0
# /dev/datavg/mix0weblv  /data_mix0_web                  xfs     defaults        0 0
EOF

mount -a
df -h | grep \/data

dd if=/dev/zero of=/data/testfile bs=4k count=9999 oflag=dsync
dd if=/dev/zero of=/data_ssd/testfile bs=4k count=9999 oflag=dsync
dd if=/dev/zero of=/data_mix/testfile bs=4k count=9999 oflag=dsync

dd if=/dev/zero of=/data/testfile bs=4M count=9999 oflag=dsync
dd if=/dev/zero of=/data_ssd/testfile bs=4M count=9999 oflag=dsync
dd if=/dev/zero of=/data_mix/testfile bs=4M count=9999 oflag=dsync

dd if=/data/testfile of=/dev/null bs=4k count=9999 oflag=dsync
dd if=/data_ssd/testfile of=/dev/null bs=4k count=9999 oflag=dsync
dd if=/data_mix/testfile of=/dev/null bs=4k count=9999 oflag=dsync

dd if=/dev/zero of=/data/testfile.large bs=4M count=9999 oflag=direct
dd if=/dev/zero of=/data_ssd/testfile.large bs=4M count=9999 oflag=direct
dd if=/dev/zero of=/data_mix/testfile.large bs=4M count=9999 oflag=direct

dd if=/dev/zero of=/data/testfile.large bs=4M count=9999
dd if=/dev/zero of=/data_ssd/testfile.large bs=4M count=9999 
dd if=/dev/zero of=/data_mix/testfile.large bs=4M count=9999 

dd if=/data/testfile.large of=/dev/null bs=4k count=9999 oflag=dsync
dd if=/data_ssd/testfile.large of=/dev/null bs=4k count=9999 oflag=dsync
dd if=/data_mix/testfile.large of=/dev/null bs=4k count=9999 oflag=dsync

dd if=/data/testfile.large of=/dev/null bs=4M count=9999 oflag=dsync
dd if=/data_ssd/testfile.large of=/dev/null bs=4M count=9999 oflag=dsync
dd if=/data_mix/testfile.large of=/dev/null bs=4M count=9999 oflag=dsync

dd if=/data/testfile.large of=/dev/null bs=4M count=9999
dd if=/data_ssd/testfile.large of=/dev/null bs=4M count=9999
dd if=/data_mix/testfile.large of=/dev/null bs=4M count=9999

dd if=/data/testfile.large of=/dev/null bs=40M count=9999
dd if=/data_ssd/testfile.large of=/dev/null bs=40M count=9999
dd if=/data_mix/testfile.large of=/dev/null bs=40M count=9999

# cleanup
umount /data/
umount /data_ssd/
umount /data_mix/
umount /data_mix0/
lvremove -f /dev/datavg/hddlv
lvremove -f /dev/datavg/ssdlv
lvremove -f /dev/datavg/mixlv
lvremove -f /dev/datavg/mix0lv

# ssd tunning
# https://serverfault.com/questions/80134/linux-md-vs-lvm-performance
hdparm -tT /dev/md0

# https://www.ibm.com/developerworks/cn/linux/l-lo-io-scheduler-optimize-performance/index.html
cat /sys/block/*/queue/scheduler

lsblk | grep 894 | awk '{print $1}' | xargs -I DEMO cat /sys/block/DEMO/queue/scheduler

lsblk | grep 894 | awk '{print &quot;echo deadline &gt; /sys/block/&quot;$1&quot;/queue/scheduler&quot;}' 

iostat -x -m 3 /dev/mapper/datavg-mix0weblv /dev/mapper/datavg-mix0weblv_corig /dev/mapper/datavg-cachemix0web_cdata /dev/mapper/datavg-cachemix0web_cmeta


dstat -D /dev/mapper/datavg-hddlv,sdh,sdab -N bond0

dstat -D /dev/mapper/datavg-hddlv,sdh,sdab --disk-util 

bmon -p eno1,eno2,ens2f0,ens2f1,bond0

lvs -o+lv_all datavg/mixlv_corig

lvs -o+Layout datavg/mixlv_corig

lvs -o+CacheReadHits,CacheReadMisses

lvs -o+Layout

blockdev --report 
# https://access.redhat.com/solutions/3588841
/sbin/blockdev --setra 1048576 /dev/mapper/datavg-hddlv

/sbin/blockdev --setra 524288 /dev/mapper/datavg-hddlv

/sbin/blockdev --setra 262144 /dev/mapper/datavg-hddlv

/sbin/blockdev --setra 131072 /dev/mapper/datavg-hddlv

/sbin/blockdev --setra 65536 /dev/mapper/datavg-hddlv

/sbin/blockdev --setra 32768 /dev/mapper/datavg-hddlv

/sbin/blockdev --setra 16384 /dev/mapper/datavg-hddlv

/sbin/blockdev --setra 8192 /dev/mapper/datavg-hddlv

/sbin/blockdev --setra 8192 /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx


for f in /dev/mapper/datavg-hddlv_rimage_*; do /sbin/blockdev --setra 8192 $f ; done

for f in /dev/mapper/datavg-hddlv_rimage_*; do /sbin/blockdev --setra 16384 $f ; done

blktrace /dev/datavg/hddlv  /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

blkparse -o /dev/null -i dm-24 -d dm-24.bin
btt -i dm-24.bin | less

blkparse -o /dev/null -i sda -d sda.bin
btt -i sda.bin | less


# 5.5
# find /data/mnt/ -type f -size -2M -size +512k  &gt; list
var_basedir=&quot;/data_mix/mnt&quot;
find $var_basedir -type f -size -2M  &gt; list.2m
find $var_basedir -type f -size -10M  -size +2M &gt; list.10m
find $var_basedir -type f -size +10M &gt; list.100m

find /data/mnt/ -type f &gt; list
dstat --output /root/dstat.csv -D /dev/mapper/datavg-mixlv,/dev/mapper/datavg-mixlv_corig,sdh,sdab -N bond0

dstat -D /dev/mapper/datavg-hddlv,/dev/datavg/testfslv,sdh,sdab -N bond0

mkdir -p /data_mix/mnt
i=11265199
while read f; do
  /bin/cp -f $f /data_mix/mnt/$i &amp;
  ((i++))
  if (( $i % 200 == 0 )) ; then
    wait
  fi
done &lt; list.100m

while true; do
  df -h | grep /data
  sleep 60
done

find /data_mix/mnt/ -type f &gt; list

cat list | shuf &gt; list.shuf.all

cat list.2m | shuf &gt; list.shuf.2m
cat list.10m | shuf &gt; list.shuf.10m
cat list.100m | shuf &gt; list.shuf.100m
cat list.10m list.100m | shuf &gt; list.shuf.+2m

# zte use 1800
var_total=10
split -n l/$var_total list.shuf.all split.list.all.
split -n l/$var_total list.shuf.2m split.list.2m.
split -n l/$var_total list.shuf.10m split.list.10m.
split -n l/$var_total list.shuf.100m split.list.100m.
split -n l/$var_total list.shuf.+2m split.list.+2m.

rm -f split.list.*

for f in split.list.2m.*; do 
    cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
done
# for f in split.list.+2m.*; do 
#     cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
# done
for f in split.list.10m.*; do 
    cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
done
for f in split.list.100m.*; do 
    cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
done

for f in split.list.all.*; do 
    cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
done

jobs -p | xargs kill


ps -ef | grep xargs | grep DEMO | grep cat | awk '{print $2}' | xargs -I DEMO kill DEMO

ps -ef | grep /data_mix/mnt | grep cat | awk '{print $2}' | xargs -I DEMO kill DEMO


rclone sync /data/mnt/ /data/backup/mnt/ -P -L --transfers 64
rclone sync /data/home/ /data/backup/home/ -P -L --transfers 64
rclone sync /data/ztecdn/ /data/backup/ztecdn/ -P -L --transfers 64

rclone sync /data/backup/mnt/ /data/mnt/ -P -L --transfers 64


# check sn
dmidecode -t 1
# # dmidecode 3.2
# Getting SMBIOS data from sysfs.
# SMBIOS 3.0.0 present.

# Handle 0x0001, DMI type 1, 27 bytes
# System Information
#         Manufacturer: Huawei
#         Product Name: 5288 V5
#         Version: Purley
#         Serial Number: 2102312CJSN0K9000028
#         UUID: a659bd21-cc64-83c1-e911-6cd6de4f8050
#         Wake-up Type: Power Switch
#         SKU Number: Purley
#         Family: Purley

# check disk
lshw -c disk
  # *-disk:0
  #      description: ATA Disk
  #      product: HUS726T6TALE600
  #      physical id: 0.2.0
  #      bus info: scsi@0:0.2.0
  #      logical name: /dev/sda
  #      version: T010
  #      serial: xLkuQ2-XVVp-sfs3-8Rgm-vRgS-uysW-ncIudq
  #      size: 5589GiB (6001GB)
  #      capacity: 5589GiB (6001GB)
  #      capabilities: 7200rpm lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:1
  #      description: ATA Disk
  #      product: HUS726T6TALE600
  #      physical id: 0.3.0
  #      bus info: scsi@0:0.3.0
  #      logical name: /dev/sdb
  #      version: T010
  #      serial: 5d2geD-fGih-Q6yK-2xVs-lWUG-tH38-qQWRC6
  #      size: 5589GiB (6001GB)
  #      capacity: 5589GiB (6001GB)
  #      capabilities: 7200rpm lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:2
  #      description: ATA Disk
  #      product: HUS726T6TALE600
  #      physical id: 0.c.0
  #      bus info: scsi@0:0.12.0
  #      logical name: /dev/sdk
  #      version: T010
  #      serial: fePKOb-MTZv-j4Xz-qNjo-cPTr-078I-vZYiPH
  #      size: 5589GiB (6001GB)
  #      capacity: 5589GiB (6001GB)
  #      capabilities: 7200rpm lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:3
  #      description: ATA Disk
  #      product: HUS726T6TALE600
  #      physical id: 0.d.0
  #      bus info: scsi@0:0.13.0
  #      logical name: /dev/sdl
  #      version: T010
  #      serial: fUTBJp-fXg0-0uJX-V4Qp-vSfZ-yxmb-G8LNam
  #      size: 5589GiB (6001GB)
  #      capacity: 5589GiB (6001GB)
  #      capabilities: 7200rpm lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:4
  #      description: ATA Disk
  #      product: HUS726T6TALE600
  #      physical id: 0.e.0
  #      bus info: scsi@0:0.14.0
  #      logical name: /dev/sdm
  #      version: T010
  #      serial: SNfxce-ytX2-7j4p-opnQ-lOxC-AFIp-VbCfec
  #      size: 5589GiB (6001GB)
  #      capacity: 5589GiB (6001GB)
  #      capabilities: 7200rpm lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:5
  #      description: ATA Disk
  #      product: HUS726T6TALE600
  #      physical id: 0.f.0
  #      bus info: scsi@0:0.15.0
  #      logical name: /dev/sdn
  #      version: T010
  #      serial: HJqH2G-XT7i-2R27-dSb0-q36n-T4Ut-Ml4GiE
  #      size: 5589GiB (6001GB)
  #      capacity: 5589GiB (6001GB)
  #      capabilities: 7200rpm lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:6
  #      description: ATA Disk
  #      product: HUS726T6TALE600
  #      physical id: 0.10.0
  #      bus info: scsi@0:0.16.0
  #      logical name: /dev/sdo
  #      version: T010
  #      serial: IBh87y-SOWJ-rI3R-Mshu-agWM-TyHs-6ko0iu
  #      size: 5589GiB (6001GB)
  #      capacity: 5589GiB (6001GB)
  #      capabilities: 7200rpm lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:7
  #      description: ATA Disk
  #      product: HUS726T6TALE600
  #      physical id: 0.11.0
  #      bus info: scsi@0:0.17.0
  #      logical name: /dev/sdp
  #      version: T010
  #      serial: erBKxc-gBsD-msEq-aXMJ-8akE-FGRb-SjBk1w
  #      size: 5589GiB (6001GB)
  #      capacity: 5589GiB (6001GB)
  #      capabilities: 7200rpm lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:8
  #      description: ATA Disk
  #      product: HUS726T6TALE600
  #      physical id: 0.12.0
  #      bus info: scsi@0:0.18.0
  #      logical name: /dev/sdq
  #      version: T010
  #      serial: HsiL2h-6736-4x4H-0OTz-HuXj-My1c-RRShQP
  #      size: 5589GiB (6001GB)
  #      capacity: 5589GiB (6001GB)
  #      capabilities: 7200rpm lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:9
  #      description: ATA Disk
  #      product: HUS726T6TALE600
  #      physical id: 0.13.0
  #      bus info: scsi@0:0.19.0
  #      logical name: /dev/sdr
  #      version: T010
  #      serial: yZQ8MH-7SCw-KIFL-fphN-S0W0-GS4V-Wc2gwx
  #      size: 5589GiB (6001GB)
  #      capacity: 5589GiB (6001GB)
  #      capabilities: 7200rpm lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:10
  #      description: ATA Disk
  #      product: HUS726T6TALE600
  #      physical id: 0.14.0
  #      bus info: scsi@0:0.20.0
  #      logical name: /dev/sds
  #      version: T010
  #      serial: pp6xvN-MBT9-aLkB-65hF-7fwE-29vt-hA51K9
  #      size: 5589GiB (6001GB)
  #      capacity: 5589GiB (6001GB)
  #      capabilities: 7200rpm lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:11
  #      description: ATA Disk
  #      product: HUS726T6TALE600
  #      physical id: 0.15.0
  #      bus info: scsi@0:0.21.0
  #      logical name: /dev/sdt
  #      version: T010
  #      serial: jXj3cL-qvoJ-JWP0-jvp9-WEbn-yD63-e6vFmP
  #      size: 5589GiB (6001GB)
  #      capacity: 5589GiB (6001GB)
  #      capabilities: 7200rpm lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:12
  #      description: ATA Disk
  #      product: HUS726T6TALE600
  #      physical id: 0.4.0
  #      bus info: scsi@0:0.4.0
  #      logical name: /dev/sdc
  #      version: T010
  #      serial: Ca6Nyo-Oq5p-UdAY-oqIs-DlK5-1PPy-ugvF3P
  #      size: 5589GiB (6001GB)
  #      capacity: 5589GiB (6001GB)
  #      capabilities: 7200rpm lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:13
  #      description: ATA Disk
  #      product: HUS726T6TALE600
  #      physical id: 0.16.0
  #      bus info: scsi@0:0.22.0
  #      logical name: /dev/sdu
  #      version: T010
  #      serial: GOTXh2-34fo-rZfh-IB5d-RkwW-o5EC-rDD4R1
  #      size: 5589GiB (6001GB)
  #      capacity: 5589GiB (6001GB)
  #      capabilities: 7200rpm lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:14
  #      description: ATA Disk
  #      product: HUS726T6TALE600
  #      physical id: 0.17.0
  #      bus info: scsi@0:0.23.0
  #      logical name: /dev/sdv
  #      version: T010
  #      serial: 7Yn8xd-68Xu-A0RC-nx5Q-YEvJ-QPEG-CwjkP0
  #      size: 5589GiB (6001GB)
  #      capacity: 5589GiB (6001GB)
  #      capabilities: 7200rpm lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:15
  #      description: ATA Disk
  #      product: HUS726T6TALE600
  #      physical id: 0.18.0
  #      bus info: scsi@0:0.24.0
  #      logical name: /dev/sdw
  #      version: T010
  #      serial: hdz5tv-f2Zm-wuf8-qtKO-XIlN-4Z1E-uHapKc
  #      size: 5589GiB (6001GB)
  #      capacity: 5589GiB (6001GB)
  #      capabilities: 7200rpm lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:16
  #      description: ATA Disk
  #      product: HUS726T6TALE600
  #      physical id: 0.19.0
  #      bus info: scsi@0:0.25.0
  #      logical name: /dev/sdx
  #      version: T010
  #      serial: C3VFhO-mh9a-vKIR-Gi1o-pc05-LOqY-oErH8r
  #      size: 5589GiB (6001GB)
  #      capacity: 5589GiB (6001GB)
  #      capabilities: 7200rpm lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:17
  #      description: SCSI Disk
  #      product: HW-SAS3408
  #      vendor: AVAGO
  #      physical id: 2.0.0
  #      bus info: scsi@0:2.0.0
  #      logical name: /dev/sdy
  #      version: 5.06
  #      serial: 00457f537b174eb025007018406c778a
  #      size: 446GiB (478GB)
  #      capabilities: gpt-1.00 partitioned partitioned:gpt
  #      configuration: ansiversion=5 guid=f72b8f56-6e5d-4a0c-a2a0-bf641ac2c2ff logicalsectorsize=512 sectorsize=4096
  # *-disk:18
  #      description: ATA Disk
  #      product: HUS726T6TALE600
  #      physical id: 0.5.0
  #      bus info: scsi@0:0.5.0
  #      logical name: /dev/sdd
  #      version: T010
  #      serial: 1sulWQ-pttz-zf0P-WTEe-cydl-lY6Q-CdX4Hv
  #      size: 5589GiB (6001GB)
  #      capacity: 5589GiB (6001GB)
  #      capabilities: 7200rpm lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:19
  #      description: ATA Disk
  #      product: HUS726T6TALE600
  #      physical id: 0.6.0
  #      bus info: scsi@0:0.6.0
  #      logical name: /dev/sde
  #      version: T010
  #      serial: JF6q37-XaYh-qoXg-mPeZ-4Ofr-Qrkt-nh21RR
  #      size: 5589GiB (6001GB)
  #      capacity: 5589GiB (6001GB)
  #      capabilities: 7200rpm lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:20
  #      description: ATA Disk
  #      product: HUS726T6TALE600
  #      physical id: 0.7.0
  #      bus info: scsi@0:0.7.0
  #      logical name: /dev/sdf
  #      version: T010
  #      serial: vvF48a-k1sq-7v1m-dpSh-yb50-KLLk-otk7lA
  #      size: 5589GiB (6001GB)
  #      capacity: 5589GiB (6001GB)
  #      capabilities: 7200rpm lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:21
  #      description: ATA Disk
  #      product: HUS726T6TALE600
  #      physical id: 0.8.0
  #      bus info: scsi@0:0.8.0
  #      logical name: /dev/sdg
  #      version: T010
  #      serial: NHU0VX-vm31-DyRP-V4dc-gx7T-dXGI-Bb8qlw
  #      size: 5589GiB (6001GB)
  #      capacity: 5589GiB (6001GB)
  #      capabilities: 7200rpm lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:22
  #      description: ATA Disk
  #      product: HUS726T6TALE600
  #      physical id: 0.9.0
  #      bus info: scsi@0:0.9.0
  #      logical name: /dev/sdh
  #      version: T010
  #      serial: jCIRNL-K08S-oYZc-Q5Eb-Y2ht-0NYt-0luz1T
  #      size: 5589GiB (6001GB)
  #      capacity: 5589GiB (6001GB)
  #      capabilities: 7200rpm lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:23
  #      description: ATA Disk
  #      product: HUS726T6TALE600
  #      physical id: 0.a.0
  #      bus info: scsi@0:0.10.0
  #      logical name: /dev/sdi
  #      version: T010
  #      serial: wiQiLJ-Arua-8vcg-m6ta-KgSL-f1kD-rgzKxD
  #      size: 5589GiB (6001GB)
  #      capacity: 5589GiB (6001GB)
  #      capabilities: 7200rpm lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:24
  #      description: ATA Disk
  #      product: HUS726T6TALE600
  #      physical id: 0.b.0
  #      bus info: scsi@0:0.11.0
  #      logical name: /dev/sdj
  #      version: T010
  #      serial: T7vZ96-uTGr-tvFz-jKoZ-479j-vRvh-WeCVRJ
  #      size: 5589GiB (6001GB)
  #      capacity: 5589GiB (6001GB)
  #      capabilities: 7200rpm lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:0
  #      description: ATA Disk
  #      product: MTFDDAK960TDC-1A
  #      physical id: 0.e.0
  #      bus info: scsi@15:0.14.0
  #      logical name: /dev/sdz
  #      version: M030
  #      serial: HE21uM-4KRw-heFX-IFVf-zO8Y-Rzah-ncwlwL
  #      size: 894GiB (960GB)
  #      capacity: 894GiB (960GB)
  #      capabilities: lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:1
  #      description: ATA Disk
  #      product: MTFDDAK960TDC-1A
  #      physical id: 0.f.0
  #      bus info: scsi@15:0.15.0
  #      logical name: /dev/sdaa
  #      version: M030
  #      serial: RGeqtd-dTEc-hV8g-Xd9o-I1Ke-sDH1-UK6mZg
  #      size: 894GiB (960GB)
  #      capacity: 894GiB (960GB)
  #      capabilities: lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:2
  #      description: ATA Disk
  #      product: MTFDDAK960TDC-1A
  #      physical id: 0.10.0
  #      bus info: scsi@15:0.16.0
  #      logical name: /dev/sdab
  #      version: M030
  #      serial: 1ROsNp-0J4j-DuWM-1nNl-Fo3K-gWfg-d7VDLq
  #      size: 894GiB (960GB)
  #      capacity: 894GiB (960GB)
  #      capabilities: lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:3
  #      description: ATA Disk
  #      product: MTFDDAK960TDC-1A
  #      physical id: 0.11.0
  #      bus info: scsi@15:0.17.0
  #      logical name: /dev/sdac
  #      version: M030
  #      serial: s0XeSI-Zl3B-0xcU-8wi3-BvVo-vU3k-cLZx22
  #      size: 894GiB (960GB)
  #      capacity: 894GiB (960GB)
  #      capabilities: lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:4
  #      description: ATA Disk
  #      product: MTFDDAK960TDC-1A
  #      physical id: 0.12.0
  #      bus info: scsi@15:0.18.0
  #      logical name: /dev/sdad
  #      version: M030
  #      serial: rZZ7yM-KImV-6Ld8-xmOJ-KyiC-Wstp-4t35S3
  #      size: 894GiB (960GB)
  #      capacity: 894GiB (960GB)
  #      capabilities: lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:5
  #      description: ATA Disk
  #      product: MTFDDAK960TDC-1A
  #      physical id: 0.13.0
  #      bus info: scsi@15:0.19.0
  #      logical name: /dev/sdae
  #      version: M030
  #      serial: LI50dd-vn2G-RiYE-5iuL-nxYI-TXCT-zs1lSY
  #      size: 894GiB (960GB)
  #      capacity: 894GiB (960GB)
  #      capabilities: lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:6
  #      description: ATA Disk
  #      product: MTFDDAK960TDC-1A
  #      physical id: 0.14.0
  #      bus info: scsi@15:0.20.0
  #      logical name: /dev/sdaf
  #      version: M030
  #      serial: 2hkDxG-90a2-mkEJ-GxmQ-doAv-SPT1-8qyo10
  #      size: 894GiB (960GB)
  #      capacity: 894GiB (960GB)
  #      capabilities: lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:7
  #      description: ATA Disk
  #      product: MTFDDAK960TDC-1A
  #      physical id: 0.15.0
  #      bus info: scsi@15:0.21.0
  #      logical name: /dev/sdag
  #      version: M030
  #      serial: bMQrTa-IKF7-vDFU-5RSR-cj4a-cOUL-QAY2yI
  #      size: 894GiB (960GB)
  #      capacity: 894GiB (960GB)
  #      capabilities: lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:8
  #      description: ATA Disk
  #      product: MTFDDAK960TDC-1A
  #      physical id: 0.16.0
  #      bus info: scsi@15:0.22.0
  #      logical name: /dev/sdah
  #      version: M030
  #      serial: q0VZpE-4sub-HKbe-RkRx-G0wM-HOeU-NDRXRe
  #      size: 894GiB (960GB)
  #      capacity: 894GiB (960GB)
  #      capabilities: lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:9
  #      description: ATA Disk
  #      product: MTFDDAK960TDC-1A
  #      physical id: 0.17.0
  #      bus info: scsi@15:0.23.0
  #      logical name: /dev/sdai
  #      version: M030
  #      serial: fEj7Rr-FSS8-ruwb-IjSj-xW6l-oj6v-q1pSNV
  #      size: 894GiB (960GB)
  #      capacity: 894GiB (960GB)
  #      capabilities: lvm2
  #      configuration: ansiversion=6 logicalsectorsize=512 sectorsize=4096
  # *-disk:10
  #      description: SCSI Disk
  #      product: HW-SAS3408
  #      vendor: AVAGO
  #      physical id: 2.0.0
  #      bus info: scsi@15:2.0.0
  #      logical name: /dev/sdaj
  #      version: 5.06
  #      serial: 00a6b489499e4cb02500904af3624ac6
  #      size: 893GiB (958GB)
  #      capabilities: partitioned partitioned:dos
  #      configuration: ansiversion=5 logicalsectorsize=512 sectorsize=4096 signature=550d3974

yum -y install fio

# https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/vdo-ev-performance-testing

lvs -o+cache_policy,cache_settings,chunksize datavg/mix0weblv

# https://access.redhat.com/solutions/2961861
for i in  /proc/[0-9]* ; do echo $i &gt;&gt; /tmp/mountinfo ;  grep -q &quot;/dev/mapper/datavg-mix0weblv&quot; $i/mountinfo ; echo $? &gt;&gt; /tmp/mountinfo ; done

grep -B 1 '^0$' /tmp/mountinfo 

lvcreate --type raid5 -L 120G --stripes 23 -n mixtestlv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

fio --rw=rw --rwmixread=80 --bsrange=4k-256k --name=vdo \
    --filename=/dev/datavg/mixtestlv --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=0 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g

lvremove -f datavg/mixtestlv
# Run status group 0 (all jobs):
#    READ: bw=587MiB/s (615MB/s), 587MiB/s-587MiB/s (615MB/s-615MB/s), io=79.9GiB (85.8GB), run=139473-139473msec
#   WRITE: bw=147MiB/s (155MB/s), 147MiB/s-147MiB/s (155MB/s-155MB/s), io=20.1GiB (21.6GB), run=139473-139473msec

lvcreate --type raid6 -L 120G --stripes 22 -n mixtestlv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

fio --rw=rw --rwmixread=80 --bsrange=4k-256k --name=vdo \
    --filename=/dev/datavg/mixtestlv --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=0 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g

lvremove -f datavg/mixtestlv
# Run status group 0 (all jobs):
#    READ: bw=586MiB/s (614MB/s), 586MiB/s-586MiB/s (614MB/s-614MB/s), io=79.9GiB (85.8GB), run=139739-139739msec
#   WRITE: bw=147MiB/s (154MB/s), 147MiB/s-147MiB/s (154MB/s-154MB/s), io=20.1GiB (21.6GB), run=139739-139739msec

lvcreate --type raid0 -L 120G --stripes 24 -n mixtestlv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

fio --rw=rw --rwmixread=80 --bsrange=4k-256k --name=vdo \
    --filename=/dev/datavg/mixtestlv --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=0 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g

lvremove -f datavg/mixtestlv
# Run status group 0 (all jobs):
#    READ: bw=1139MiB/s (1194MB/s), 1139MiB/s-1139MiB/s (1194MB/s-1194MB/s), io=79.9GiB (85.8GB), run=71841-71841msec
#   WRITE: bw=286MiB/s (300MB/s), 286MiB/s-286MiB/s (300MB/s-300MB/s), io=20.1GiB (21.6GB), run=71841-71841msec

lvcreate --type raid0 -L 100G --stripes 10 -n mixtestlv datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

fio --rw=rw --rwmixread=80 --bsrange=4k-256k --name=vdo \
    --filename=/dev/datavg/mixtestlv --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=0 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g

lvremove -f datavg/mixtestlv
# Run status group 0 (all jobs):
#    READ: bw=1358MiB/s (1424MB/s), 1358MiB/s-1358MiB/s (1424MB/s-1424MB/s), io=79.9GiB (85.8GB), run=60282-60282msec
#   WRITE: bw=341MiB/s (358MB/s), 341MiB/s-341MiB/s (358MB/s-358MB/s), io=20.1GiB (21.6GB), run=60282-60282msec


lvcreate --type raid5 -L 100G --stripes 9 -n mixtestlv datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

fio --rw=rw --rwmixread=80 --bsrange=4k-256k --name=vdo \
    --filename=/dev/datavg/mixtestlv --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=0 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g

lvremove -f datavg/mixtestlv



lvcreate --type raid6 -L 100G --stripes 9 -n mixtestlv datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

fio --rw=rw --rwmixread=80 --bsrange=4k-256k --name=vdo \
    --filename=/dev/datavg/mixtestlv --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=0 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g

lvremove -f datavg/mixtestlv



lvcreate --type raid5 -L 120G --stripes 23 -n mixtestlv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

lvcreate --type raid0 -L 40G --stripes 10 -n cachetest datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvcreate --type raid0 -L 400M --stripes 10 -n cachetestmeta datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvconvert --type cache-pool --poolmetadata datavg/cachetestmeta datavg/cachetest

lvconvert --type cache --cachepool datavg/cachetest datavg/mixtestlv

fio --rw=rw --rwmixread=80 --bsrange=4k-256k --name=vdo \
    --filename=/dev/datavg/mixtestlv --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=0 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g -random_distribution=zoned:60/10:30/20:8/30:2/40

lvremove -f datavg/mixtestlv
# Run status group 0 (all jobs):
#    READ: bw=716MiB/s (750MB/s), 716MiB/s-716MiB/s (750MB/s-750MB/s), io=31.0GiB (34.3GB), run=45744-45744msec
#   WRITE: bw=180MiB/s (189MB/s), 180MiB/s-180MiB/s (189MB/s-189MB/s), io=8228MiB (8628MB), run=45744-45744msec

lvcreate --type raid5 -L 120G --stripes 23 -n mixtestlv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

lvcreate --type raid5 -L 40G --stripes 9 -n cachetest datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvcreate --type raid5 -L 400M --stripes 9 -n cachetestmeta datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvconvert --type cache-pool --poolmetadata datavg/cachetestmeta datavg/cachetest

lvconvert --type cache --cachepool datavg/cachetest datavg/mixtestlv

fio --rw=rw --rwmixread=80 --bsrange=4k-256k --name=vdo \
    --filename=/dev/datavg/mixtestlv --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=0 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g -random_distribution=zoned:60/10:30/20:8/30:2/40

lvremove -f datavg/mixtestlv
# Run status group 0 (all jobs):
#    READ: bw=487MiB/s (511MB/s), 487MiB/s-487MiB/s (511MB/s-511MB/s), io=79.9GiB (85.8GB), run=167880-167880msec
#   WRITE: bw=122MiB/s (128MB/s), 122MiB/s-122MiB/s (128MB/s-128MB/s), io=20.1GiB (21.6GB), run=167880-167880msec

lvcreate -L 100G -n singledisklv datavg /dev/sda

fio --rw=rw --rwmixread=80 --bsrange=4k-256k --name=vdo \
    --filename=/dev/datavg/singledisklv --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=0 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g -random_distribution=zoned:60/10:30/20:8/30:2/40

lvremove -f datavg/singledisklv
# Run status group 0 (all jobs):
#    READ: bw=151MiB/s (158MB/s), 151MiB/s-151MiB/s (158MB/s-158MB/s), io=44.2GiB (47.5GB), run=300031-300031msec
#   WRITE: bw=37.0MiB/s (39.8MB/s), 37.0MiB/s-37.0MiB/s (39.8MB/s-39.8MB/s), io=11.1GiB (11.9GB), run=300031-300031msec

lvcreate -L 20G -n singledisklv datavg /dev/sdai

fio --rw=rw --rwmixread=80 --bsrange=4k-256k --name=vdo \
    --filename=/dev/datavg/singledisklv --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=0 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=20g -random_distribution=zoned:60/10:30/20:8/30:2/40

lvremove -f datavg/singledisklv
# Run status group 0 (all jobs):
#    READ: bw=431MiB/s (452MB/s), 431MiB/s-431MiB/s (452MB/s-452MB/s), io=16.0GiB (17.2GB), run=38005-38005msec
#   WRITE: bw=108MiB/s (113MB/s), 108MiB/s-108MiB/s (113MB/s-113MB/s), io=4088MiB (4287MB), run=38005-38005msec

fio --rw=rw --rwmixread=99 --bsrange=4k-256k --name=vdo \
    --directory=./ --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=0 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g 

fio --rw=rw --rwmixread=99 --bsrange=4k-256k --name=vdo \
    --directory=./ --ioengine=sync --size=100g 

blktrace /dev/datavg/mixlv 
# http benchmark tools
yum install httpd-tools
# https://github.com/philipgloyne/apachebench-for-multi-url
# https://hub.docker.com/r/chrisipa/ab-multi-url
# https://www.simonholywell.com/post/2015/06/parallel-benchmark-many-urls-with-apachebench/


fio --rw=rw --rwmixread=99 --bsrange=4k-256k --name=vdo \
    --filename=/dev/datavg/ssd0lv --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=0 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g

fio --rw=rw --rwmixread=99 --bsrange=128k-256k --name=vdo \
    --filename=/dev/datavg/ssd0lv --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=0 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g



</code></pre>
<h3 id="worker-1-nic-bond"><a class="header" href="#worker-1-nic-bond">worker-1 nic bond</a></h3>
<pre><code class="language-bash">ip link show
# 2: eno1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
#     link/ether cc:64:a6:59:bd:24 brd ff:ff:ff:ff:ff:ff
# 3: eno2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
#     link/ether cc:64:a6:59:bd:25 brd ff:ff:ff:ff:ff:ff
# 4: ens2f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
#     link/ether 08:4f:0a:b5:a2:be brd ff:ff:ff:ff:ff:ff
# 5: eno3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
#     link/ether cc:64:a6:59:bd:26 brd ff:ff:ff:ff:ff:ff
# 6: eno4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
#     link/ether cc:64:a6:59:bd:27 brd ff:ff:ff:ff:ff:ff
# 7: ens2f1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
#     link/ether 08:4f:0a:b5:a2:bf brd ff:ff:ff:ff:ff:ff

ip a s eno1
# 2: eno1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000
#     link/ether cc:64:a6:59:bd:24 brd ff:ff:ff:ff:ff:ff
#     inet 39.134.201.65/27 brd 39.134.201.95 scope global noprefixroute eno1
#        valid_lft forever preferred_lft forever
#     inet6 fe80::149f:d0ce:2700:4bf2/64 scope link noprefixroute
#        valid_lft forever preferred_lft forever

ethtool eno1  # 10000baseT/Full
ethtool eno2  # 10000baseT/Full
ethtool eno3  # 1000baseT/Full
ethtool eno4  # 1000baseT/Full
ethtool ens2f0  #  10000baseT/Full
ethtool ens2f1  #  10000baseT/Full

nmcli con add type bond \
    con-name bond0 \
    ifname bond0 \
    mode 802.3ad 

nmcli con mod id bond0 bond.options \
    mode=802.3ad,miimon=100,lacp_rate=fast,xmit_hash_policy=layer2+3
    
nmcli con add type bond-slave ifname eno2 con-name eno2 master bond0
nmcli con add type bond-slave ifname ens2f0 con-name ens2f0 master bond0
nmcli con add type bond-slave ifname ens2f1 con-name ens2f1 master bond0

nmcli con down eno2
nmcli con up eno2
nmcli con down ens2f0
nmcli con up ens2f0
nmcli con down ens2f1
nmcli con up ens2f1
nmcli con down bond0
nmcli con start bond0       


#######################################
# nic bond
cat &gt; /root/nic.bond.sh &lt;&lt; 'EOF'
#!/bin/bash

set -x 

# delete all connection 
nmcli -g uuid con | while read i ; do nmcli c delete  ${i} ; done 

nmcli con add type bond \
    con-name bond0 \
    ifname bond0 \
    mode 802.3ad \
    ipv4.method 'manual' \
    ipv4.address '39.134.201.65/27' \
    ipv4.gateway '39.134.201.94' \
    ipv4.dns '117.177.241.16'
    
nmcli con mod id bond0 bond.options \
    mode=802.3ad,miimon=100,lacp_rate=fast,xmit_hash_policy=layer2+3

nmcli con add type bond-slave ifname eno1 con-name eno1 master bond0    
nmcli con add type bond-slave ifname eno2 con-name eno2 master bond0
nmcli con add type bond-slave ifname ens2f0 con-name ens2f0 master bond0
nmcli con add type bond-slave ifname ens2f1 con-name ens2f1 master bond0

systemctl restart network

EOF

cat &gt; /root/nic.restore.sh &lt;&lt; 'EOF'
#!/bin/bash

set -x 

# delete all connection 
nmcli -g uuid con | while read i ; do nmcli c delete  ${i} ; done 

# re-create primary connection 
nmcli con add type ethernet \
    con-name eno1 \
    ifname eno1 \
    ipv4.method 'manual' \
    ipv4.address '39.134.201.65/27' \
    ipv4.gateway '39.134.201.94' \
    ipv4.dns '117.177.241.16'

systemctl restart network

exit 0
EOF

chmod +x /root/nic.restore.sh

cat &gt; ~/cron-network-con-recreate &lt;&lt; EOF
*/20 * * * * /bin/bash /root/nic.restore.sh
EOF

crontab ~/cron-network-con-recreate

bash /root/nic.bond.sh

# debug
cat /proc/net/bonding/bond0
cat /sys/class/net/bond*/bonding/xmit_hash_policy
# https://access.redhat.com/solutions/666853
ip -s -h link show master bond0
</code></pre>
<h3 id="worker-2-host"><a class="header" href="#worker-2-host">worker-2 host</a></h3>
<pre><code class="language-bash">
mkdir /etc/yum.repos.d.bak
mv /etc/yum.repos.d/* /etc/yum.repos.d.bak

cat &lt;&lt; EOF &gt; /etc/yum.repos.d/remote.repo
[remote]
name=RHEL FTP
baseurl=ftp://117.177.241.16/data
enabled=1
gpgcheck=0

EOF

yum clean all
yum --disableplugin=subscription-manager  repolist

yum install -y byobu htop iostat

yum -y update

hostnamectl set-hostname worker-2.ocpsc.redhat.ren

nmcli connection modify eno1 ipv4.dns 117.177.241.16
nmcli connection reload
nmcli connection up eno1

yum -y install fail2ban

cat &lt;&lt; EOF &gt; /etc/fail2ban/jail.d/wzh.conf
[sshd]
enabled = true

EOF

systemctl enable fail2ban
systemctl restart fail2ban

cat &lt;&lt; EOF &gt; /etc/fail2ban/jail.d/wzh.conf
[sshd]
enabled = true

[recidive]
enabled = true

EOF

systemctl restart fail2ban

fail2ban-client status sshd
fail2ban-client status recidive
systemctl status fail2ban
tail -F /var/log/fail2ban.log

cp /etc/ssh/sshd_config /etc/ssh/sshd_config.BAK
sed -i 's/#UseDNS yes/UseDNS no/g' /etc/ssh/sshd_config

diff /etc/ssh/sshd_config /etc/ssh/sshd_config.BAK

systemctl restart sshd

passwd

useradd -m wzh

lsblk | grep 5.5 | awk '{print $1}' | xargs -I DEMO echo -n &quot;/dev/DEMO &quot;
# /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk
lsblk | grep 5.5 | awk '{print $1}' | wc -l
# 24

yum install -y lvm2

pvcreate -y /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

vgcreate datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

vgs

lvcreate --type raid0 -l 100%FREE --stripes 24 -n datalv datavg

mkfs.xfs /dev/datavg/datalv

lvdisplay /dev/datavg/datalv -m

mkdir -p /data

cp /etc/fstab /etc/fstab.bak

cat &lt;&lt; EOF &gt;&gt; /etc/fstab
/dev/datavg/datalv /data                  xfs     defaults        0 0

EOF

mount -a

yum install -y sysstat
lsblk | grep disk | awk '{print $1}' | xargs -I DEMO echo -n &quot;DEMO &quot;
# sda sdb sdc sdd sde sdf sdg sdh sdi sdj sdk sdl sdm
iostat -m -x sda sdb sdc sdd sde sdf sdg sdh sdi sdj sdk 5
iostat -m -x dm-10 5


########################################
# ntp
yum install -y chrony
systemctl enable chronyd
systemctl restart chronyd
systemctl status chronyd
chronyc tracking

systemctl disable --now firewalld.service

# setup time server
/bin/cp -f /etc/chrony.conf /etc/chrony.conf.bak

cat &lt;&lt; EOF &gt; /etc/chrony.conf
server 117.177.241.16 iburst
server 0.rhel.pool.ntp.org iburst
server 1.rhel.pool.ntp.org iburst
server 2.rhel.pool.ntp.org iburst
server 3.rhel.pool.ntp.org iburst
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
logdir /var/log/chrony
EOF

systemctl restart chronyd
systemctl status chronyd
chronyc tracking
chronyc sources -v

# update ntp
cat &lt;&lt; EOF &gt; /etc/chrony.conf
server 223.87.20.100 iburst
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
logdir /var/log/chrony
EOF

systemctl restart chronyd
systemctl status chronyd
chronyc tracking


</code></pre>
<h3 id="worker-2-disk"><a class="header" href="#worker-2-disk">worker-2 disk</a></h3>
<pre><code class="language-bash">

#########################################
# ssd cache + hdd
# https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/logical_volume_manager_administration/index#lvm_cache_volume_creation
umount /data
lsblk -d -o name,rota

lvremove  /dev/datavg/datalv

# lsblk | grep 894 | awk '{print $1}'

pvcreate /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

# https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/logical_volume_manager_administration/vg_grow
vgextend datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

## raid5

lvcreate --type raid5 -L 1G --stripes 23 -n hddlv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

lvcreate --type raid5 -L 1G --stripes 23 -n mixlv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

lvcreate --type raid5 -L 1G --stripes 9 -n ssdlv datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai


lvcreate --type raid5 -L 3T --stripes 23 -n mix0lv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx


lvcreate --type raid0 -L 1.3536T --stripes 10 -n cachemix0 datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvcreate --type raid0 -L 13G --stripes 10 -n cachemix0meta datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvconvert --type cache-pool --poolmetadata datavg/cachemix0meta datavg/cachemix0

lvconvert --type cache --cachepool datavg/cachemix0 datavg/mix0lv

# lvcreate --type raid5 --stripes 9 -L 1T -I 16M -R 4096K -n hddlv datavg /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk


## raid0 + stripe



lvcreate --type raid0 -L 1T --stripes 24 -n hdd0lv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

fio --rw=rw --rwmixread=99 --bsrange=4k-256k --name=vdo \
    --filename=/dev/datavg/hdd0lv --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=1 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g 
# Run status group 0 (all jobs):
#    READ: bw=2453MiB/s (2572MB/s), 2453MiB/s-2453MiB/s (2572MB/s-2572MB/s), io=98.0GiB (106GB), run=41331-41331msec
#   WRITE: bw=24.9MiB/s (26.1MB/s), 24.9MiB/s-24.9MiB/s (26.1MB/s-26.1MB/s), io=1029MiB (1079MB), run=41331-41331msec
lvs -o+stripesize,chunksize datavg/hdd0lv
  # LV     VG     Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert Stripe Chunk
  # hdd0lv datavg rwi-aor--- 1.00t                                                     64.00k    0
lvremove -f datavg/hdd0lv

lvcreate --type raid0 -L 1T -I 128 --stripes 24 -n hdd1lv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

fio --rw=rw --rwmixread=99 --bsrange=4k-256k --name=vdo \
    --filename=/dev/datavg/hdd1lv --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=1 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g 
# Run status group 0 (all jobs):
#    READ: bw=2674MiB/s (2804MB/s), 2674MiB/s-2674MiB/s (2804MB/s-2804MB/s), io=98.0GiB (106GB), run=37912-37912msec
#   WRITE: bw=27.1MiB/s (28.4MB/s), 27.1MiB/s-27.1MiB/s (28.4MB/s-28.4MB/s), io=1029MiB (1079MB), run=37912-37912msec
lvs -o+stripesize,chunksize datavg/hdd1lv
  # LV     VG     Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert Stripe  Chunk
  # hdd1lv datavg rwi-a-r--- 1.00t                                                     128.00k    0
lvremove -f datavg/hdd1lv

lvcreate --type raid0 -L 1T -I 256 --stripes 24 -n hdd1lv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

fio --rw=rw --rwmixread=99 --bsrange=4k-256k --name=vdo \
    --filename=/dev/datavg/hdd1lv --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=1 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g 
# Run status group 0 (all jobs):
#    READ: bw=2674MiB/s (2804MB/s), 2674MiB/s-2674MiB/s (2804MB/s-2804MB/s), io=98.0GiB (106GB), run=37912-37912msec
#   WRITE: bw=27.1MiB/s (28.4MB/s), 27.1MiB/s-27.1MiB/s (28.4MB/s-28.4MB/s), io=1029MiB (1079MB), run=37912-37912msec
lvs -o+stripesize,chunksize datavg/hdd1lv
  # LV     VG     Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert Stripe  Chunk
  # hdd1lv datavg rwi-a-r--- 1.00t                                                     256.00k    0k    0
lvremove -f datavg/hdd1lv


lvcreate --type raid0 -L 300G --stripes 10 -n ssd0lv datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

fio --rw=rw --rwmixread=99 --bsrange=4k-256k --name=vdo \
    --filename=/dev/datavg/ssd0lv --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=1 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g 
# Run status group 0 (all jobs):
#    READ: bw=2602MiB/s (2728MB/s), 2602MiB/s-2602MiB/s (2728MB/s-2728MB/s), io=98.0GiB (106GB), run=38965-38965msec
#   WRITE: bw=26.4MiB/s (27.7MB/s), 26.4MiB/s-26.4MiB/s (27.7MB/s-27.7MB/s), io=1029MiB (1079MB), run=38965-38965msec
lvs -o+stripesize,chunksize datavg/ssd0lv
  # LV     VG     Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert Stripe Chunk
  # ssd0lv datavg rwi-a-r--- 300.00g                                                     64.00k    0
lvremove -f datavg/ssd0lv

lvcreate --type raid0 -L 300G -I 128 --stripes 10 -n ssd0lv datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

fio --rw=rw --rwmixread=99 --bsrange=4k-256k --name=vdo \
    --filename=/dev/datavg/ssd0lv --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=1 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g 
# Run status group 0 (all jobs):
#    READ: bw=2438MiB/s (2556MB/s), 2438MiB/s-2438MiB/s (2556MB/s-2556MB/s), io=98.0GiB (106GB), run=41584-41584msec
#   WRITE: bw=24.7MiB/s (25.9MB/s), 24.7MiB/s-24.7MiB/s (25.9MB/s-25.9MB/s), io=1029MiB (1079MB), run=41584-41584msec
lvs -o+stripesize,chunksize datavg/ssd0lv
  # LV     VG     Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert Stripe  Chunk
  # ssd0lv datavg rwi-a-r--- 300.00g                                                     128.00k    0
lvremove -f datavg/ssd0lv

lvcreate --type raid0 -L 300G -I 256 --stripes 10 -n ssd0lv datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

fio --rw=rw --rwmixread=99 --bsrange=4k-256k --name=vdo \
    --filename=/dev/datavg/ssd0lv --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=1 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g 
# Run status group 0 (all jobs):
#    READ: bw=1908MiB/s (2000MB/s), 1908MiB/s-1908MiB/s (2000MB/s-2000MB/s), io=98.0GiB (106GB), run=53135-53135msec
#   WRITE: bw=19.4MiB/s (20.3MB/s), 19.4MiB/s-19.4MiB/s (20.3MB/s-20.3MB/s), io=1029MiB (1079MB), run=53135-53135msec
lvs -o+stripesize,chunksize datavg/ssd0lv
  LV     VG     Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert Stripe  Chunk
  # ssd0lv datavg rwi-a-r--- 300.00g                                                     256.00k    0   0
lvremove -f datavg/ssd0lv


lvcreate --type raid5 -L 120G --stripes 23 -n hdd5lv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

fio --rw=rw --rwmixread=99 --bsrange=4k-256k --name=vdo \
    --filename=/dev/datavg/hdd5lv --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=1 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g 
# Run status group 0 (all jobs):
#    READ: bw=474MiB/s (497MB/s), 474MiB/s-474MiB/s (497MB/s-497MB/s), io=98.0GiB (106GB), run=214073-214073msec
#   WRITE: bw=4920KiB/s (5038kB/s), 4920KiB/s-4920KiB/s (5038kB/s-5038kB/s), io=1029MiB (1079MB), run=214073-214073msec
lvs -o+stripesize,chunksize datavg/hdd5lv
  # LV     VG     Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert Stripe Chunk
  # hdd5lv datavg rwi-a-r--- 120.03g                                    100.00           64.00k    0
lvremove -f datavg/hdd5lv


lvcreate --type raid5 -L 120G -I 128 --stripes 23 -n hdd5lv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

fio --rw=rw --rwmixread=99 --bsrange=4k-256k --name=vdo \
    --filename=/dev/datavg/hdd5lv --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=1 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g 
# Run status group 0 (all jobs):
#    READ: bw=449MiB/s (471MB/s), 449MiB/s-449MiB/s (471MB/s-471MB/s), io=98.0GiB (106GB), run=225892-225892msec
#   WRITE: bw=4663KiB/s (4775kB/s), 4663KiB/s-4663KiB/s (4775kB/s-4775kB/s), io=1029MiB (1079MB), run=225892-225892msec
lvs -o+stripesize,chunksize datavg/hdd5lv
  # LV     VG     Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert Stripe  Chunk
  # hdd5lv datavg rwi-a-r--- 120.03g                                    100.00           128.00k    0
lvremove -f datavg/hdd5lv


lvcreate --type raid5 -L 120G --stripes 23 -n mixtestlv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

lvcreate --type raid0 -L 40G --stripes 10 -n cachetest datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvcreate --type raid0 -L 1G --stripes 10 -n cache1testmeta datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvconvert --type cache-pool --poolmetadata datavg/cache1testmeta datavg/cachetest

lvconvert --type cache --cachepool datavg/cachetest datavg/mixtestlv

fio --rw=rw --rwmixread=99 --bsrange=4k-256k --name=vdo \
    --filename=/dev/datavg/mixtestlv --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=1 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g 
# Run status group 0 (all jobs):
#    READ: bw=449MiB/s (471MB/s), 449MiB/s-449MiB/s (471MB/s-471MB/s), io=98.0GiB (106GB), run=225892-225892msec
#   WRITE: bw=4663KiB/s (4775kB/s), 4663KiB/s-4663KiB/s (4775kB/s-4775kB/s), io=1029MiB (1079MB), run=225892-225892msec
lvs -o+stripesize,chunksize datavg/mixtestlv
  # LV     VG     Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert Stripe  Chunk
  # hdd5lv datavg rwi-a-r--- 120.03g                                    100.00           128.00k    0
lvremove -f datavg/mixtestlv



lvcreate --type raid0 -L 1T --stripes 24 -n hdd1lv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

fio --rw=randrw --rwmixread=99 --bsrange=4k-256k --name=vdo \
    --filename=/dev/datavg/hdd1lv --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=1 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g 
# Run status group 0 (all jobs):
#    READ: bw=2453MiB/s (2572MB/s), 2453MiB/s-2453MiB/s (2572MB/s-2572MB/s), io=98.0GiB (106GB), run=41331-41331msec
#   WRITE: bw=24.9MiB/s (26.1MB/s), 24.9MiB/s-24.9MiB/s (26.1MB/s-26.1MB/s), io=1029MiB (1079MB), run=41331-41331msec
lvs -o+stripesize,chunksize datavg/hdd1lv
  # LV     VG     Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert Stripe Chunk
  # hdd0lv datavg rwi-aor--- 1.00t                                                     64.00k    0
lvremove -f datavg/hdd1lv



lvcreate --type raid0 -L 300G --stripes 10 -n ssd0lv datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

fio --rw=randrw --rwmixread=99 --bsrange=4k-256k --name=vdo \
    --filename=/dev/datavg/ssd0lv --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=1 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g 
# Run status group 0 (all jobs):
#    READ: bw=1527MiB/s (1601MB/s), 1527MiB/s-1527MiB/s (1601MB/s-1601MB/s), io=98.0GiB (106GB), run=66375-66375msec
#   WRITE: bw=15.5MiB/s (16.2MB/s), 15.5MiB/s-15.5MiB/s (16.2MB/s-16.2MB/s), io=1029MiB (1079MB), run=66375-66375msec
lvs -o+stripesize,chunksize datavg/ssd0lv
  # LV     VG     Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert Stripe Chunk
  # ssd0lv datavg rwi-a-r--- 300.00g                                                     64.00k    0
lvremove -f datavg/ssd0lv








lvcreate --type raid0 -L 1G --stripes 24 -n hddlv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx



lvcreate --type raid0 -L 130T --stripes 24 -n mixlv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

# lvcreate --type raid0 -L 300G --stripes 10 -n ssdlv datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvcreate --type raid0 -L 8.6T --stripes 10 -n cache1 datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvcreate --type raid0 -L 40G --stripes 10 -n cache1meta datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvconvert --type cache-pool --poolmetadata datavg/cache1meta datavg/cache1

# lvs -a -o name,size,attr,devices datavg

lvconvert --type cache --cachepool datavg/cache1 datavg/mixlv

lvconvert --splitcache datavg/mixlv

# lvs -a -o name,size,attr,devices datavg
# lvs -o+cache_mode datavg

mkfs.xfs /dev/datavg/hddlv
mkfs.xfs /dev/datavg/ssdlv
mkfs.xfs /dev/datavg/mixlv
mkfs.xfs /dev/datavg/mix0lv

mkdir -p /data/
mkdir -p /data_ssd/
mkdir -p /data_mix/
mkdir -p /data_mix0

cat /etc/fstab

cat &lt;&lt; EOF &gt;&gt; /etc/fstab
/dev/datavg/hddlv /data                  xfs     defaults        0 0
/dev/datavg/ssdlv /data_ssd                  xfs     defaults        0 0
/dev/datavg/mixlv /data_mix                  xfs     defaults        0 0
/dev/datavg/mix0lv  /data_mix0                  xfs     defaults        0 0
EOF

mount -a
df -h | grep \/data

dd if=/dev/zero of=/data/testfile bs=4k count=9999 oflag=dsync
dd if=/dev/zero of=/data_ssd/testfile bs=4k count=9999 oflag=dsync
dd if=/dev/zero of=/data_mix/testfile bs=4k count=9999 oflag=dsync

dd if=/dev/zero of=/data/testfile bs=4M count=9999 oflag=dsync
dd if=/dev/zero of=/data_ssd/testfile bs=4M count=9999 oflag=dsync
dd if=/dev/zero of=/data_mix/testfile bs=4M count=9999 oflag=dsync

dd if=/dev/zero of=/data/testfile.large bs=4M count=9999 oflag=direct
dd if=/dev/zero of=/data_ssd/testfile.large bs=4M count=9999 oflag=direct
dd if=/dev/zero of=/data_mix/testfile.large bs=4M count=9999 oflag=direct

dd if=/dev/zero of=/data/testfile.large bs=4M count=9999
dd if=/dev/zero of=/data_ssd/testfile.large bs=4M count=9999 
dd if=/dev/zero of=/data_mix/testfile.large bs=4M count=9999 

dd if=/data/testfile.large of=/dev/null bs=4k count=9999 oflag=dsync
dd if=/data_ssd/testfile.large of=/dev/null bs=4k count=9999 oflag=dsync
dd if=/data_mix/testfile.large of=/dev/null bs=4k count=999999 oflag=dsync

dd if=/data/testfile.large of=/dev/null bs=4M count=9999 oflag=dsync
dd if=/data_ssd/testfile.large of=/dev/null bs=4M count=9999 oflag=dsync
dd if=/data_mix/testfile.large of=/dev/null bs=4M count=9999 oflag=dsync

dd if=/data/testfile.large of=/dev/null bs=4M count=9999
dd if=/data_ssd/testfile.large of=/dev/null bs=4M count=9999
dd if=/data_mix/testfile.large of=/dev/null bs=4M count=9999

# cleanup
umount /data/
umount /data_ssd/
umount /data_mix/
umount /data_mix0/
lvremove -f /dev/datavg/hddlv
lvremove -f /dev/datavg/ssdlv
lvremove -f /dev/datavg/mixlv
lvremove -f /dev/datavg/mix0lv


# ssd tunning
# https://serverfault.com/questions/80134/linux-md-vs-lvm-performance
hdparm -tT /dev/md0

# https://www.ibm.com/developerworks/cn/linux/l-lo-io-scheduler-optimize-performance/index.html
cat /sys/block/*/queue/scheduler

lsblk | grep 894 | awk '{print $1}' | xargs -I DEMO cat /sys/block/DEMO/queue/scheduler

lsblk | grep 894 | awk '{print &quot;echo deadline &gt; /sys/block/&quot;$1&quot;/queue/scheduler&quot;}' 

fio --rw=rw --rwmixread=99 --bsrange=4k-256k --name=vdo \
    --directory=./ --ioengine=libaio --numjobs=1 --thread \
    --norandommap --runtime=300 --direct=0 --iodepth=8 \
    --scramble_buffers=1 --offset=0 --size=100g 

fio --rw=rw --rwmixread=99 --bsrange=4k-256k --name=vdo \
    --directory=./ --ioengine=sync --size=100g 

blktrace /dev/datavg/mix0lv /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai     /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

blkparse -o /dev/null -i dm-244 -d dm-244.bin
btt -i dm-244.bin | less

blkparse -o /dev/null -i sdaa -d sdaa.bin
btt -i sdaa.bin | less

blkparse -o /dev/null -i sda -d sda.bin
btt -i sda.bin | less


blktrace /dev/datavg/ssd0lv /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai    


lvmconfig --typeconfig default --withcomments --withspaces

lvmconfig --type default --withcomments allocation/cache_policy
lvmconfig --type default --withcomments allocation/cache_settings
lvmconfig --type list --withcomments allocation/cache_settings

iostat -x -m 3 /dev/mapper/datavg-mixlv sdh sdab

dstat -D /dev/mapper/datavg-mixlv,/dev/mapper/datavg-mixlv_corig,sdh,sdab -N bond0

dstat -D /dev/mapper/datavg-mixlv,/dev/mapper/datavg-mixlv_corig,sdh,sdab --disk-util 

bmon -p eno1,eno2,ens2f0,ens2f1,bond0

lvs -o+lv_all datavg/mixlv_corig

lvs -o+Layout datavg/mixlv_corig

lvs -o+CacheReadHits,CacheReadMisses

lvs -o+Layout


blockdev --report    
# RO    RA   SSZ   BSZ   StartSec            Size   Device
# rw  8192   512  4096          0    478998953984   /dev/sdy
# rw  8192   512   512       2048      1073741824   /dev/sdy1
# rw  8192   512  4096    2099200      1073741824   /dev/sdy2
# rw  8192   512  4096    4196352    476849373184   /dev/sdy3
# rw  8192   512  4096          0    958999298048   /dev/sdaj
# rw  8192   512  4096       2048    958998249472   /dev/sdaj1
# rw  8192   512  4096          0   6001175126016   /dev/sda
# rw  8192   512  4096          0   6001175126016   /dev/sdd
# rw  8192   512  4096          0   6001175126016   /dev/sde
# rw  8192   512  4096          0   6001175126016   /dev/sdc
# rw  8192   512  4096          0   6001175126016   /dev/sdf
# rw  8192   512  4096          0   6001175126016   /dev/sdb
# rw  8192   512  4096          0   6001175126016   /dev/sdg
# rw  8192   512  4096          0   6001175126016   /dev/sdh
# rw  8192   512  4096          0   6001175126016   /dev/sdk
# rw  8192   512  4096          0   6001175126016   /dev/sdi
# rw  8192   512  4096          0   6001175126016   /dev/sdm
# rw  8192   512  4096          0   6001175126016   /dev/sdj
# rw  8192   512  4096          0   6001175126016   /dev/sdl
# rw  8192   512  4096          0   6001175126016   /dev/sdn
# rw  8192   512  4096          0   6001175126016   /dev/sdo
# rw  8192   512  4096          0   6001175126016   /dev/sdp
# rw  8192   512  4096          0   6001175126016   /dev/sdx
# rw  8192   512  4096          0   6001175126016   /dev/sdq
# rw  8192   512  4096          0   6001175126016   /dev/sdr
# rw  8192   512  4096          0   6001175126016   /dev/sdu
# rw  8192   512  4096          0   6001175126016   /dev/sdw
# rw  8192   512  4096          0   6001175126016   /dev/sds
# rw  8192   512  4096          0   6001175126016   /dev/sdt
# rw  8192   512  4096          0   6001175126016   /dev/sdv
# rw  8192   512  4096          0    960197124096   /dev/sdz
# rw  8192   512  4096          0    960197124096   /dev/sdaa
# rw  8192   512  4096          0    960197124096   /dev/sdac
# rw  8192   512  4096          0    960197124096   /dev/sdab
# rw  8192   512  4096          0    960197124096   /dev/sdad
# rw  8192   512  4096          0    960197124096   /dev/sdae
# rw  8192   512  4096          0    960197124096   /dev/sdag
# rw  8192   512  4096          0    960197124096   /dev/sdaf
# rw  8192   512  4096          0    960197124096   /dev/sdai
# rw  8192   512  4096          0    960197124096   /dev/sdah
# rw  8192   512  4096          0   5955689381888   /dev/dm-0
# rw  8192   512  4096          0   5955689381888   /dev/dm-1
# rw  8192   512  4096          0   5955689381888   /dev/dm-2
# rw  8192   512  4096          0   5955689381888   /dev/dm-3
# rw  8192   512  4096          0   5955689381888   /dev/dm-4
# rw  8192   512  4096          0   5955689381888   /dev/dm-5
# rw  8192   512  4096          0   5955689381888   /dev/dm-6
# rw  8192   512  4096          0   5955689381888   /dev/dm-7
# rw  8192   512  4096          0   5955689381888   /dev/dm-8
# rw  8192   512  4096          0   5955689381888   /dev/dm-9
# rw  8192   512  4096          0   5955689381888   /dev/dm-10
# rw  8192   512  4096          0   5955689381888   /dev/dm-11
# rw  8192   512  4096          0   5955689381888   /dev/dm-12
# rw  8192   512  4096          0   5955689381888   /dev/dm-13
# rw  8192   512  4096          0   5955689381888   /dev/dm-14
# rw  8192   512  4096          0   5955689381888   /dev/dm-15
# rw  8192   512  4096          0   5955689381888   /dev/dm-16
# rw  8192   512  4096          0   5955689381888   /dev/dm-17
# rw  8192   512  4096          0   5955689381888   /dev/dm-18
# rw  8192   512  4096          0   5955689381888   /dev/dm-19
# rw  8192   512  4096          0   5955689381888   /dev/dm-20
# rw  8192   512  4096          0   5955689381888   /dev/dm-21
# rw  8192   512  4096          0   5955689381888   /dev/dm-22
# rw  8192   512  4096          0   5955689381888   /dev/dm-23
# rw  8192   512  4096          0 142936545165312   /dev/dm-24
# rw  8192   512  4096          0    945580670976   /dev/dm-25
# rw  8192   512  4096          0    945580670976   /dev/dm-26
# rw  8192   512  4096          0    945580670976   /dev/dm-27
# rw  8192   512  4096          0    945580670976   /dev/dm-28
# rw  8192   512  4096          0    945580670976   /dev/dm-29
# rw  8192   512  4096          0    945580670976   /dev/dm-30
# rw  8192   512  4096          0    945580670976   /dev/dm-31
# rw  8192   512  4096          0    945580670976   /dev/dm-32
# rw  8192   512  4096          0    945580670976   /dev/dm-33
# rw  8192   512  4096          0    945580670976   /dev/dm-34
# rw  8192   512  4096          0   9455806709760   /dev/dm-35
# rw  8192   512  4096          0      4294967296   /dev/dm-36
# rw  8192   512  4096          0      4294967296   /dev/dm-37
# rw  8192   512  4096          0      4294967296   /dev/dm-38
# rw  8192   512  4096          0      4294967296   /dev/dm-39
# rw  8192   512  4096          0      4294967296   /dev/dm-40
# rw  8192   512  4096          0      4294967296   /dev/dm-41
# rw  8192   512  4096          0      4294967296   /dev/dm-42
# rw  8192   512  4096          0      4294967296   /dev/dm-43
# rw  8192   512  4096          0      4294967296   /dev/dm-44
# rw  8192   512  4096          0      4294967296   /dev/dm-45
# rw  8192   512  4096          0     42949672960   /dev/dm-46
# rw  8192   512  4096          0 142936545165312   /dev/dm-47
# rw  8192   512  4096          0        46137344   /dev/dm-48
# rw  8192   512  4096          0        46137344   /dev/dm-49
# rw  8192   512  4096          0        46137344   /dev/dm-50
# rw  8192   512  4096          0        46137344   /dev/dm-51
# rw  8192   512  4096          0        46137344   /dev/dm-52
# rw  8192   512  4096          0        46137344   /dev/dm-53
# rw  8192   512  4096          0        46137344   /dev/dm-54
# rw  8192   512  4096          0        46137344   /dev/dm-55
# rw  8192   512  4096          0        46137344   /dev/dm-56
# rw  8192   512  4096          0        46137344   /dev/dm-57
# rw  8192   512  4096          0        46137344   /dev/dm-58
# rw  8192   512  4096          0        46137344   /dev/dm-59
# rw  8192   512  4096          0        46137344   /dev/dm-60
# rw  8192   512  4096          0        46137344   /dev/dm-61
# rw  8192   512  4096          0        46137344   /dev/dm-62
# rw  8192   512  4096          0        46137344   /dev/dm-63
# rw  8192   512  4096          0        46137344   /dev/dm-64
# rw  8192   512  4096          0        46137344   /dev/dm-65
# rw  8192   512  4096          0        46137344   /dev/dm-66
# rw  8192   512  4096          0        46137344   /dev/dm-67
# rw  8192   512  4096          0        46137344   /dev/dm-68
# rw  8192   512  4096          0        46137344   /dev/dm-69
# rw  8192   512  4096          0        46137344   /dev/dm-70
# rw  8192   512  4096          0        46137344   /dev/dm-71
# rw  8192   512  4096          0      1107296256   /dev/dm-72    

# https://access.redhat.com/solutions/3588841
/sbin/blockdev --setra 4096 /dev/mapper/datavg-mixlv
/sbin/blockdev --setra 8192 /dev/mapper/datavg-mixlv
/sbin/blockdev --setra 16384 /dev/mapper/datavg-mixlv
/sbin/blockdev --setra 32768 /dev/mapper/datavg-mixlv
/sbin/blockdev --setra 65536 /dev/mapper/datavg-mixlv
/sbin/blockdev --setra 131072 /dev/mapper/datavg-mixlv
/sbin/blockdev --setra 262144 /dev/mapper/datavg-mixlv

# final config
/sbin/blockdev --setra 16384 /dev/mapper/datavg-mixlv
for f in /dev/mapper/datavg-mixlv_corig_rimage_*; do /sbin/blockdev --setra 16384  $f ; done

# worker2
# 5.5
find /data_mix/mnt/ -type f &gt; list
dstat --output /root/dstat.csv -D /dev/mapper/datavg-mixlv,/dev/mapper/datavg-mixlv_corig,sdh,sdab -N bond0

var_basedir=&quot;/data_mix/mnt&quot;
find $var_basedir -type f -size -511M  &gt; list.512m
find $var_basedir -type f -size -2049M  -size +511M &gt; list.2g
find $var_basedir -type f -size +2049M &gt; list.+2g

cat list | shuf &gt; list.shuf.all

cat list.512m | shuf &gt; list.shuf.512m
cat list.2g | shuf &gt; list.shuf.2g
cat list.+2g | shuf &gt; list.shuf.+2g
cat list.2g list.+2g | shuf &gt; list.shuf.+512m

rm -f split.list.*
# zte use 1800
var_total=10
# split -n l/$var_total list.shuf.all split.list.all.
split -n l/$var_total list.shuf.512m split.list.512m.
split -n l/$var_total list.shuf.2g split.list.2g.
split -n l/$var_total list.shuf.+2g split.list.+2g.
split -n l/$var_total list.shuf.+512m split.list.+512m.

for f in split.list.512m.*; do 
    cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
done
# for f in split.list.+512m.*; do 
#     cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
# done
for f in split.list.2g.*; do 
    cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
done
for f in split.list.+2g.*; do 
    cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
done

ps -ef | grep /data_mix/mnt | grep cat | awk '{print $2}' | xargs -I DEMO kill DEMO

tmux kill-window -t 3


# rm -f split.*

# 2.8
var_num=`echo &quot;scale=0;$(cat list | wc -l  )/5&quot; | bc -l`
head -n $var_num list &gt; list.20
tail -n +$var_num list &gt; list.80

var_total=1500
# split -n l/$(echo &quot;scale=0;$var_total/5*4&quot;|bc -l) list.20 split.list.20.
# while true; do
#   for f in split.list.20.*; do 
#       cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
#   done
#   echo &quot;wait to finish&quot;
#   wait
# done
var_runtimes=$(echo &quot;scale=0;$var_total/5*4&quot;|bc -l)
while true; do
  for ((i=1; i&lt;=$var_runtimes; i++)); do
    echo &quot;Welcome $i times&quot;
    cat list.20 | shuf | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
  done
  echo &quot;wait to finish&quot;
  wait
done

var_total=1500
# split -n l/$(echo &quot;scale=0;$var_total/5*1&quot;|bc -l) list.80 split.list.80.
# while true; do
#   for f in split.list.80.*; do 
#       cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
#   done
#   echo &quot;wait to finish&quot;
#   wait
# done
var_runtimes=$(echo &quot;scale=0;$var_total/5*1&quot;|bc -l)
while true; do
  for ((i=1; i&lt;=$var_runtimes; i++)); do
    echo &quot;Welcome $i times&quot;
    cat list.80 | shuf | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
  done
  echo &quot;wait to finish&quot;
  wait
done
# 500M-1.2GB/s
ps -ef | grep /data_mix/mnt | grep cat | awk '{print $2}' | xargs -I DEMO kill DEMO



</code></pre>
<h3 id="worker-2-disk-tunning"><a class="header" href="#worker-2-disk-tunning">worker-2 disk tunning</a></h3>
<pre><code class="language-bash">
# 8.6T cache / 130T hdd = 6.6%
# 660G cache / 10T hdd 

lvcreate --type raid0 -L 10T --stripesize 2048k --stripes 24 -n ext02lv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

lvcreate --type raid0 -L 10T --stripesize 4096k --stripes 24 -n ext04lv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

lvcreate --type raid5 -L 10T --stripesize 2048k --stripes 23 -n ext52lv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

lvcreate --type raid5 -L 10T --stripesize 2048k --stripes 11 -n ext52lv12 datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl 



lvcreate --type raid0 -L 10T --stripesize 2048k --stripes 24 -n xfs02lv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

lvcreate --type raid0 -L 10T --stripesize 4096k --stripes 24 -n xfs04lv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

lvcreate --type raid5 -L 10T --stripesize 2048k --stripes 23 -n xfs52lv datavg /dev/sda /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx

lvcreate --type raid5 -L 10T --stripesize 2048k --stripes 11 -n xfs52lv12 datavg /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx


lvcreate --type raid0 -L 3.5T --stripesize 1024k --stripes 10 -n ext01lvssd datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvcreate --type raid0 -L 3.5T --stripesize 1024k --stripes 10 -n xfs01lvssd datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvcreate --type raid0 -L 700G --stripesize 2048k --stripes 10 -n cachelv datavg /dev/sdz /dev/sdaa /dev/sdab /dev/sdac /dev/sdad /dev/sdae /dev/sdaf /dev/sdag /dev/sdah /dev/sdai

lvconvert --type cache-pool datavg/cachelv

lvconvert --type cache --cachepool datavg/cachelv datavg/ext02lv

# lvconvert --splitcache datavg/ext02lv
# lvconvert --uncache datavg/ext02lv

lvs -o+layout,stripesize
  # LV         VG     Attr       LSize  Pool      Origin          Data%  Meta%  Move Log Cpy%Sync Convert Layout              Stripe
  # ext01lvssd datavg rwi-a-r---  3.50t                                                                   raid,raid0           1.00m
  # ext02lv    datavg Cwi-a-C--- 10.00t [cachelv] [ext02lv_corig] 0.01   16.41           0.00             cache                   0
  # ext04lv    datavg rwi-a-r--- 10.00t                                                                   raid,raid0           4.00m
  # ext52lv    datavg rwi-a-r--- 10.00t                                                  9.72             raid,raid5,raid5_ls  2.00m
  # xfs01lvssd datavg rwi-a-r---  3.50t                                                                   raid,raid0           1.00m

mkdir -p /data_ext02
mkdir -p /data_ext04
mkdir -p /data_ext52
mkdir -p /data_ext01
mkdir -p /data_xfs01
mkdir -p /data_xfs02
mkdir -p /data_xfs04
mkdir -p /data_xfs52

mkdir -p /data_ext52_12
mkdir -p /data_xfs52_12

mkfs.ext4 /dev/datavg/ext02lv
mkfs.ext4 /dev/datavg/ext04lv
mkfs.ext4 /dev/datavg/ext52lv
mkfs.ext4 /dev/datavg/ext01lvssd
mkfs.xfs  /dev/datavg/xfs01lvssd
mkfs.xfs  /dev/datavg/xfs02lv
mkfs.xfs  /dev/datavg/xfs04lv
mkfs.xfs  /dev/datavg/xfs52lv

mkfs.ext4 /dev/datavg/ext52lv12
mkfs.xfs  /dev/datavg/xfs52lv12

mount /dev/datavg/ext02lv /data_ext02
mount /dev/datavg/ext04lv /data_ext04
mount /dev/datavg/ext52lv /data_ext52
mount /dev/datavg/ext01lvssd /data_ext01
mount /dev/datavg/xfs01lvssd /data_xfs01
mount /dev/datavg/xfs02lv /data_xfs02
mount /dev/datavg/xfs04lv /data_xfs04
mount /dev/datavg/xfs52lv /data_xfs52

mount /dev/datavg/ext52lv12 /data_ext52_12
mount /dev/datavg/xfs52lv12 /data_xfs52_12

dstat -d -D /dev/datavg/ext02lv,/dev/datavg/ext04lv,/dev/datavg/ext52lv,/dev/datavg/ext01lvssd,/dev/datavg/xfs01lvssd,/dev/datavg/xfs02lv,/dev/datavg/xfs04lv,/dev/datavg/xfs52lv,/dev/datavg/ext52lv12,/dev/datavg/xfs52lv12,/dev/sdaa
dstat -d -D /dev/datavg/ext02lv,/dev/datavg/ext04lv,/dev/datavg/ext52lv,/dev/datavg/ext01lvssd,/dev/datavg/xfs01lvssd,/dev/datavg/xfs02lv,/dev/datavg/xfs04lv,/dev/datavg/xfs52lv,/dev/datavg/ext52lv12,/dev/datavg/xfs52lv12,/dev/sdaa,/dev/sdb --disk-util
bmon -p bond0,enp*

# on worker1
rclone config
rclone lsd worker-2:
rclone sync /data_ssd/mnt/ worker-2:/data_ext01/mnt/ -P -L --transfers 64


# on worker-2

# fill data
# for 256M
var_basedir_ext=&quot;/data_ext04/mnt&quot;

mkdir -p $var_basedir_ext

# how may write concurrency
var_total_write=10
# how much size each file, this value is in MB
# 512M
var_size=512
# how much size to write totally, in TB
# write 3T
var_total_size=3

var_number=$(echo &quot;scale=0;$var_total_size*1024*1024/$var_size/$var_total_write&quot;|bc -l)
var_len=$(echo &quot;scale=0;$var_size*1024/1&quot;|bc -l)

for ((i=1; i&lt;=$var_number; i++)); do
  for ((j=1; j&lt;=$var_total_write; j++)); do
    head -c ${var_len}K &lt; /dev/urandom &gt; $var_basedir_ext/$var_size-$j-$i &amp;
  done
  echo &quot;wait to finish: $i&quot;
  wait
done



# fill data
# for 1G
var_basedir_ext=&quot;/data_ext04/mnt&quot;

mkdir -p $var_basedir_ext

# how may write concurrency
var_total_write=10
# how much size each file, this value is in MB
# 512M
var_size=1024
# how much size to write totally, in TB
# write 3T
var_total_size=3

var_number=$(echo &quot;scale=0;$var_total_size*1024*1024/$var_size/$var_total_write&quot;|bc -l)
var_len=$(echo &quot;scale=0;$var_size*1024/1&quot;|bc -l)

for ((i=1; i&lt;=$var_number; i++)); do
  for ((j=1; j&lt;=$var_total_write; j++)); do
    head -c ${var_len}K &lt; /dev/urandom &gt; $var_basedir_ext/$var_size-$j-$i &amp;
  done
  echo &quot;wait to finish: $i&quot;
  wait
done



# fill data
# for 2G
var_basedir_ext=&quot;/data_ext04/mnt&quot;

mkdir -p $var_basedir_ext

# how may write concurrency
var_total_write=10
# how much size each file, this value is in MB
# 512M
var_size=2048
# how much size to write totally, in TB
# write 3T
var_total_size=3

var_number=$(echo &quot;scale=0;$var_total_size*1024*1024/$var_size/$var_total_write&quot;|bc -l)
var_len=$(echo &quot;scale=0;$var_size*1024/1&quot;|bc -l)

for ((i=1; i&lt;=$var_number; i++)); do
  for ((j=1; j&lt;=$var_total_write; j++)); do
    head -c ${var_len}K &lt; /dev/urandom &gt; $var_basedir_ext/$var_size-$j-$i &amp;
  done
  echo &quot;wait to finish: $i&quot;
  wait
done


# copy data
rclone sync /data_ext01/mnt/ /data_xfs01/mnt/ -P -L --transfers 64
rclone sync /data_ext04/mnt/ /data_xfs02/mnt/ -P -L --transfers 64

rclone sync /data_ext04/mnt/ /data_xfs04/mnt/ -P -L --transfers 10
rclone sync /data_ext04/mnt/ /data_xfs52/mnt/ -P -L --transfers 10
rclone sync /data_ext04/mnt/ /data_xfs52_12/mnt/ -P -L --transfers 10

rclone sync /data_ext04/mnt/ /data_ext02/mnt/ -P -L --transfers 10
rclone sync /data_ext04/mnt/ /data_ext52/mnt/ -P -L --transfers 10
rclone sync /data_ext04/mnt/ /data_ext52_12/mnt/ -P -L --transfers 10




var_truebase=&quot;/data_xfs52&quot;
mkdir -p $var_truebase/list.tmp
cd $var_truebase/list.tmp

var_basedir=&quot;$var_truebase/mnt&quot;
find $var_basedir -type f -size -600M  &gt; list.512m
find $var_basedir -type f -size -1100M  -size +600M &gt; list.1g
find $var_basedir -type f -size +1100M &gt; list.+1g
find $var_basedir -type f &gt; list

cat list | xargs ls -l &gt; list.size
cat list.size | awk '{ n=int(log($5)/log(2));                         \
          if (n&lt;10) n=10;                                               \
          size[n]++ }                                                   \
      END { for (i in size) printf(&quot;%d %d\n&quot;, 2^i, size[i]) }'          \
 | sort -n                                                              \
 | awk 'function human(x) { x[1]/=1024;                                 \
                            if (x[1]&gt;=1024) { x[2]++;                   \
                                              human(x) } }              \
        { a[1]=$1;                                                      \
          a[2]=0;                                                       \
          human(a);                                                     \
          printf(&quot;%3d - %4d %s: %6d\n&quot;, a[1], a[1]*2,substr(&quot;kMGTEPYZ&quot;,a[2]+1,1),$2) }' 


# seperate read
for i in 512m 1g +1g ; do
  cat list.$i | shuf &gt; list.shuf.$i
done

rm -f split.list.*
# zte use 1800
var_total=30

for i in 512m 1g +1g ; do
  split -n l/$var_total list.shuf.$i split.list.$i.
done


for f in split.list.512m.*; do 
  cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
done

for f in split.list.1g.*; do 
  cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
done

for f in split.list.+1g.*; do 
  cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
done


# mix read
for i in 512m 1g +1g ; do
  cat list.$i | shuf &gt; list.shuf.$i
done

rm -f split.list.*
# zte use 1800
var_total=10

for i in 512m 1g +1g ; do
  split -n l/$var_total list.shuf.$i split.list.$i.
done

for i in 512m 1g +1g ; do
  for f in split.list.$i.*; do 
    cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
  done
done



ps -ef | grep xargs | grep DEMO | grep cat | awk '{print $2}' | xargs -I DEMO kill DEMO

ps -ef | grep cat | grep /data | awk '{print $2}' | xargs -I DEMO kill -9 DEMO

lvconvert --splitcache datavg/ext02lv



var_truebase=&quot;/data_ext01&quot;
mkdir -p $var_truebase/list.tmp
cd $var_truebase/list.tmp

var_basedir=&quot;$var_truebase/mnt&quot;
find $var_basedir -type f -size -16k  &gt; list.16k
find $var_basedir -type f -size -128k  -size +16k &gt; list.128k
find $var_basedir -type f -size +128k &gt; list.+128k
find $var_basedir -type f &gt; list

cat list | xargs ls -l &gt; list.size
cat list.size | awk '{ n=int(log($5)/log(2));                         \
          if (n&lt;10) n=10;                                               \
          size[n]++ }                                                   \
      END { for (i in size) printf(&quot;%d %d\n&quot;, 2^i, size[i]) }'          \
 | sort -n                                                              \
 | awk 'function human(x) { x[1]/=1024;                                 \
                            if (x[1]&gt;=1024) { x[2]++;                   \
                                              human(x) } }              \
        { a[1]=$1;                                                      \
          a[2]=0;                                                       \
          human(a);                                                     \
          printf(&quot;%3d - %4d %s: %6d\n&quot;, a[1], a[1]*2,substr(&quot;kMGTEPYZ&quot;,a[2]+1,1),$2) }' 


# seperate read
for i in 16k 128k +128k ; do
  cat list.$i | shuf &gt; list.shuf.$i
done

rm -f split.list.*
# zte use 1800
var_total=30

for i in 16k 128k +128k ; do
  split -n l/$var_total list.shuf.$i split.list.$i.
done


for f in split.list.16k.*; do 
  cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
done

for f in split.list.128k.*; do 
  cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
done

for f in split.list.+128k.*; do 
  cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
done


# mix read
for i in 16k 128k +128k ; do
  cat list.$i | shuf &gt; list.shuf.$i
done

rm -f split.list.*
# zte use 1800
var_total=10

for i in 16k 128k +128k ; do
  split -n l/$var_total list.shuf.$i split.list.$i.
done

for i in 16k 128k +128k ; do
  for f in split.list.$i.*; do 
    cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
  done
done

ps -ef | grep xargs | grep DEMO | grep cat | awk '{print $2}' | xargs -I DEMO kill DEMO



</code></pre>
<h3 id="worker-2-nic-bond"><a class="header" href="#worker-2-nic-bond">worker-2 nic bond</a></h3>
<pre><code class="language-bash">ip link show
# 2: eno1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
#     link/ether cc:64:a6:59:bb:80 brd ff:ff:ff:ff:ff:ff
# 3: ens2f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
#     link/ether 08:4f:0a:b5:a4:6e brd ff:ff:ff:ff:ff:ff
# 4: eno2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
#     link/ether cc:64:a6:59:bb:81 brd ff:ff:ff:ff:ff:ff
# 5: eno3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
#     link/ether cc:64:a6:59:bb:82 brd ff:ff:ff:ff:ff:ff
# 6: ens2f1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
#     link/ether 08:4f:0a:b5:a4:6f brd ff:ff:ff:ff:ff:ff
# 7: eno4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
#     link/ether cc:64:a6:59:bb:83 brd ff:ff:ff:ff:ff:ff

ip a s eno1
# 2: eno1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000
#     link/ether cc:64:a6:59:bb:80 brd ff:ff:ff:ff:ff:ff
#     inet 39.134.201.66/27 brd 39.134.201.95 scope global noprefixroute eno1
#        valid_lft forever preferred_lft forever
#     inet6 fe80::f690:1c45:b8c3:96d/64 scope link noprefixroute
#        valid_lft forever preferred_lft forever

ethtool eno1  # 10000baseT/Full
ethtool eno2  # 10000baseT/Full
ethtool eno3  # 1000baseT/Full
ethtool eno4  # 1000baseT/Full
ethtool ens2f0  #  10000baseT/Full
ethtool ens2f1  #  10000baseT/Full

nmcli con add type bond \
    con-name bond0 \
    ifname bond0 \
    mode 802.3ad 

nmcli con mod id bond0 bond.options \
    mode=802.3ad,miimon=100,lacp_rate=fast,xmit_hash_policy=layer2+3

nmcli con add type bond-slave ifname eno2 con-name eno2 master bond0
nmcli con add type bond-slave ifname ens2f0 con-name ens2f0 master bond0
nmcli con add type bond-slave ifname ens2f1 con-name ens2f1 master bond0

nmcli con down eno2
nmcli con up eno2
nmcli con down ens2f0
nmcli con up ens2f0
nmcli con down ens2f1
nmcli con up ens2f1
nmcli con down bond0
nmcli con start bond0     


#######################################
# nic bond
cat &gt; /root/nic.bond.sh &lt;&lt; 'EOF'
#!/bin/bash

set -x 

# delete all connection 
nmcli -g uuid con | while read i ; do nmcli c delete  ${i} ; done 

nmcli con add type bond \
    con-name bond0 \
    ifname bond0 \
    mode 802.3ad \
    ipv4.method 'manual' \
    ipv4.address '39.134.201.66/27' \
    ipv4.gateway '39.134.201.94' \
    ipv4.dns '117.177.241.16'
    
nmcli con mod id bond0 bond.options \
    mode=802.3ad,miimon=100,lacp_rate=fast,xmit_hash_policy=layer2+3

nmcli con add type bond-slave ifname eno1 con-name eno1 master bond0    
nmcli con add type bond-slave ifname eno2 con-name eno2 master bond0
nmcli con add type bond-slave ifname ens2f0 con-name ens2f0 master bond0
nmcli con add type bond-slave ifname ens2f1 con-name ens2f1 master bond0

systemctl restart network

EOF

cat &gt; /root/nic.restore.sh &lt;&lt; 'EOF'
#!/bin/bash

set -x 

# delete all connection 
nmcli -g uuid con | while read i ; do nmcli c delete  ${i} ; done 

# re-create primary connection 
nmcli con add type ethernet \
    con-name eno1 \
    ifname eno1 \
    ipv4.method 'manual' \
    ipv4.address '39.134.201.66/27' \
    ipv4.gateway '39.134.201.94' \
    ipv4.dns '117.177.241.16'

systemctl restart network

exit 0
EOF

chmod +x /root/nic.restore.sh

cat &gt; ~/cron-network-con-recreate &lt;&lt; EOF
*/20 * * * * /bin/bash /root/nic.restore.sh
EOF

crontab ~/cron-network-con-recreate

bash /root/nic.bond.sh


</code></pre>
<h3 id="worker-3-host"><a class="header" href="#worker-3-host">worker-3 host</a></h3>
<pre><code class="language-bash">
systemctl stop firewalld
systemctl disable firewalld

cat &lt;&lt; EOF &gt; /etc/rc.local
#!/bin/bash
# THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES
#
# It is highly advisable to create own systemd services or udev rules
# to run scripts during boot instead of using this file.
#
# In contrast to previous versions due to parallel execution during boot
# this script will NOT be run after all other services.
#
# Please note that you must run 'chmod +x /etc/rc.d/rc.local' to ensure
# that this script will be executed during boot.

touch /var/lock/subsys/local

ipset create my-allow-set hash:net
ipset add my-allow-set 127.0.0.1/32
ipset add my-allow-set 223.87.20.0/24
ipset add my-allow-set 117.177.241.0/24
ipset add my-allow-set 39.134.200.0/24
ipset add my-allow-set 39.134.201.0/24
ipset add my-allow-set 39.137.101.0/24
ipset add my-allow-set 192.168.7.0/24
ipset add my-allow-set 112.44.102.224/27
ipset add my-allow-set 47.93.86.113/32
ipset add my-allow-set 221.226.0.75/32
ipset add my-allow-set 210.21.236.182/32
ipset add my-allow-set 61.132.54.2/32

ipset add my-allow-set 39.134.198.0/24

ipset add my-allow-set 39.134.204.0/24

iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
iptables -A INPUT -m set --match-set my-allow-set src -j ACCEPT
iptables -A INPUT -p tcp -j REJECT
iptables -A INPUT -p udp -j REJECT

EOF

chmod +x /etc/rc.d/rc.local
systemctl enable rc-local

# systemctl restart rc-local


#######################################
# nic bond
cat &lt;&lt; 'EOF' &gt; /root/nic.bond.sh
#!/bin/bash

# delete all connection 
nmcli -g uuid con | while read i ; do nmcli c delete uuid ${i} ; done 

nmcli con add type bond \
    con-name bond0 \
    ifname bond0 \
    mode 802.3ad \
    ipv4.method 'manual' \
    ipv4.address '39.134.204.73/27' \
    ipv4.gateway '39.134.204.65' \
    ipv4.dns '117.177.241.16'
    
nmcli con mod id bond0 bond.options \
    mode=802.3ad,miimon=100,lacp_rate=fast,xmit_hash_policy=layer2+3
    
nmcli con add type bond-slave ifname enp176s0f0 con-name enp176s0f0 master bond0
nmcli con add type bond-slave ifname enp176s0f1 con-name enp176s0f1 master bond0

systemctl restart network

EOF

cat &gt; /root/nic.restore.sh &lt;&lt; 'EOF'
#!/bin/bash

# delete all connection 
nmcli -g uuid con | while read i ; do nmcli c delete uuid ${i} ; done 

# re-create primary connection 
nmcli con add type ethernet \
    con-name enp176s0f0 \
    ifname enp176s0f0 \
    ipv4.method 'manual' \
    ipv4.address '39.134.204.73/27' \
    ipv4.gateway '39.134.204.65' \
    ipv4.dns '117.177.241.16'

systemctl restart network

exit 0
EOF

chmod +x /root/nic.restore.sh

cat &gt; ~/cron-network-con-recreate &lt;&lt; EOF
*/2 * * * * /bin/bash /root/nic.restore.sh
EOF

crontab ~/cron-network-con-recreate



mkdir /etc/yum.repos.d.bak
mv /etc/yum.repos.d/* /etc/yum.repos.d.bak

cat &lt;&lt; EOF &gt; /etc/yum.repos.d/remote.repo
[remote]
name=RHEL FTP
baseurl=ftp://117.177.241.16/data
enabled=1
gpgcheck=0

EOF

yum clean all
yum --disableplugin=subscription-manager  repolist

yum -y update

hostnamectl set-hostname worker-3.ocpsc.redhat.ren

nmcli connection modify enp176s0f0 ipv4.dns 117.177.241.16
nmcli connection reload
nmcli connection up enp176s0f0



# ntp
yum install -y chrony
systemctl enable chronyd
systemctl restart chronyd
systemctl status chronyd
chronyc tracking

systemctl disable --now firewalld.service

# update ntp
cat &lt;&lt; EOF &gt; /etc/chrony.conf
server 223.87.20.100 iburst
driftfile /var/lib/chrony/drift
makestep 1.0 3
rtcsync
logdir /var/log/chrony
EOF

systemctl restart chronyd
systemctl status chronyd
chronyc tracking





</code></pre>
<h3 id="worker-3-disk"><a class="header" href="#worker-3-disk">worker-3 disk</a></h3>
<pre><code class="language-bash">lshw -class disk

lsblk | grep 5.5 | awk '{print $1}' | xargs -I DEMO echo -n &quot;/dev/DEMO &quot;
# /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx /dev/sdy
lsblk | grep 5.5 | awk '{print $1}' | wc -l
# 24

pvcreate -y /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx /dev/sdy

vgcreate datavg /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx /dev/sdy

lsblk -d -o name,rota

lvcreate --type raid0 -L 120T  --stripesize 128k --stripes 24 -n hddlv datavg /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx /dev/sdy


mkfs.ext4 /dev/datavg/hddlv



lvcreate --type raid0 -L 5T  --stripesize 512k --stripes 24 -n xfslv datavg /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx /dev/sdy

lvcreate --type raid0 -L 110T  --stripesize 4096k --stripes 24 -n extzxlv datavg /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx /dev/sdy

lvcreate --type raid0 -L 3.5T  --stripesize 4096k --stripes 24 -n ext04lv datavg /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx /dev/sdy

lvcreate --type raid6 -L 3.5T  --stripesize 2048k --stripes 22 -n ext62lv datavg /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx /dev/sdy

lvcreate --type raid5 -L 3.5T  --stripesize 2048k --stripes 23 -n ext52lv datavg /dev/sdb /dev/sdc /dev/sdd /dev/sde /dev/sdf /dev/sdg /dev/sdh /dev/sdi /dev/sdj /dev/sdk /dev/sdl /dev/sdm /dev/sdn /dev/sdo /dev/sdp /dev/sdq /dev/sdr /dev/sds /dev/sdt /dev/sdu /dev/sdv /dev/sdw /dev/sdx /dev/sdy



mkfs.ext4 -E lazy_itable_init=0,lazy_journal_init=0 /dev/mapper/fc-root

mkfs.xfs /dev/datavg/xfslv
mkfs.ext4 /dev/datavg/extlv



mkfs.ext4 /dev/datavg/ext04lv
mkfs.ext4 /dev/datavg/ext62lv

mkfs.ext4 /dev/datavg/ext52lv

mkfs.ext4 /dev/datavg/extzxlv
# mkfs.xfs /dev/datavg/extzxlv
mount /dev/datavg/extzxlv /data
rclone sync /data_ext04/mnt/ /data/redhat_mnt/  -P -L --transfers 64

mount /dev/datavg/xfslv /data_xfs
mount /dev/datavg/extlv /data_ext

mkdir -p /data_ext02
mkdir -p /data_ext04
mkdir -p /data_ext62
mkdir -p /data_ext52

mount /dev/datavg/ext02lv /data_ext02
mount /dev/datavg/ext04lv /data_ext04
# mount /dev/datavg/ext62lv /data_ext62
mount /dev/datavg/ext52lv /data_ext52

umount /data_xfs
lvremove -f datavg/xfslv
# rsync --info=progress2 -P -ar  /data_ext/mnt/ /data_xfs/mnt/
rclone sync /data_ext/mnt/ /data_xfs/mnt/ -P -L --transfers 64

umount /data_ext
lvremove -f datavg/extlv
rclone sync /data_xfs/mnt/ /data_ext/mnt/ -P -L --transfers 64

umount /data_ext52
rclone sync /data_xfs/mnt/ /data_ext04/mnt/ -P -L --transfers 64
rclone sync /data_xfs/mnt/ /data_ext62/mnt/ -P -L --transfers 64
rclone sync /data_xfs/mnt/ /data_ext52/mnt/ -P -L --transfers 64

lvs -o+stripesize

dstat -D /dev/datavg/xfslv,/dev/datavg/extlv,/dev/sdb,/dev/sdc 5
dstat -D /dev/datavg/xfslv,/dev/datavg/extlv,/dev/sdb,/dev/sdc --disk-util
bmon -p bond0,enp*

blockdev --report 
# https://access.redhat.com/solutions/3588841
# orig: 12288
/sbin/blockdev --setra 131072 /dev/datavg/xfslv
/sbin/blockdev --setra 131072 /dev/datavg/extlv

/sbin/blockdev --setra 12288 /dev/datavg/xfslv
/sbin/blockdev --setra 12288 /dev/datavg/extlv


mkdir -p /data/

cat /etc/fstab

cat &lt;&lt; EOF &gt;&gt; /etc/fstab
/dev/datavg/hddlv /data                  ext4     defaults        0 0
EOF

mount -a
df -h | grep \/data

while true; do df -h | grep /data; sleep 10; done

dstat -D /dev/datavg/hddlv 
dstat -D /dev/sdb,/dev/sdc
dstat -D /dev/sdb,/dev/sdc --disk-util

mkfs.xfs -f /dev/sdb
mkfs.ext4 -F /dev/sdc

mkdir -p /data_xfs
mkdir -p /data_ext

mount /dev/sdb /data_xfs
mount /dev/sdc /data_ext


# fill data
# for 1.5M
var_basedir_xfs=&quot;/data_xfs/mnt&quot;
var_basedir_ext=&quot;/data_ext/mnt&quot;

mkdir -p $var_basedir_xfs
mkdir -p $var_basedir_ext


var_basedir_xfs=&quot;/data_xfs/mnt&quot;
var_basedir_ext=&quot;/data_ext/mnt&quot;
var_total=10
# 512k
var_size=0.5
# write 1T
var_number=$(echo &quot;scale=0;1024*1024/$var_size/$var_total&quot;|bc -l)
var_len=$(echo &quot;scale=0;$var_size*1024/1&quot;|bc -l)

for ((i=1; i&lt;=$var_number; i++)); do
  for ((j=1; j&lt;=$var_total; j++)); do
    # echo &quot;Welcome $i times&quot;
    head -c ${var_len}K &lt; /dev/urandom &gt; $var_basedir_xfs/$var_size-$j-$i &amp;
    head -c ${var_len}K &lt; /dev/urandom &gt; $var_basedir_ext/$var_size-$j-$i &amp;
  done
  echo &quot;wait to finish: $i&quot;
  wait
done

var_basedir_xfs=&quot;/data_xfs/mnt&quot;
var_basedir_ext=&quot;/data_ext/mnt&quot;
var_total=10
# 4M
var_size=4
# write 1T
var_number=$(echo &quot;scale=0;1024*1024/$var_size/$var_total&quot;|bc -l)
var_len=$(echo &quot;scale=0;$var_size*1024/1&quot;|bc -l)

for ((i=1; i&lt;=$var_number; i++)); do
  for ((j=1; j&lt;=$var_total; j++)); do
    # echo &quot;Welcome $i times&quot;
    head -c ${var_len}K &lt; /dev/urandom &gt; $var_basedir_xfs/$var_size-$j-$i &amp;
    head -c ${var_len}K &lt; /dev/urandom &gt; $var_basedir_ext/$var_size-$j-$i &amp;
  done
  echo &quot;wait to finish: $i&quot;
  wait
done


var_basedir_xfs=&quot;/data_xfs/mnt&quot;
var_basedir_ext=&quot;/data_ext/mnt&quot;
var_total=10
# 8M
var_size=8
# write 1T
var_number=$(echo &quot;scale=0;1024*1024/$var_size/$var_total&quot;|bc -l)
var_len=$(echo &quot;scale=0;$var_size*1024/1&quot;|bc -l)

for ((i=1; i&lt;=$var_number; i++)); do
  for ((j=1; j&lt;=$var_total; j++)); do
    # echo &quot;Welcome $i times&quot;
    head -c ${var_len}K &lt; /dev/urandom &gt; $var_basedir_xfs/$var_size-$j-$i &amp;
    head -c ${var_len}K &lt; /dev/urandom &gt; $var_basedir_ext/$var_size-$j-$i &amp;
  done
  echo &quot;wait to finish: $i&quot;
  wait
done

var_basedir_xfs=&quot;/data_xfs/mnt&quot;
var_basedir_ext=&quot;/data_ext/mnt&quot;
var_total=10
# 32M
var_size=32
# write 1T
var_number=$(echo &quot;scale=0;1024*1024/$var_size/$var_total&quot;|bc -l)
var_len=$(echo &quot;scale=0;$var_size*1024/1&quot;|bc -l)

for ((i=1; i&lt;=$var_number; i++)); do
  for ((j=1; j&lt;=$var_total; j++)); do
    # echo &quot;Welcome $i times&quot;
    head -c ${var_len}K &lt; /dev/urandom &gt; $var_basedir_xfs/$var_size-$j-$i &amp;
    head -c ${var_len}K &lt; /dev/urandom &gt; $var_basedir_ext/$var_size-$j-$i &amp;
  done
  echo &quot;wait to finish: $i&quot;
  wait
done

var_basedir_xfs=&quot;/data_xfs/mnt&quot;
var_basedir_ext=&quot;/data_ext/mnt&quot;
var_total=10
# 64M
var_size=64
# write 1T
var_number=$(echo &quot;scale=0;1024*1024/$var_size/$var_total&quot;|bc -l)
var_len=$(echo &quot;scale=0;$var_size*1024/1&quot;|bc -l)

for ((i=1; i&lt;=$var_number; i++)); do
  for ((j=1; j&lt;=$var_total; j++)); do
    # echo &quot;Welcome $i times&quot;
    head -c ${var_len}K &lt; /dev/urandom &gt; $var_basedir_xfs/$var_size-$j-$i &amp;
    head -c ${var_len}K &lt; /dev/urandom &gt; $var_basedir_ext/$var_size-$j-$i &amp;
  done
  echo &quot;wait to finish: $i&quot;
  wait
done

mkdir -p /data_xfs/list.tmp
cd /data_xfs/list.tmp
var_basedir=&quot;/data_xfs/mnt&quot;
find $var_basedir -type f -size -2M  &gt; list.2m
find $var_basedir -type f -size -10M  -size +2M &gt; list.10m
find $var_basedir -type f -size +10M &gt; list.100m
find $var_basedir -type f &gt; list


var_truebase=&quot;/data&quot;
mkdir -p $var_truebase/list.tmp
cd $var_truebase/list.tmp

var_basedir=&quot;$var_truebase/mnt&quot;
find $var_basedir -type f -size -2M  &gt; list.2m
find $var_basedir -type f -size -10M  -size +2M &gt; list.10m
find $var_basedir -type f -size +10M &gt; list.100m
find $var_basedir -type f &gt; list

cat list | xargs ls -l &gt; list.size
cat list.size | awk '{ n=int(log($5)/log(2));                         \
          if (n&lt;10) n=10;                                              \
          size[n]++ }                                                   \
      END { for (i in size) printf(&quot;%d %d\n&quot;, 2^i, size[i]) }'          \
 | sort -n                                                              \
 | awk 'function human(x) { x[1]/=1024;                                 \
                            if (x[1]&gt;=1024) { x[2]++;                   \
                                              human(x) } }              \
        { a[1]=$1;                                                      \
          a[2]=0;                                                       \
          human(a);                                                     \
          printf(&quot;%3d - %4d %s: %6d\n&quot;, a[1], a[1]*2,substr(&quot;kMGTEPYZ&quot;,a[2]+1,1),$2) }' 





cat list | shuf &gt; list.shuf.all

cat list.2m | shuf &gt; list.shuf.2m
cat list.10m | shuf &gt; list.shuf.10m
cat list.100m | shuf &gt; list.shuf.100m
cat list.10m list.100m | shuf &gt; list.shuf.+2m

rm -f split.list.*
# zte use 1800
var_total=10
split -n l/$var_total list.shuf.all split.list.all.
split -n l/$var_total list.shuf.2m split.list.2m.
split -n l/$var_total list.shuf.10m split.list.10m.
split -n l/$var_total list.shuf.100m split.list.100m.
split -n l/$var_total list.shuf.+2m split.list.+2m.

for f in split.list.2m.*; do 
    cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
done
# for f in split.list.+2m.*; do 
#     cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
# done

for f in split.list.10m.*; do 
    cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
done
for f in split.list.100m.*; do 
    cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
done

for f in split.list.all.*; do 
    cat $f | xargs -I DEMO cat DEMO &gt; /dev/null &amp;
done

jobs -p | xargs kill

ps -ef | grep xargs | grep DEMO | grep cat | awk '{print $2}' | xargs -I DEMO kill DEMO



</code></pre>
<h2 id="install-ocp"><a class="header" href="#install-ocp">install ocp</a></h2>
<h3 id="helper-node-day1"><a class="header" href="#helper-node-day1">helper node day1</a></h3>
<pre><code class="language-bash">############################################################
# on macbook
mkdir -p /Users/wzh/Documents/redhat/tools/redhat.ren/etc
mkdir -p /Users/wzh/Documents/redhat/tools/redhat.ren/lib
mkdir -p /Users/wzh/Documents/redhat/tools/ocpsc.redhat.ren/etc
mkdir -p /Users/wzh/Documents/redhat/tools/ocpsc.redhat.ren/lib
rm -rf /Users/wzh/Documents/redhat/tools/apps.ocpsc.redhat.ren/
mkdir -p /Users/wzh/Documents/redhat/tools/apps.ocpsc.redhat.ren/etc
mkdir -p /Users/wzh/Documents/redhat/tools/apps.ocpsc.redhat.ren/lib

cd /Users/wzh/Documents/redhat/tools/redhat.ren/
docker run -it --rm --name certbot \
            -v &quot;/Users/wzh/Documents/redhat/tools/redhat.ren/etc:/etc/letsencrypt&quot; \
            -v &quot;/Users/wzh/Documents/redhat/tools/redhat.ren/lib:/var/lib/letsencrypt&quot; \
            certbot/certbot certonly  -d &quot;*.redhat.ren&quot; --manual --preferred-challenges dns-01  --server https://acme-v02.api.letsencrypt.org/directory

cp ./etc/archive/redhat.ren/fullchain4.pem redhat.ren.crt
cp ./etc/archive/redhat.ren/privkey4.pem redhat.ren.key

cd /Users/wzh/Documents/redhat/tools/ocpsc.redhat.ren/
docker run -it --rm --name certbot \
            -v &quot;/Users/wzh/Documents/redhat/tools/ocpsc.redhat.ren/etc:/etc/letsencrypt&quot; \
            -v &quot;/Users/wzh/Documents/redhat/tools/ocpsc.redhat.ren/lib:/var/lib/letsencrypt&quot; \
            certbot/certbot certonly  -d &quot;*.ocpsc.redhat.ren&quot; --manual --preferred-challenges dns-01  --server https://acme-v02.api.letsencrypt.org/directory

cp ./etc/archive/ocpsc.redhat.ren/fullchain1.pem ocpsc.redhat.ren.crt
cp ./etc/archive/ocpsc.redhat.ren/privkey1.pem ocpsc.redhat.ren.key


cd /Users/wzh/Documents/redhat/tools/apps.ocpsc.redhat.ren/
docker run -it --rm --name certbot \
            -v &quot;/Users/wzh/Documents/redhat/tools/apps.ocpsc.redhat.ren/etc:/etc/letsencrypt&quot; \
            -v &quot;/Users/wzh/Documents/redhat/tools/apps.ocpsc.redhat.ren/lib:/var/lib/letsencrypt&quot; \
            certbot/certbot certonly  -d &quot;*.apps.ocpsc.redhat.ren&quot; --manual --preferred-challenges dns-01  --server https://acme-v02.api.letsencrypt.org/directory

cp ./etc/archive/apps.ocpsc.redhat.ren/fullchain1.pem apps.ocpsc.redhat.ren.crt
cp ./etc/archive/apps.ocpsc.redhat.ren/privkey1.pem apps.ocpsc.redhat.ren.key

# scp these keys to helper
# /data/cert/*

####################################################
# on helper node
yum -y install podman docker-distribution pigz skopeo httpd-tools

# https://access.redhat.com/solutions/3175391
htpasswd -cbB /etc/docker-distribution/registry_passwd admin ***************

cat &lt;&lt; EOF &gt; /etc/docker-distribution/registry/config.yml
version: 0.1
log:
  fields:
    service: registry
storage:
    cache:
        layerinfo: inmemory
    filesystem:
        rootdirectory: /data/registry
    delete:
        enabled: true
http:
    addr: :5443
    tls:
       certificate: /data/cert/redhat.ren.crt
       key: /data/cert/redhat.ren.key
auth:
  htpasswd:
    realm: basic‑realm
    path: /etc/docker-distribution/registry_passwd
EOF
# systemctl restart docker
systemctl stop docker-distribution
systemctl enable docker-distribution
systemctl restart docker-distribution
# 

firewall-cmd --permanent --add-port=5443/tcp
firewall-cmd --reload

podman login registry.redhat.ren:5443 -u admin -p *******************

yum install -y docker
systemctl start docker
docker login registry.redhat.ren:5443 -u admin

# upload vars-static.yaml to helper
yum -y install ansible-2.8.10 git unzip podman python36

cd /data/ocp4/ocp4-upi-helpernode
ansible-playbook -e @vars-static.yaml -e staticips=true tasks/main.yml

# upload install-config.yaml to helper /data/ocp4
cd /data/ocp4

/bin/rm -rf *.ign .openshift_install_state.json auth bootstrap master0 master1 master2 worker0 worker1 worker2

openshift-install create ignition-configs --dir=/data/ocp4

/bin/cp -f bootstrap.ign /var/www/html/ignition/bootstrap-static.ign
/bin/cp -f master.ign /var/www/html/ignition/master-0.ign
/bin/cp -f master.ign /var/www/html/ignition/master-1.ign
/bin/cp -f master.ign /var/www/html/ignition/master-2.ign
/bin/cp -f worker.ign /var/www/html/ignition/worker-0.ign
/bin/cp -f worker.ign /var/www/html/ignition/worker-1.ign
/bin/cp -f worker.ign /var/www/html/ignition/worker-2.ign

chmod 644 /var/www/html/ignition/*

########################################################
# on helper node, create iso
yum -y install genisoimage libguestfs-tools
systemctl start libvirtd

export NGINX_DIRECTORY=/data/ocp4
export RHCOSVERSION=4.3.0
export VOLID=$(isoinfo -d -i ${NGINX_DIRECTORY}/rhcos-${RHCOSVERSION}-x86_64-installer.iso | awk '/Volume id/ { print $3 }')
TEMPDIR=$(mktemp -d)
echo $VOLID
echo $TEMPDIR

cd ${TEMPDIR}
# Extract the ISO content using guestfish (to avoid sudo mount)
guestfish -a ${NGINX_DIRECTORY}/rhcos-${RHCOSVERSION}-x86_64-installer.iso \
  -m /dev/sda tar-out / - | tar xvf -

# Helper function to modify the config files
modify_cfg(){
  for file in &quot;EFI/redhat/grub.cfg&quot; &quot;isolinux/isolinux.cfg&quot;; do
    # Append the proper image and ignition urls
    sed -e '/coreos.inst=yes/s|$| coreos.inst.install_dev=vda coreos.inst.image_url='&quot;${URL}&quot;'\/install\/'&quot;${BIOSMODE}&quot;'.raw.gz coreos.inst.ignition_url='&quot;${URL}&quot;'\/ignition\/'&quot;${NODE}&quot;'.ign ip='&quot;${IP}&quot;'::'&quot;${GATEWAY}&quot;':'&quot;${NETMASK}&quot;':'&quot;${FQDN}&quot;':'&quot;${NET_INTERFACE}&quot;':none:'&quot;${DNS}&quot;' nameserver='&quot;${DNS}&quot;'|' ${file} &gt; $(pwd)/${NODE}_${file##*/}
    # Boot directly in the installation
    sed -i -e 's/default vesamenu.c32/default linux/g' -e 's/timeout 600/timeout 10/g' $(pwd)/${NODE}_${file##*/}
  done
}

URL=&quot;http://117.177.241.16:8080/&quot;
GATEWAY=&quot;117.177.241.1&quot;
NETMASK=&quot;255.255.255.0&quot;
DNS=&quot;117.177.241.16&quot;

# BOOTSTRAP
# TYPE=&quot;bootstrap&quot;
NODE=&quot;bootstrap-static&quot;
IP=&quot;117.177.241.243&quot;
FQDN=&quot;vm-bootstrap&quot;
BIOSMODE=&quot;bios&quot;
NET_INTERFACE=&quot;ens3&quot;
modify_cfg

# MASTERS
# TYPE=&quot;master&quot;
# MASTER-0
NODE=&quot;master-0&quot;
IP=&quot;117.177.241.240&quot;
FQDN=&quot;vm-master0&quot;
BIOSMODE=&quot;bios&quot;
NET_INTERFACE=&quot;ens3&quot;
modify_cfg

# MASTER-1
NODE=&quot;master-1&quot;
IP=&quot;117.177.241.241&quot;
FQDN=&quot;vm-master1&quot;
BIOSMODE=&quot;bios&quot;
NET_INTERFACE=&quot;ens3&quot;
modify_cfg

# MASTER-2
NODE=&quot;master-2&quot;
IP=&quot;117.177.241.242&quot;
FQDN=&quot;vm-master2&quot;
BIOSMODE=&quot;bios&quot;
NET_INTERFACE=&quot;ens3&quot;
modify_cfg

# WORKERS
NODE=&quot;worker-0&quot;
IP=&quot;117.177.241.244&quot;
FQDN=&quot;vm-worker0&quot;
BIOSMODE=&quot;bios&quot;
NET_INTERFACE=&quot;ens3&quot;
modify_cfg

NODE=&quot;worker-1&quot;
IP=&quot;117.177.241.245&quot;
FQDN=&quot;vm-worker1&quot;
BIOSMODE=&quot;bios&quot;
NET_INTERFACE=&quot;ens3&quot;
modify_cfg


# Generate the images, one per node as the IP configuration is different...
# https://github.com/coreos/coreos-assembler/blob/master/src/cmd-buildextend-installer#L97-L103
for node in master-0 master-1 master-2 worker-0 worker-1 worker-2 bootstrap-static; do
  # Overwrite the grub.cfg and isolinux.cfg files for each node type
  for file in &quot;EFI/redhat/grub.cfg&quot; &quot;isolinux/isolinux.cfg&quot;; do
    /bin/cp -f $(pwd)/${node}_${file##*/} ${file}
  done
  # As regular user!
  genisoimage -verbose -rock -J -joliet-long -volset ${VOLID} \
    -eltorito-boot isolinux/isolinux.bin -eltorito-catalog isolinux/boot.cat \
    -no-emul-boot -boot-load-size 4 -boot-info-table \
    -eltorito-alt-boot -efi-boot images/efiboot.img -no-emul-boot \
    -o ${NGINX_DIRECTORY}/${node}.iso .
done

# Optionally, clean up
cd /data/ocp4
rm -Rf ${TEMPDIR}

cd ${NGINX_DIRECTORY}

# mkdir -p /data/ocp4
# mkdir -p /data/kvm
scp master-*.iso root@117.177.241.17:/data/ocp4/

scp master-*.iso root@117.177.241.21:/data/ocp4/
scp worker-*.iso root@117.177.241.21:/data/ocp4/
scp bootstrap-*.iso root@117.177.241.21:/data/ocp4/

scp master-*.iso root@117.177.241.18:/data/ocp4/

# after you create and boot master vm, worker vm, you can track the result
export KUBECONFIG=/data/ocp4/auth/kubeconfig
echo &quot;export KUBECONFIG=/data/ocp4/auth/kubeconfig&quot; &gt;&gt; ~/.bashrc
source ~/.bashrc
oc get nodes

openshift-install wait-for bootstrap-complete --log-level debug

oc get csr

openshift-install wait-for install-complete

bash add.image.load.sh /data_ssd/is.samples/mirror_dir/

oc apply -f ./99-worker-zzz-container-registries.yaml -n openshift-config
oc apply -f ./99-master-zzz-container-registries.yaml -n openshift-config

</code></pre>
<h3 id="helper-node-day1-oper"><a class="header" href="#helper-node-day1-oper">helper node day1 oper</a></h3>
<pre><code class="language-bash">
# https://docs.openshift.com/container-platform/4.3/openshift_images/managing_images/using-image-pull-secrets.html#images-update-global-pull-secret_using-image-pull-secrets
oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=/data/pull-secret.json

# https://docs.openshift.com/container-platform/4.3/networking/ingress-operator.html#nw-ingress-controller-tls-profiles_configuring-ingress
oc --namespace openshift-ingress-operator get ingresscontrollers

oc --namespace openshift-ingress create secret tls custom-certs-default --cert=/data/cert/apps.ocpsc.redhat.ren.crt --key=/data/cert/apps.ocpsc.redhat.ren.key

oc patch --type=merge --namespace openshift-ingress-operator ingresscontrollers/default \
  --patch '{&quot;spec&quot;:{&quot;defaultCertificate&quot;:{&quot;name&quot;:&quot;custom-certs-default&quot;}}}'

oc get --namespace openshift-ingress-operator ingresscontrollers/default \
  --output jsonpath='{.spec.defaultCertificate}'

# upgrade ingress ca
oc --namespace openshift-ingress create secret tls custom-certs-default-01 --cert=/data/cert/apps.ocpsc.redhat.ren.crt --key=/data/cert/apps.ocpsc.redhat.ren.key

oc patch --type=merge --namespace openshift-ingress-operator ingresscontrollers/default \
  --patch '{&quot;spec&quot;:{&quot;defaultCertificate&quot;:{&quot;name&quot;:&quot;custom-certs-default-01&quot;}}}'

##################################################3
# add rhel hw node, and remove vm worker node
ssh-copy-id root@infra-0.ocpsc.redhat.ren
ssh root@infra-0.ocpsc.redhat.ren

ssh-copy-id root@infra-1.ocpsc.redhat.ren
ssh root@infra-1.ocpsc.redhat.ren

# disable firewalld on infra-0, infra-1

yum -y install openshift-ansible openshift-clients jq

# create rhel-ansible-host
cat &lt;&lt;EOF &gt; /data/ocp4/rhel-ansible-host
[all:vars]
ansible_user=root 
#ansible_become=True 

openshift_kubeconfig_path=&quot;/data/ocp4/auth/kubeconfig&quot; 

[new_workers] 
infra-0.ocpsc.redhat.ren
infra-1.ocpsc.redhat.ren

EOF

ansible-playbook -i /data/ocp4/rhel-ansible-host /usr/share/ansible/openshift-ansible/playbooks/scaleup.yml

# then remove old vm-worker0, vm-worker1
oc get nodes -o wide
oc adm cordon vm-worker-0.ocpsc.redhat.ren
oc adm cordon vm-worker-1.ocpsc.redhat.ren
oc adm drain vm-worker-0.ocpsc.redhat.ren --force --delete-local-data --ignore-daemonsets
oc adm drain vm-worker-1.ocpsc.redhat.ren --force --delete-local-data --ignore-daemonsets  
oc delete nodes vm-worker-0.ocpsc.redhat.ren
oc delete nodes vm-worker-1.ocpsc.redhat.ren
oc get nodes -o wide

# create nfs storage and enable image operator
bash ocp4-upi-helpernode/files/nfs-provisioner-setup.sh

oc patch configs.imageregistry.operator.openshift.io cluster -p '{&quot;spec&quot;:{&quot;managementState&quot;: &quot;Managed&quot;,&quot;storage&quot;:{&quot;pvc&quot;:{&quot;claim&quot;:&quot;&quot;}}}}' --type=merge

# create operator catalog
oc patch OperatorHub cluster --type json \
    -p '[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/disableAllDefaultSources&quot;, &quot;value&quot;: true}]'

cat &lt;&lt;EOF &gt; redhat-operator-catalog.yaml
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: redhat-operator-catalog
  namespace: openshift-marketplace
spec:
  displayName: Redhat Operator Catalog
  sourceType: grpc
  image: registry.redhat.ren:5443/docker.io/wangzheng422/operator-catalog:redhat-2020-03-23
  publisher: Red Hat
EOF
oc create -f redhat-operator-catalog.yaml

# create infra node
# https://access.redhat.com/solutions/4287111
oc get node

oc label node infra0.hsc.redhat.ren node-role.kubernetes.io/infra=&quot;&quot;
oc label node infra1.hsc.redhat.ren node-role.kubernetes.io/infra=&quot;&quot;

oc patch ingresscontroller default -n openshift-ingress-operator --type=merge --patch='{&quot;spec&quot;:{&quot;nodePlacement&quot;:{&quot;nodeSelector&quot;: {&quot;matchLabels&quot;:{&quot;node-role.kubernetes.io/infra&quot;:&quot;&quot;}}}}}'

oc patch configs.imageregistry.operator.openshift.io/cluster -n openshift-image-registry --type=merge --patch '{&quot;spec&quot;:{&quot;nodeSelector&quot;:{&quot;node-role.kubernetes.io/infra&quot;:&quot;&quot;}}}'

oc get pod -o wide -n openshift-image-registry --sort-by=&quot;.spec.nodeName&quot;

cat &lt;&lt;EOF &gt; /data/ocp4/monitoring-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |+
    alertmanagerMain:
      nodeSelector:
        node-role.kubernetes.io/infra: &quot;&quot;
    prometheusK8s:
      nodeSelector:
        node-role.kubernetes.io/infra: &quot;&quot;
      volumeClaimTemplate:
        metadata:
          name: localpvc
        spec:
          storageClassName: local-sc
          resources:
            requests:
              storage: 400Gi
    prometheusOperator:
      nodeSelector:
        node-role.kubernetes.io/infra: &quot;&quot;
    grafana:
      nodeSelector:
        node-role.kubernetes.io/infra: &quot;&quot;
    k8sPrometheusAdapter:
      nodeSelector:
        node-role.kubernetes.io/infra: &quot;&quot;
    kubeStateMetrics:
      nodeSelector:
        node-role.kubernetes.io/infra: &quot;&quot;
    telemeterClient:
      nodeSelector:
        node-role.kubernetes.io/infra: &quot;&quot;
EOF

oc apply -f /data/ocp4/monitoring-cm.yaml -n openshift-monitoring

oc get pods -n openshift-monitoring -o wide --sort-by=&quot;.spec.nodeName&quot;

###########################################
## add user for zte
cd /data/ocp4
touch /data/ocp4/htpasswd
htpasswd -B /data/ocp4/htpasswd zteca
htpasswd -B /data/ocp4/htpasswd zteadm

oc create secret generic htpasswd --from-file=/data/ocp4/htpasswd -n openshift-config

oc apply -f - &lt;&lt;EOF
apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  name: cluster
spec:
  identityProviders:
  - name: Local Password
    mappingMethod: claim
    type: HTPasswd
    htpasswd:
      fileData:
        name: htpasswd
EOF

watch oc get pod -n openshift-authentication

oc adm policy add-cluster-role-to-user cluster-admin  zteca

oc new-project zte
oc adm policy add-role-to-user admin zteadm -n zte

oc get clusterrolebinding.rbac

oc get clusterrole.rbac

oc adm policy add-cluster-role-to-user cluster-reader  zteadm
oc adm policy remove-cluster-role-from-user cluster-reader  zteadm

#########################################
# add more rhel-ansible-host

# scp vars_static.yaml to helper
cd /data/ocp4/ocp4-upi-helpernode
ansible-playbook -e @vars-static.yaml -e staticips=true tasks/main.yml

ssh-copy-id root@worker-0.ocpsc.redhat.ren

cat &lt;&lt;EOF &gt; /data/ocp4/rhel-ansible-host
[all:vars]
ansible_user=root 
#ansible_become=True 

openshift_kubeconfig_path=&quot;/data/ocp4/auth/kubeconfig&quot; 

[workers] 
infra-0.ocpsc.redhat.ren
infra-1.ocpsc.redhat.ren

[new_workers]
worker-0.ocpsc.redhat.ren

EOF

ansible-playbook -i /data/ocp4/rhel-ansible-host /usr/share/ansible/openshift-ansible/playbooks/scaleup.yml

#########################################
# add more rhel-ansible-host
cat &lt;&lt; EOF  &gt; /etc/yum/pluginconf.d/subscription-manager.conf
[main]
enabled=0
EOF
# scp vars_static.yaml to helper
cd /data/ocp4/ocp4-upi-helpernode
ansible-playbook -e @vars-static.yaml -e staticips=true tasks/main.yml

ssh-copy-id root@worker-1.ocpsc.redhat.ren
ssh-copy-id root@worker-2.ocpsc.redhat.ren

cat &lt;&lt;EOF &gt; /data/ocp4/rhel-ansible-host
[all:vars]
ansible_user=root 
#ansible_become=True 

openshift_kubeconfig_path=&quot;/data/ocp4/auth/kubeconfig&quot; 

[workers] 
infra-0.ocpsc.redhat.ren
infra-1.ocpsc.redhat.ren
worker-0.ocpsc.redhat.ren

[new_workers]
worker-1.ocpsc.redhat.ren
worker-2.ocpsc.redhat.ren

EOF

ansible-playbook -i /data/ocp4/rhel-ansible-host /usr/share/ansible/openshift-ansible/playbooks/scaleup.yml


#########################################
# add worker-3 rhel-ansible-host
# upload vars-static.yaml 
cd /data/ocp4/ocp4-upi-helpernode
ansible-playbook -e @vars-static.yaml -e staticips=true tasks/main.yml

cat &lt;&lt; EOF  &gt; /etc/yum/pluginconf.d/subscription-manager.conf
[main]
enabled=0
EOF
# scp vars_static.yaml to helper
cd /data/ocp4/ocp4-upi-helpernode
ansible-playbook -e @vars-static.yaml -e staticips=true tasks/main.yml

ssh-copy-id root@worker-3.ocpsc.redhat.ren

cat &lt;&lt;EOF &gt; /data/ocp4/rhel-ansible-host
[all:vars]
ansible_user=root 
#ansible_become=True 

openshift_kubeconfig_path=&quot;/data/ocp4/auth/kubeconfig&quot; 

[workers] 
infra-0.ocpsc.redhat.ren
infra-1.ocpsc.redhat.ren
worker-0.ocpsc.redhat.ren
worker-1.ocpsc.redhat.ren
worker-2.ocpsc.redhat.ren

[new_workers]
worker-3.ocpsc.redhat.ren

EOF

ansible-playbook -i /data/ocp4/rhel-ansible-host /usr/share/ansible/openshift-ansible/playbooks/scaleup.yml


</code></pre>
<h3 id="helper-node-day-2-sec"><a class="header" href="#helper-node-day-2-sec">helper node day 2 sec</a></h3>
<pre><code class="language-bash">
cat &lt;&lt; EOF &gt; wzh.script
#!/bin/bash

iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
iptables -A INPUT -s 127.0.0.1/32 -j ACCEPT
iptables -A INPUT -s 223.87.20.0/24 -j ACCEPT
iptables -A INPUT -s 117.177.241.0/24 -j ACCEPT
iptables -A INPUT -s 39.134.200.0/24 -j ACCEPT
iptables -A INPUT -s 39.134.201.0/24 -j ACCEPT
iptables -A INPUT -s 39.137.101.0/24 -j ACCEPT
iptables -A INPUT -s 192.168.7.0/24 -j ACCEPT
iptables -A INPUT -s 112.44.102.224/27 -j ACCEPT
iptables -A INPUT -s 47.93.86.113/32 -j ACCEPT
iptables -A INPUT -s 39.134.204.0/24 -j ACCEPT
iptables -A INPUT -p tcp -j REJECT
iptables -A INPUT -p udp -j REJECT

EOF

var_local=$(cat ./wzh.script | python3 -c &quot;import sys, urllib.parse; print(urllib.parse.quote(''.join(sys.stdin.readlines())))&quot;  )

cat &lt;&lt;EOF &gt; 45-wzh-service.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 45-wzh-service
spec:
  config:
    ignition:
      version: 2.2.0
    storage:
      files:
      - contents:
          source: data:text/plain,${var_local}
          verification: {}
        filesystem: root
        mode: 0755
        path: /etc/rc.d/wzh.local
    systemd:
      units:
      - name: wzh.service
        enabled: true
        contents: |
          [Unit]
          Description=/etc/rc.d/wzh.local Compatibility
          Documentation=zhengwan@redhat.com
          ConditionFileIsExecutable=/etc/rc.d/wzh.local
          After=network.target

          [Service]
          Type=oneshot
          User=root
          Group=root
          ExecStart=/bin/bash -c /etc/rc.d/wzh.local

          [Install]
          WantedBy=multi-user.target

EOF
oc apply -f 45-wzh-service.yaml -n openshift-config


</code></pre>
<h3 id="helper-node-quay"><a class="header" href="#helper-node-quay">helper node quay</a></h3>
<pre><code class="language-bash"># on helper node
firewall-cmd --permanent --zone=public --add-port=4443/tcp
firewall-cmd --reload

podman pod create --infra-image registry.redhat.ren:5443/gcr.io/google_containers/pause-amd64:3.0 --name quay -p 4443:8443 

cd /data
rm -rf /data/quay
podman run -d --name quay-fs --entrypoint &quot;tail&quot; registry.redhat.ren:5443/docker.io/wangzheng422/quay-fs:3.2.0-init -f /dev/null
podman cp quay-fs:/quay.tgz /data/
tar zxf quay.tgz
podman rm -fv quay-fs

export MYSQL_CONTAINER_NAME=quay-mysql
export MYSQL_DATABASE=enterpriseregistrydb
export MYSQL_PASSWORD=zvbk3fzp5f5m2a8j
export MYSQL_USER=quayuser
export MYSQL_ROOT_PASSWORD=q98u335musckfqxe

podman run \
    --detach \
    --restart=always \
    --env MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD} \
    --env MYSQL_USER=${MYSQL_USER} \
    --env MYSQL_PASSWORD=${MYSQL_PASSWORD} \
    --env MYSQL_DATABASE=${MYSQL_DATABASE} \
    --name ${MYSQL_CONTAINER_NAME} \
    --privileged=true \
    --pod quay \
    -v /data/quay/lib/mysql:/var/lib/mysql/data:Z \
    registry.redhat.ren:5443/registry.access.redhat.com/rhscl/mysql-57-rhel7

podman run -d --restart=always \
    --pod quay \
    --privileged=true \
    --name quay-redis \
    -v  /data/quay/lib/redis:/var/lib/redis/data:Z \
    registry.redhat.ren:5443/registry.access.redhat.com/rhscl/redis-32-rhel7

sleep 10

/bin/cp -f /data/cert/redhat.ren.crt /data/quay/config/extra_ca_certs/redhat.ren.crt
/bin/cp -f /data/cert/redhat.ren.crt /data/quay/config/ssl.cert
/bin/cp -f /data/cert/redhat.ren.key /data/quay/config/ssl.key

podman run --restart=always \
    --sysctl net.core.somaxconn=4096 \
    --privileged=true \
    --name quay-master \
    --pod quay \
    --add-host mysql:127.0.0.1 \
    --add-host redis:127.0.0.1 \
    --add-host clair:127.0.0.1 \
    -v /data/quay/config:/conf/stack:Z \
    -v /data/quay/storage:/datastorage:Z \
    -d registry.redhat.ren:5443/quay.io/redhat/quay:v3.2.1

# https://registry.redhat.ren:4443/

podman run --name clair-postgres --pod quay \
    -v /data/quay/lib/postgresql/data:/var/lib/postgresql/data:Z \
    -d registry.redhat.ren:5443/docker.io/library/postgres

# change /data/quay/clair-config/config.yaml
# https://registry.redhat.ren:4443 -&gt; https://registry.redhat.ren:8443
podman run --restart=always -d \
    --name clair \
    -v /data/quay/clair-config:/clair/config:Z \
    -v /data/quay/clair-config/ca.crt:/etc/pki/ca-trust/source/anchors/ca.crt  \
    --pod quay \
    registry.redhat.ren:5443/quay.io/redhat/clair-jwt:v3.2.1

# stop and restart
podman stop clair
podman stop clair-postgres
podman stop quay-master
podman stop quay-redis
podman stop quay-mysql

podman rm quay-master
podman rm quay-redis
podman rm quay-mysql

podman rm clair
podman rm clair-postgres

podman pod ps
podman pod stop quay
podman pod rm quay

</code></pre>
<h3 id="helper-node-zte-oper"><a class="header" href="#helper-node-zte-oper">helper node zte oper</a></h3>
<pre><code class="language-bash">cd /data/ocp4/zte

oc project zxcdn
oc adm policy add-role-to-user admin zteadm -n zxcdn

oc create serviceaccount -n zxcdn zxcdn-app
oc adm policy add-scc-to-user privileged -z zxcdn-app -n zxcdn

# oc adm policy remove-scc-from-user privileged -z  zxcdn-app

oc get networks.operator.openshift.io cluster -o yaml

oc apply -f zte-macvlan.yaml

oc apply -f slbl7-configmap.yaml  
# oc apply -f slbl7-deployment.yaml 
oc apply -f slbl7-pod.yaml

oc apply -f ottcache-configmap.yaml  
oc apply -f ottcache-pod.yaml

# oc apply -f ott-service.yaml

oc delete -f slbl7-pod.yaml
oc delete -f ottcache-pod.yaml

## web cache
oc apply -f slb-configmap.yaml  
oc apply -f slb-deployment.yaml

oc delete -f slb-deployment.yaml

oc apply -f webcache-configmap.yaml  
oc apply -f webcache-deployment.yaml

oc delete -f webcache-deployment.yaml

</code></pre>
<h3 id="helper-host-add-vm-router"><a class="header" href="#helper-host-add-vm-router">helper host add vm-router</a></h3>
<pre><code class="language-bash">
cd /data/ocp4/ocp4-upi-helpernode
ansible-playbook -e @vars-static.yaml -e staticips=true tasks/config.files.yml

# upload install-config.yaml to helper /data/ocp4
cd /data/ocp4

/bin/cp -f worker.ign /var/www/html/ignition/router-0.ign
/bin/cp -f worker.ign /var/www/html/ignition/router-1.ign
/bin/cp -f worker.ign /var/www/html/ignition/router-2.ign
/bin/cp -f worker.ign /var/www/html/ignition/router-3.ign
/bin/cp -f worker.ign /var/www/html/ignition/router-4.ign
/bin/cp -f worker.ign /var/www/html/ignition/router-5.ign
/bin/cp -f worker.ign /var/www/html/ignition/router-6.ign
/bin/cp -f worker.ign /var/www/html/ignition/router-7.ign
/bin/cp -f worker.ign /var/www/html/ignition/router-8.ign

chmod 644 /var/www/html/ignition/*


export NGINX_DIRECTORY=/data/ocp4
export RHCOSVERSION=4.3.0
export VOLID=$(isoinfo -d -i ${NGINX_DIRECTORY}/rhcos-${RHCOSVERSION}-x86_64-installer.iso | awk '/Volume id/ { print $3 }')
TEMPDIR=$(mktemp -d)
echo $VOLID
echo $TEMPDIR

cd ${TEMPDIR}
# Extract the ISO content using guestfish (to avoid sudo mount)
guestfish -a ${NGINX_DIRECTORY}/rhcos-${RHCOSVERSION}-x86_64-installer.iso \
  -m /dev/sda tar-out / - | tar xvf -

# Helper function to modify the config files
modify_cfg(){
  for file in &quot;EFI/redhat/grub.cfg&quot; &quot;isolinux/isolinux.cfg&quot;; do
    # Append the proper image and ignition urls
    sed -e '/coreos.inst=yes/s|$| coreos.inst.install_dev=vda coreos.inst.image_url='&quot;${URL}&quot;'\/install\/'&quot;${BIOSMODE}&quot;'.raw.gz coreos.inst.ignition_url='&quot;${URL}&quot;'\/ignition\/'&quot;${NODE}&quot;'.ign ip='&quot;${IP}&quot;'::'&quot;${GATEWAY}&quot;':'&quot;${NETMASK}&quot;':'&quot;${FQDN}&quot;':'&quot;${NET_INTERFACE}&quot;':none:'&quot;${DNS}&quot;' nameserver='&quot;${DNS}&quot;'|' ${file} &gt; $(pwd)/${NODE}_${file##*/}
    # Boot directly in the installation
    sed -i -e 's/default vesamenu.c32/default linux/g' -e 's/timeout 600/timeout 10/g' $(pwd)/${NODE}_${file##*/}
  done
}

URL=&quot;http://117.177.241.16:8080/&quot;
GATEWAY=&quot;117.177.241.1&quot;
NETMASK=&quot;255.255.255.0&quot;
DNS=&quot;117.177.241.16&quot;

NODE=&quot;router-0&quot;
IP=&quot;117.177.241.243&quot;
FQDN=&quot;vm-router-0&quot;
BIOSMODE=&quot;bios&quot;
NET_INTERFACE=&quot;ens3&quot;
modify_cfg

NODE=&quot;router-1&quot;
IP=&quot;117.177.241.244&quot;
FQDN=&quot;vm-router-1&quot;
BIOSMODE=&quot;bios&quot;
NET_INTERFACE=&quot;ens3&quot;
modify_cfg

NODE=&quot;router-2&quot;
IP=&quot;117.177.241.245&quot;
FQDN=&quot;vm-router-2&quot;
BIOSMODE=&quot;bios&quot;
NET_INTERFACE=&quot;ens3&quot;
modify_cfg

NODE=&quot;router-3&quot;
IP=&quot;117.177.241.246&quot;
FQDN=&quot;vm-router-3&quot;
BIOSMODE=&quot;bios&quot;
NET_INTERFACE=&quot;ens3&quot;
modify_cfg

NODE=&quot;router-4&quot;
IP=&quot;117.177.241.247&quot;
FQDN=&quot;vm-router-4&quot;
BIOSMODE=&quot;bios&quot;
NET_INTERFACE=&quot;ens3&quot;
modify_cfg

NODE=&quot;router-5&quot;
IP=&quot;117.177.241.248&quot;
FQDN=&quot;vm-router-5&quot;
BIOSMODE=&quot;bios&quot;
NET_INTERFACE=&quot;ens3&quot;
modify_cfg

NODE=&quot;router-6&quot;
IP=&quot;117.177.241.249&quot;
FQDN=&quot;vm-router-6&quot;
BIOSMODE=&quot;bios&quot;
NET_INTERFACE=&quot;ens3&quot;
modify_cfg

NODE=&quot;router-7&quot;
IP=&quot;117.177.241.250&quot;
FQDN=&quot;vm-router-7&quot;
BIOSMODE=&quot;bios&quot;
NET_INTERFACE=&quot;ens3&quot;
modify_cfg

NODE=&quot;router-8&quot;
IP=&quot;117.177.241.251&quot;
FQDN=&quot;vm-router-8&quot;
BIOSMODE=&quot;bios&quot;
NET_INTERFACE=&quot;ens3&quot;
modify_cfg

# Generate the images, one per node as the IP configuration is different...
# https://github.com/coreos/coreos-assembler/blob/master/src/cmd-buildextend-installer#L97-L103
for node in router-0 router-1 router-2 router-3 router-4 router-5 router-6 router-7 router-8; do
  # Overwrite the grub.cfg and isolinux.cfg files for each node type
  for file in &quot;EFI/redhat/grub.cfg&quot; &quot;isolinux/isolinux.cfg&quot;; do
    /bin/cp -f $(pwd)/${node}_${file##*/} ${file}
  done
  # As regular user!
  genisoimage -verbose -rock -J -joliet-long -volset ${VOLID} \
    -eltorito-boot isolinux/isolinux.bin -eltorito-catalog isolinux/boot.cat \
    -no-emul-boot -boot-load-size 4 -boot-info-table \
    -eltorito-alt-boot -efi-boot images/efiboot.img -no-emul-boot \
    -o ${NGINX_DIRECTORY}/${node}.iso .
done

# Optionally, clean up
cd /data/ocp4
rm -Rf ${TEMPDIR}

cd ${NGINX_DIRECTORY}

scp router-*.iso root@117.177.241.21:/data/ocp4/

# after vm on bootstrap created
oc get csr
oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{&quot;\n&quot;}}{{end}}{{end}}' | xargs oc adm certificate approve

oc label node vm-router-0.ocpsc.redhat.ren node-role.kubernetes.io/router=''
oc label node vm-router-1.ocpsc.redhat.ren node-role.kubernetes.io/router=''
oc label node vm-router-2.ocpsc.redhat.ren node-role.kubernetes.io/router=''
oc label node vm-router-3.ocpsc.redhat.ren node-role.kubernetes.io/router=''
oc label node vm-router-4.ocpsc.redhat.ren node-role.kubernetes.io/router=''
# oc label node vm-router-5.ocpsc.redhat.ren node-role.kubernetes.io/router=''
# oc label node vm-router-6.ocpsc.redhat.ren node-role.kubernetes.io/router=''
# oc label node vm-router-7.ocpsc.redhat.ren node-role.kubernetes.io/router=''
# oc label node vm-router-8.ocpsc.redhat.ren node-role.kubernetes.io/router=''

##########################
## secure the router vm

cat &lt;&lt; EOF &gt; router.mcp.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: router
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,router]}
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/router: &quot;&quot;
EOF
oc apply -f router.mcp.yaml

cat &lt;&lt; EOF &gt; wzh.script
#!/bin/bash

iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
iptables -A INPUT -s 127.0.0.1/32 -j ACCEPT
iptables -A INPUT -s 223.87.20.0/24 -j ACCEPT
iptables -A INPUT -s 117.177.241.0/24 -j ACCEPT
iptables -A INPUT -s 39.134.200.0/24 -j ACCEPT
iptables -A INPUT -s 39.134.201.0/24 -j ACCEPT
iptables -A INPUT -s 39.137.101.0/24 -j ACCEPT
iptables -A INPUT -s 192.168.7.0/24 -j ACCEPT
iptables -A INPUT -s 112.44.102.224/27 -j ACCEPT
iptables -A INPUT -s 47.93.86.113/32 -j ACCEPT
iptables -A INPUT -p tcp -j REJECT
iptables -A INPUT -p udp -j REJECT

EOF

var_local=$(cat ./wzh.script | python3 -c &quot;import sys, urllib.parse; print(urllib.parse.quote(''.join(sys.stdin.readlines())))&quot;  )

cat &lt;&lt;EOF &gt; 45-router-wzh-service.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: router
  name: 45-router-wzh-service
spec:
  config:
    ignition:
      version: 2.2.0
    storage:
      files:
      - contents:
          source: data:text/plain,${var_local}
          verification: {}
        filesystem: root
        mode: 0755
        path: /etc/rc.d/wzh.local
    systemd:
      units:
      - name: wzh.service
        enabled: true
        contents: |
          [Unit]
          Description=/etc/rc.d/wzh.local Compatibility
          Documentation=zhengwan@redhat.com
          ConditionFileIsExecutable=/etc/rc.d/wzh.local
          After=network.target

          [Service]
          Type=oneshot
          User=root
          Group=root
          ExecStart=/bin/bash -c /etc/rc.d/wzh.local

          [Install]
          WantedBy=multi-user.target

EOF
oc apply -f 45-router-wzh-service.yaml -n openshift-config

# DO NOT
# cp 99-master-zzz-container-registries.yaml 99-router-zzz-container-registries.yaml 
# # change: machineconfiguration.openshift.io/role: router
# oc apply -f ./99-router-zzz-container-registries.yaml -n openshift-config

# on helper node
cat &lt;&lt; EOF &gt; /etc/docker-distribution/registry/config.yml
version: 0.1
log:
  fields:
    service: registry
storage:
    cache:
        layerinfo: inmemory
    filesystem:
        rootdirectory: /data/registry
    delete:
        enabled: true
http:
    addr: :5443
    tls:
       certificate: /data/cert/redhat.ren.crt
       key: /data/cert/redhat.ren.key

EOF

systemctl restart docker-distribution


</code></pre>
<h3 id="helper-node-zte-tcp-router"><a class="header" href="#helper-node-zte-tcp-router">helper node zte tcp-router</a></h3>
<pre><code class="language-bash">
oc project openshift-ingress

# install the tcp-router and demo
oc create configmap customrouter-wzh --from-file=haproxy-config.template
oc apply -f haproxy.router.yaml

oc project zxcdn

oc apply -f ott-service.tcp.route.yaml


</code></pre>
<h3 id="helper-node-cluster-tunning"><a class="header" href="#helper-node-cluster-tunning">helper node cluster tunning</a></h3>
<pre><code class="language-bash"># tunning for pid.max

oc label mcp worker custom-kubelet-pod-pids-limit=true

cat &lt;&lt; EOF &gt; PodPidsLimit.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: pod-pids-limit
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet-pod-pids-limit: 'true'
  kubeletConfig:
    PodPidsLimit: 4096
EOF
oc apply -f PodPidsLimit.yaml

cat &lt;&lt; EOF &gt; crio.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: ContainerRuntimeConfig
metadata:
 name: set-log-and-pid
spec:
 machineConfigPoolSelector:
   matchLabels:
     custom-kubelet-pod-pids-limit: 'true'
 containerRuntimeConfig:
   pidsLimit: 10240
EOF
oc apply -f crio.yaml


</code></pre>
<h3 id="helper-node-local-storage"><a class="header" href="#helper-node-local-storage">helper node local storage</a></h3>
<p>https://docs.openshift.com/container-platform/4.3/storage/persistent_storage/persistent-storage-local.html</p>
<pre><code class="language-bash">
oc new-project local-storage


apiVersion: &quot;local.storage.openshift.io/v1&quot;
kind: &quot;LocalVolume&quot;
metadata:
  name: &quot;local-disks&quot;
  namespace: &quot;local-storage&quot; 
spec:
  nodeSelector: 
    nodeSelectorTerms:
    - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - infra0.hsc.redhat.ren
          - infra1.hsc.redhat.ren
  storageClassDevices:
    - storageClassName: &quot;local-sc&quot;
      volumeMode: Filesystem 
      fsType: xfs 
      devicePaths: 
        - /dev/datavg/monitorlv


</code></pre>
<h3 id="bootstrap-node-day1"><a class="header" href="#bootstrap-node-day1">bootstrap node day1</a></h3>
<pre><code class="language-bash">##########################################################3
## on bootstrap
yum -y install tigervnc-server tigervnc gnome-terminal gnome-session gnome-classic-session gnome-terminal nautilus-open-terminal control-center liberation-mono-fonts google-noto-sans-cjk-fonts google-noto-sans-fonts fonts-tweak-tool

yum install -y    qgnomeplatform   xdg-desktop-portal-gtk   NetworkManager-libreswan-gnome   PackageKit-command-not-found   PackageKit-gtk3-module   abrt-desktop   at-spi2-atk   at-spi2-core   avahi   baobab   caribou   caribou-gtk2-module   caribou-gtk3-module   cheese   compat-cheese314   control-center   dconf   empathy   eog   evince   evince-nautilus   file-roller   file-roller-nautilus   firewall-config   firstboot   fprintd-pam   gdm   gedit   glib-networking   gnome-bluetooth   gnome-boxes   gnome-calculator   gnome-classic-session   gnome-clocks   gnome-color-manager   gnome-contacts   gnome-dictionary   gnome-disk-utility   gnome-font-viewer   gnome-getting-started-docs   gnome-icon-theme   gnome-icon-theme-extras   gnome-icon-theme-symbolic   gnome-initial-setup   gnome-packagekit   gnome-packagekit-updater   gnome-screenshot   gnome-session   gnome-session-xsession   gnome-settings-daemon   gnome-shell   gnome-software   gnome-system-log   gnome-system-monitor   gnome-terminal   gnome-terminal-nautilus   gnome-themes-standard   gnome-tweak-tool   nm-connection-editor   orca   redhat-access-gui   sane-backends-drivers-scanners   seahorse   setroubleshoot   sushi   totem   totem-nautilus   vinagre   vino   xdg-user-dirs-gtk   yelp

yum install -y    cjkuni-uming-fonts   dejavu-sans-fonts   dejavu-sans-mono-fonts   dejavu-serif-fonts   gnu-free-mono-fonts   gnu-free-sans-fonts   gnu-free-serif-fonts   google-crosextra-caladea-fonts   google-crosextra-carlito-fonts   google-noto-emoji-fonts   jomolhari-fonts   khmeros-base-fonts   liberation-mono-fonts   liberation-sans-fonts   liberation-serif-fonts   lklug-fonts   lohit-assamese-fonts   lohit-bengali-fonts   lohit-devanagari-fonts   lohit-gujarati-fonts   lohit-kannada-fonts   lohit-malayalam-fonts   lohit-marathi-fonts   lohit-nepali-fonts   lohit-oriya-fonts   lohit-punjabi-fonts   lohit-tamil-fonts   lohit-telugu-fonts   madan-fonts   nhn-nanum-gothic-fonts   open-sans-fonts   overpass-fonts   paktype-naskh-basic-fonts   paratype-pt-sans-fonts   sil-abyssinica-fonts   sil-nuosu-fonts   sil-padauk-fonts   smc-meera-fonts   stix-fonts   thai-scalable-waree-fonts   ucs-miscfixed-fonts   vlgothic-fonts   wqy-microhei-fonts   wqy-zenhei-fonts

vncpasswd

cat &lt;&lt; EOF &gt; ~/.vnc/xstartup
#!/bin/sh
unset SESSION_MANAGER
unset DBUS_SESSION_BUS_ADDRESS
gnome-session &amp;
EOF
chmod +x ~/.vnc/xstartup

vncserver :1 -geometry 1280x800
# 如果你想停掉vnc server，这么做
vncserver -kill :1

firewall-cmd --permanent --add-port=6001/tcp
firewall-cmd --permanent --add-port=5901/tcp
firewall-cmd --reload

# 配置kvm环境
yum -y install qemu-kvm libvirt libvirt-python libguestfs-tools virt-install virt-viewer virt-manager

systemctl enable libvirtd
systemctl start libvirtd

brctl show
virsh net-list

cat &lt;&lt; EOF &gt;  /data/virt-net.xml
&lt;network&gt;
  &lt;name&gt;br0&lt;/name&gt;
  &lt;forward mode='bridge'&gt;
    &lt;bridge name='br0'/&gt;
  &lt;/forward&gt;
&lt;/network&gt;
EOF

virsh net-define --file virt-net.xml
virsh net-dumpxml br0
# virsh net-undefine openshift4
# virsh net-destroy openshift4
virsh net-autostart br0
virsh net-start br0

cp /etc/sysconfig/network-scripts/ifcfg-em1 /etc/sysconfig/network-scripts/ifcfg-em1.orig

cat &lt;&lt; EOF &gt; /etc/sysconfig/network-scripts/ifcfg-em1
TYPE=Ethernet
PROXY_METHOD=none
BROWSER_ONLY=no
BOOTPROTO=none
DEFROUTE=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_FAILURE_FATAL=no
IPV6_ADDR_GEN_MODE=stable-privacy
NAME=em1
DEVICE=em1
ONBOOT=yes
# IPADDR=117.177.241.21
# PREFIX=24
# GATEWAY=117.177.241.1
IPV6_PRIVACY=no
# DNS1=117.177.241.16
BRIDGE=br0
EOF

cat &lt;&lt;EOF &gt; /etc/sysconfig/network-scripts/ifcfg-br0 
TYPE=Bridge
BOOTPROTO=static
IPADDR=117.177.241.21
GATEWAY=117.177.241.1
DNS1=117.177.241.16
ONBOOT=yes
DEFROUTE=yes
NAME=br0
DEVICE=br0
PREFIX=24
EOF

systemctl restart network

virt-install --name=ocp4-bootstrap --vcpus=2 --ram=16384 \
--disk path=/data/kvm/ocp4-bootstrap.qcow2,bus=virtio,size=200 \
--os-variant rhel8.0 --network bridge=br0,model=virtio \
--boot menu=on --cdrom /data/ocp4/bootstrap-static.iso   

virt-install --name=ocp4-master0 --vcpus=8 --ram=65536 \
--disk path=/data/kvm/ocp4-master0.qcow2,bus=virtio,size=200 \
--os-variant rhel8.0 --network bridge=br0,model=virtio \
--boot menu=on --cdrom /data/ocp4/master-0.iso 

# virt-install --name=ocp4-master1 --vcpus=20 --ram=200704 \
# --disk path=/data/kvm/ocp4-master1.qcow2,bus=virtio,size=200 \
# --os-variant rhel8.0 --network bridge=br0,model=virtio \
# --boot menu=on --cdrom /data/ocp4/master-1.iso 

virt-install --name=ocp4-master2 --vcpus=8 --ram=65536 \
--disk path=/data/kvm/ocp4-master2.qcow2,bus=virtio,size=200 \
--os-variant rhel8.0 --network bridge=br0,model=virtio \
--boot menu=on --cdrom /data/ocp4/master-2.iso 

virt-install --name=ocp4-worker0 --vcpus=4 --ram=32768 \
--disk path=/data/kvm/ocp4-worker0.qcow2,bus=virtio,size=200 \
--os-variant rhel8.0 --network bridge=br0,model=virtio \
--boot menu=on --cdrom /data/ocp4/worker-0.iso 

virt-install --name=ocp4-worker1 --vcpus=4 --ram=32768 \
--disk path=/data/kvm/ocp4-worker1.qcow2,bus=virtio,size=200 \
--os-variant rhel8.0 --network bridge=br0,model=virtio \
--boot menu=on --cdrom /data/ocp4/worker-1.iso 


tar -cvf - ocp4-master0.qcow2 | pigz -c &gt; /data/kvm/ocp4-master0.qcow2.tgz
rsync -e &quot;ssh -c chacha20-poly1305@openssh.com&quot; --info=progress2 -P -arz  /data/kvm/ocp4-master0.qcow2.tgz root@117.177.241.18:/data/kvm/

tar -cvf - ocp4-master2.qcow2 | pigz -c &gt; /data/kvm/ocp4-master2.qcow2.tgz
rsync -e &quot;ssh -c chacha20-poly1305@openssh.com&quot; --info=progress2 -P -arz  /data/kvm/ocp4-master2.qcow2.tgz root@117.177.241.22:/data/kvm/

# anti scan
firewall-cmd --permanent --new-ipset=my-allow-list --type=hash:net
firewall-cmd --permanent --get-ipsets

cat &gt; /root/iplist.txt &lt;&lt;EOL
127.0.0.1/32
223.87.20.0/24
117.177.241.0/24
39.134.200.0/24
39.134.201.0/24
39.137.101.0/24
192.168.7.0/24
112.44.102.224/27
47.93.86.113/32
EOL

firewall-cmd --permanent --ipset=my-allow-list --add-entries-from-file=iplist.txt

firewall-cmd --permanent --ipset=my-allow-list --get-entries

firewall-cmd --permanent --zone=trusted --add-source=ipset:my-allow-list 
firewall-cmd --reload

firewall-cmd --list-all
firewall-cmd --get-active-zones

firewall-cmd --set-default-zone=block
firewall-cmd --runtime-to-permanent
firewall-cmd --reload

# https://access.redhat.com/solutions/39604
virsh list

virsh dump ocp4-router-0 /data/tmp/ocp4-router-0.dump --memory-only --verbose

virsh dump ocp4-router-1 /data/tmp/ocp4-router-1.dump --memory-only --verbose

virsh dump ocp4-router-2 /data/tmp/ocp4-router-2.dump --memory-only --verbose

virsh dump ocp4-router-3 /data/tmp/ocp4-router-3.dump --memory-only --verbose

cd /data
tar -cvf - tmp/ | pigz -c &gt; virsh.dump.tgz



################################
## add more router vm
virt-install --name=ocp4-router-0 --vcpus=4 --ram=16384 \
--disk path=/data/kvm/ocp4-router-0.qcow2,bus=virtio,size=200 \
--os-variant rhel8.0 --network bridge=br0,model=virtio \
--boot menu=on --cdrom /data/ocp4/router-0.iso 

virt-install --name=ocp4-router-1 --vcpus=4 --ram=16384 \
--disk path=/data/kvm/ocp4-router-1.qcow2,bus=virtio,size=200 \
--os-variant rhel8.0 --network bridge=br0,model=virtio \
--boot menu=on --cdrom /data/ocp4/router-1.iso 

virt-install --name=ocp4-router-2 --vcpus=4 --ram=16384 \
--disk path=/data/kvm/ocp4-router-2.qcow2,bus=virtio,size=200 \
--os-variant rhel8.0 --network bridge=br0,model=virtio \
--boot menu=on --cdrom /data/ocp4/router-2.iso 

virt-install --name=ocp4-router-3 --vcpus=4 --ram=16384 \
--disk path=/data/kvm/ocp4-router-3.qcow2,bus=virtio,size=200 \
--os-variant rhel8.0 --network bridge=br0,model=virtio \
--boot menu=on --cdrom /data/ocp4/router-3.iso 

virt-install --name=ocp4-router-4 --vcpus=4 --ram=16384 \
--disk path=/data/kvm/ocp4-router-4.qcow2,bus=virtio,size=200 \
--os-variant rhel8.0 --network bridge=br0,model=virtio \
--boot menu=on --cdrom /data/ocp4/router-4.iso 

# virt-install --name=ocp4-router-5 --vcpus=2 --ram=8192 \
# --disk path=/data/kvm/ocp4-router-5.qcow2,bus=virtio,size=200 \
# --os-variant rhel8.0 --network bridge=br0,model=virtio \
# --boot menu=on --cdrom /data/ocp4/router-5.iso 

# virt-install --name=ocp4-router-6 --vcpus=2 --ram=8192 \
# --disk path=/data/kvm/ocp4-router-6.qcow2,bus=virtio,size=200 \
# --os-variant rhel8.0 --network bridge=br0,model=virtio \
# --boot menu=on --cdrom /data/ocp4/router-6.iso 

# virt-install --name=ocp4-router-7 --vcpus=2 --ram=8192 \
# --disk path=/data/kvm/ocp4-router-7.qcow2,bus=virtio,size=200 \
# --os-variant rhel8.0 --network bridge=br0,model=virtio \
# --boot menu=on --cdrom /data/ocp4/router-7.iso 

# virt-install --name=ocp4-router-8 --vcpus=2 --ram=8192 \
# --disk path=/data/kvm/ocp4-router-8.qcow2,bus=virtio,size=200 \
# --os-variant rhel8.0 --network bridge=br0,model=virtio \
# --boot menu=on --cdrom /data/ocp4/router-8.iso 


# helper node operation


</code></pre>
<h3 id="master1-node-day1"><a class="header" href="#master1-node-day1">master1 node day1</a></h3>
<pre><code class="language-bash">##########################################################3
## on master1
yum -y install tigervnc-server tigervnc gnome-terminal gnome-session gnome-classic-session gnome-terminal nautilus-open-terminal control-center liberation-mono-fonts google-noto-sans-cjk-fonts google-noto-sans-fonts fonts-tweak-tool

yum install -y    qgnomeplatform   xdg-desktop-portal-gtk   NetworkManager-libreswan-gnome   PackageKit-command-not-found   PackageKit-gtk3-module   abrt-desktop   at-spi2-atk   at-spi2-core   avahi   baobab   caribou   caribou-gtk2-module   caribou-gtk3-module   cheese   compat-cheese314   control-center   dconf   empathy   eog   evince   evince-nautilus   file-roller   file-roller-nautilus   firewall-config   firstboot   fprintd-pam   gdm   gedit   glib-networking   gnome-bluetooth   gnome-boxes   gnome-calculator   gnome-classic-session   gnome-clocks   gnome-color-manager   gnome-contacts   gnome-dictionary   gnome-disk-utility   gnome-font-viewer   gnome-getting-started-docs   gnome-icon-theme   gnome-icon-theme-extras   gnome-icon-theme-symbolic   gnome-initial-setup   gnome-packagekit   gnome-packagekit-updater   gnome-screenshot   gnome-session   gnome-session-xsession   gnome-settings-daemon   gnome-shell   gnome-software   gnome-system-log   gnome-system-monitor   gnome-terminal   gnome-terminal-nautilus   gnome-themes-standard   gnome-tweak-tool   nm-connection-editor   orca   redhat-access-gui   sane-backends-drivers-scanners   seahorse   setroubleshoot   sushi   totem   totem-nautilus   vinagre   vino   xdg-user-dirs-gtk   yelp

yum install -y    cjkuni-uming-fonts   dejavu-sans-fonts   dejavu-sans-mono-fonts   dejavu-serif-fonts   gnu-free-mono-fonts   gnu-free-sans-fonts   gnu-free-serif-fonts   google-crosextra-caladea-fonts   google-crosextra-carlito-fonts   google-noto-emoji-fonts   jomolhari-fonts   khmeros-base-fonts   liberation-mono-fonts   liberation-sans-fonts   liberation-serif-fonts   lklug-fonts   lohit-assamese-fonts   lohit-bengali-fonts   lohit-devanagari-fonts   lohit-gujarati-fonts   lohit-kannada-fonts   lohit-malayalam-fonts   lohit-marathi-fonts   lohit-nepali-fonts   lohit-oriya-fonts   lohit-punjabi-fonts   lohit-tamil-fonts   lohit-telugu-fonts   madan-fonts   nhn-nanum-gothic-fonts   open-sans-fonts   overpass-fonts   paktype-naskh-basic-fonts   paratype-pt-sans-fonts   sil-abyssinica-fonts   sil-nuosu-fonts   sil-padauk-fonts   smc-meera-fonts   stix-fonts   thai-scalable-waree-fonts   ucs-miscfixed-fonts   vlgothic-fonts   wqy-microhei-fonts   wqy-zenhei-fonts

vncpasswd

cat &lt;&lt; EOF &gt; ~/.vnc/xstartup
#!/bin/sh
unset SESSION_MANAGER
unset DBUS_SESSION_BUS_ADDRESS
gnome-session &amp;
EOF
chmod +x ~/.vnc/xstartup

vncserver :1 -geometry 1280x800
# 如果你想停掉vnc server，这么做
vncserver -kill :1

firewall-cmd --permanent --add-port=6001/tcp
firewall-cmd --permanent --add-port=5901/tcp
firewall-cmd --reload

# 配置kvm环境
yum -y install qemu-kvm libvirt libvirt-python libguestfs-tools virt-install virt-viewer virt-manager

systemctl enable libvirtd
systemctl start libvirtd

brctl show
virsh net-list

cat &lt;&lt; EOF &gt;  /data/virt-net.xml
&lt;network&gt;
  &lt;name&gt;br0&lt;/name&gt;
  &lt;forward mode='bridge'&gt;
    &lt;bridge name='br0'/&gt;
  &lt;/forward&gt;
&lt;/network&gt;
EOF

virsh net-define --file virt-net.xml
virsh net-dumpxml br0
# virsh net-undefine openshift4
# virsh net-destroy openshift4
virsh net-autostart br0
virsh net-start br0

cp /etc/sysconfig/network-scripts/ifcfg-em1 /etc/sysconfig/network-scripts/ifcfg-em1.orig

cat &lt;&lt; EOF &gt; /etc/sysconfig/network-scripts/ifcfg-em1
TYPE=Ethernet
PROXY_METHOD=none
BROWSER_ONLY=no
BOOTPROTO=none
DEFROUTE=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_FAILURE_FATAL=no
IPV6_ADDR_GEN_MODE=stable-privacy
NAME=em1
DEVICE=em1
ONBOOT=yes
# IPADDR=117.177.241.17
# PREFIX=24
# GATEWAY=117.177.241.1
IPV6_PRIVACY=no
# DNS1=117.177.241.16
BRIDGE=br0
EOF

cat &lt;&lt;EOF &gt; /etc/sysconfig/network-scripts/ifcfg-br0 
TYPE=Bridge
BOOTPROTO=static
IPADDR=117.177.241.17
GATEWAY=117.177.241.1
DNS1=117.177.241.16
ONBOOT=yes
DEFROUTE=yes
NAME=br0
DEVICE=br0
PREFIX=24
EOF

systemctl restart network

virt-install --name=ocp4-master1 --vcpus=20 --ram=200704 \
--disk path=/data/kvm/ocp4-master1.qcow2,bus=virtio,size=200 \
--os-variant rhel8.0 --network bridge=br0,model=virtio \
--boot menu=on --cdrom /data/ocp4/master-1.iso 

virsh list --all

virsh start ocp4-master1

# anti scan
firewall-cmd --permanent --new-ipset=my-allow-list --type=hash:net
firewall-cmd --permanent --get-ipsets

cat &gt; /root/iplist.txt &lt;&lt;EOL
127.0.0.1/32
223.87.20.0/24
117.177.241.0/24
39.134.200.0/24
39.134.201.0/24
39.137.101.0/24
192.168.7.0/24
112.44.102.224/27
47.93.86.113/32
EOL

firewall-cmd --permanent --ipset=my-allow-list --add-entries-from-file=iplist.txt

firewall-cmd --permanent --ipset=my-allow-list --get-entries

firewall-cmd --permanent --zone=trusted --add-source=ipset:my-allow-list 
firewall-cmd --reload

firewall-cmd --list-all
firewall-cmd --get-active-zones

firewall-cmd --set-default-zone=block
firewall-cmd --runtime-to-permanent
firewall-cmd --reload

</code></pre>
<h3 id="master0-node-day1"><a class="header" href="#master0-node-day1">master0 node day1</a></h3>
<pre><code class="language-bash">########################################################
# master0 
yum -y install tigervnc-server tigervnc gnome-terminal gnome-session gnome-classic-session gnome-terminal nautilus-open-terminal control-center liberation-mono-fonts google-noto-sans-cjk-fonts google-noto-sans-fonts fonts-tweak-tool

yum install -y    qgnomeplatform   xdg-desktop-portal-gtk   NetworkManager-libreswan-gnome   PackageKit-command-not-found   PackageKit-gtk3-module   abrt-desktop   at-spi2-atk   at-spi2-core   avahi   baobab   caribou   caribou-gtk2-module   caribou-gtk3-module   cheese   compat-cheese314   control-center   dconf   empathy   eog   evince   evince-nautilus   file-roller   file-roller-nautilus   firewall-config   firstboot   fprintd-pam   gdm   gedit   glib-networking   gnome-bluetooth   gnome-boxes   gnome-calculator   gnome-classic-session   gnome-clocks   gnome-color-manager   gnome-contacts   gnome-dictionary   gnome-disk-utility   gnome-font-viewer   gnome-getting-started-docs   gnome-icon-theme   gnome-icon-theme-extras   gnome-icon-theme-symbolic   gnome-initial-setup   gnome-packagekit   gnome-packagekit-updater   gnome-screenshot   gnome-session   gnome-session-xsession   gnome-settings-daemon   gnome-shell   gnome-software   gnome-system-log   gnome-system-monitor   gnome-terminal   gnome-terminal-nautilus   gnome-themes-standard   gnome-tweak-tool   nm-connection-editor   orca   redhat-access-gui   sane-backends-drivers-scanners   seahorse   setroubleshoot   sushi   totem   totem-nautilus   vinagre   vino   xdg-user-dirs-gtk   yelp

yum install -y    cjkuni-uming-fonts   dejavu-sans-fonts   dejavu-sans-mono-fonts   dejavu-serif-fonts   gnu-free-mono-fonts   gnu-free-sans-fonts   gnu-free-serif-fonts   google-crosextra-caladea-fonts   google-crosextra-carlito-fonts   google-noto-emoji-fonts   jomolhari-fonts   khmeros-base-fonts   liberation-mono-fonts   liberation-sans-fonts   liberation-serif-fonts   lklug-fonts   lohit-assamese-fonts   lohit-bengali-fonts   lohit-devanagari-fonts   lohit-gujarati-fonts   lohit-kannada-fonts   lohit-malayalam-fonts   lohit-marathi-fonts   lohit-nepali-fonts   lohit-oriya-fonts   lohit-punjabi-fonts   lohit-tamil-fonts   lohit-telugu-fonts   madan-fonts   nhn-nanum-gothic-fonts   open-sans-fonts   overpass-fonts   paktype-naskh-basic-fonts   paratype-pt-sans-fonts   sil-abyssinica-fonts   sil-nuosu-fonts   sil-padauk-fonts   smc-meera-fonts   stix-fonts   thai-scalable-waree-fonts   ucs-miscfixed-fonts   vlgothic-fonts   wqy-microhei-fonts   wqy-zenhei-fonts

vncpasswd

cat &lt;&lt; EOF &gt; ~/.vnc/xstartup
#!/bin/sh
unset SESSION_MANAGER
unset DBUS_SESSION_BUS_ADDRESS
gnome-session &amp;
EOF
chmod +x ~/.vnc/xstartup

vncserver :1 -geometry 1280x800
# 如果你想停掉vnc server，这么做
vncserver -kill :1

firewall-cmd --permanent --add-port=6001/tcp
firewall-cmd --permanent --add-port=5901/tcp
firewall-cmd --reload

# 配置kvm环境
yum -y install qemu-kvm libvirt libvirt-python libguestfs-tools virt-install virt-viewer virt-manager

systemctl enable libvirtd
systemctl start libvirtd

brctl show
virsh net-list

cat &lt;&lt; EOF &gt;  /data/virt-net.xml
&lt;network&gt;
  &lt;name&gt;br0&lt;/name&gt;
  &lt;forward mode='bridge'&gt;
    &lt;bridge name='br0'/&gt;
  &lt;/forward&gt;
&lt;/network&gt;
EOF

virsh net-define --file virt-net.xml
virsh net-dumpxml br0
# virsh net-undefine openshift4
# virsh net-destroy openshift4
virsh net-autostart br0
virsh net-start br0

cp /etc/sysconfig/network-scripts/ifcfg-em1 /etc/sysconfig/network-scripts/ifcfg-em1.orig

cat &lt;&lt; EOF &gt; /etc/sysconfig/network-scripts/ifcfg-em1
TYPE=Ethernet
PROXY_METHOD=none
BROWSER_ONLY=no
BOOTPROTO=none
DEFROUTE=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_FAILURE_FATAL=no
IPV6_ADDR_GEN_MODE=stable-privacy
NAME=em1
DEVICE=em1
ONBOOT=yes
# IPADDR=117.177.241.18
# PREFIX=24
# GATEWAY=117.177.241.1
IPV6_PRIVACY=no
# DNS1=117.177.241.16
BRIDGE=br0
EOF

cat &lt;&lt;EOF &gt; /etc/sysconfig/network-scripts/ifcfg-br0 
TYPE=Bridge
BOOTPROTO=static
IPADDR=117.177.241.18
GATEWAY=117.177.241.1
DNS1=117.177.241.16
ONBOOT=yes
DEFROUTE=yes
NAME=br0
DEVICE=br0
PREFIX=24
EOF

systemctl restart network

mkdir -p /data/ocp4
mkdir -p /data/kvm

pigz -dc ocp4-master0.qcow2.tgz | tar xf -

virt-install --name=ocp4-master0 --vcpus=20 --ram=200704 \
--disk path=/data/kvm/ocp4-master0.qcow2,bus=virtio,size=200 \
--os-variant rhel8.0 --network bridge=br0,model=virtio \
--boot menu=on 

virsh list --all

virsh start ocp4-master0

# anti scan
firewall-cmd --permanent --new-ipset=my-allow-list --type=hash:net
firewall-cmd --permanent --get-ipsets

cat &gt; /root/iplist.txt &lt;&lt;EOL
127.0.0.1/32
223.87.20.0/24
117.177.241.0/24
39.134.200.0/24
39.134.201.0/24
39.137.101.0/24
192.168.7.0/24
112.44.102.224/27
47.93.86.113/32
EOL

firewall-cmd --permanent --ipset=my-allow-list --add-entries-from-file=iplist.txt

firewall-cmd --permanent --ipset=my-allow-list --get-entries

firewall-cmd --permanent --zone=trusted --add-source=ipset:my-allow-list 
firewall-cmd --reload

firewall-cmd --list-all
firewall-cmd --get-active-zones

firewall-cmd --set-default-zone=block
firewall-cmd --runtime-to-permanent
firewall-cmd --reload

</code></pre>
<h3 id="master2-node-day1"><a class="header" href="#master2-node-day1">master2 node day1</a></h3>
<pre><code class="language-bash">########################################################
# master2 
yum -y install tigervnc-server tigervnc gnome-terminal gnome-session gnome-classic-session gnome-terminal nautilus-open-terminal control-center liberation-mono-fonts google-noto-sans-cjk-fonts google-noto-sans-fonts fonts-tweak-tool

yum install -y    qgnomeplatform   xdg-desktop-portal-gtk   NetworkManager-libreswan-gnome   PackageKit-command-not-found   PackageKit-gtk3-module   abrt-desktop   at-spi2-atk   at-spi2-core   avahi   baobab   caribou   caribou-gtk2-module   caribou-gtk3-module   cheese   compat-cheese314   control-center   dconf   empathy   eog   evince   evince-nautilus   file-roller   file-roller-nautilus   firewall-config   firstboot   fprintd-pam   gdm   gedit   glib-networking   gnome-bluetooth   gnome-boxes   gnome-calculator   gnome-classic-session   gnome-clocks   gnome-color-manager   gnome-contacts   gnome-dictionary   gnome-disk-utility   gnome-font-viewer   gnome-getting-started-docs   gnome-icon-theme   gnome-icon-theme-extras   gnome-icon-theme-symbolic   gnome-initial-setup   gnome-packagekit   gnome-packagekit-updater   gnome-screenshot   gnome-session   gnome-session-xsession   gnome-settings-daemon   gnome-shell   gnome-software   gnome-system-log   gnome-system-monitor   gnome-terminal   gnome-terminal-nautilus   gnome-themes-standard   gnome-tweak-tool   nm-connection-editor   orca   redhat-access-gui   sane-backends-drivers-scanners   seahorse   setroubleshoot   sushi   totem   totem-nautilus   vinagre   vino   xdg-user-dirs-gtk   yelp

yum install -y    cjkuni-uming-fonts   dejavu-sans-fonts   dejavu-sans-mono-fonts   dejavu-serif-fonts   gnu-free-mono-fonts   gnu-free-sans-fonts   gnu-free-serif-fonts   google-crosextra-caladea-fonts   google-crosextra-carlito-fonts   google-noto-emoji-fonts   jomolhari-fonts   khmeros-base-fonts   liberation-mono-fonts   liberation-sans-fonts   liberation-serif-fonts   lklug-fonts   lohit-assamese-fonts   lohit-bengali-fonts   lohit-devanagari-fonts   lohit-gujarati-fonts   lohit-kannada-fonts   lohit-malayalam-fonts   lohit-marathi-fonts   lohit-nepali-fonts   lohit-oriya-fonts   lohit-punjabi-fonts   lohit-tamil-fonts   lohit-telugu-fonts   madan-fonts   nhn-nanum-gothic-fonts   open-sans-fonts   overpass-fonts   paktype-naskh-basic-fonts   paratype-pt-sans-fonts   sil-abyssinica-fonts   sil-nuosu-fonts   sil-padauk-fonts   smc-meera-fonts   stix-fonts   thai-scalable-waree-fonts   ucs-miscfixed-fonts   vlgothic-fonts   wqy-microhei-fonts   wqy-zenhei-fonts

vncpasswd

cat &lt;&lt; EOF &gt; ~/.vnc/xstartup
#!/bin/sh
unset SESSION_MANAGER
unset DBUS_SESSION_BUS_ADDRESS
gnome-session &amp;
EOF
chmod +x ~/.vnc/xstartup

vncserver :1 -geometry 1280x800
# 如果你想停掉vnc server，这么做
vncserver -kill :1

firewall-cmd --permanent --add-port=6001/tcp
firewall-cmd --permanent --add-port=5901/tcp
firewall-cmd --reload

# 配置kvm环境
yum -y install qemu-kvm libvirt libvirt-python libguestfs-tools virt-install virt-viewer virt-manager

systemctl enable libvirtd
systemctl start libvirtd

brctl show
virsh net-list

cat &lt;&lt; EOF &gt;  /data/virt-net.xml
&lt;network&gt;
  &lt;name&gt;br0&lt;/name&gt;
  &lt;forward mode='bridge'&gt;
    &lt;bridge name='br0'/&gt;
  &lt;/forward&gt;
&lt;/network&gt;
EOF

virsh net-define --file virt-net.xml
virsh net-dumpxml br0
# virsh net-undefine openshift4
# virsh net-destroy openshift4
virsh net-autostart br0
virsh net-start br0

cp /etc/sysconfig/network-scripts/ifcfg-em1 /etc/sysconfig/network-scripts/ifcfg-em1.orig

cat &lt;&lt; EOF &gt; /etc/sysconfig/network-scripts/ifcfg-em1
TYPE=Ethernet
PROXY_METHOD=none
BROWSER_ONLY=no
BOOTPROTO=none
DEFROUTE=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_FAILURE_FATAL=no
IPV6_ADDR_GEN_MODE=stable-privacy
NAME=em1
DEVICE=em1
ONBOOT=yes
# IPADDR=117.177.241.22
# PREFIX=24
# GATEWAY=117.177.241.1
IPV6_PRIVACY=no
# DNS1=117.177.241.16
BRIDGE=br0
EOF

cat &lt;&lt;EOF &gt; /etc/sysconfig/network-scripts/ifcfg-br0 
TYPE=Bridge
BOOTPROTO=static
IPADDR=117.177.241.22
GATEWAY=117.177.241.1
DNS1=117.177.241.16
ONBOOT=yes
DEFROUTE=yes
NAME=br0
DEVICE=br0
PREFIX=24
EOF

systemctl restart network

mkdir -p /data/ocp4
mkdir -p /data/kvm

pigz -dc ocp4-master2.qcow2.tgz | tar xf -

virt-install --name=ocp4-master2 --vcpus=20 --ram=200704 \
--disk path=/data/kvm/ocp4-master2.qcow2,bus=virtio,size=200 \
--os-variant rhel8.0 --network bridge=br0,model=virtio \
--boot menu=on 

virsh list --all

virsh start ocp4-master2

# anti scan
firewall-cmd --permanent --new-ipset=my-allow-list --type=hash:net
firewall-cmd --permanent --get-ipsets

cat &gt; /root/iplist.txt &lt;&lt;EOL
127.0.0.1/32
223.87.20.0/24
117.177.241.0/24
39.134.200.0/24
39.134.201.0/24
39.137.101.0/24
192.168.7.0/24
112.44.102.224/27
47.93.86.113/32
EOL

firewall-cmd --permanent --ipset=my-allow-list --add-entries-from-file=iplist.txt

firewall-cmd --permanent --ipset=my-allow-list --get-entries

firewall-cmd --permanent --zone=trusted --add-source=ipset:my-allow-list 
firewall-cmd --reload

firewall-cmd --list-all
firewall-cmd --get-active-zones

firewall-cmd --set-default-zone=block
firewall-cmd --runtime-to-permanent
firewall-cmd --reload

</code></pre>
<h3 id="infra0-node-day1"><a class="header" href="#infra0-node-day1">infra0 node day1</a></h3>
<pre><code class="language-bash">systemctl disable firewalld.service
systemctl stop firewalld.service

# secure for anti-scan
cat &lt;&lt; EOF &gt;&gt; /etc/rc.local

ipset create my-allow-set hash:net
ipset add my-allow-set 127.0.0.1/32
ipset add my-allow-set 223.87.20.0/24
ipset add my-allow-set 117.177.241.0/24
ipset add my-allow-set 39.134.200.0/24
ipset add my-allow-set 39.134.201.0/24
ipset add my-allow-set 39.137.101.0/24
ipset add my-allow-set 192.168.7.0/24
ipset add my-allow-set 112.44.102.224/27
ipset add my-allow-set 47.93.86.113/32

ipset add my-allow-set 39.134.204.0/24

iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
iptables -A INPUT -m set --match-set my-allow-set src -j ACCEPT
iptables -A INPUT -p tcp -j REJECT
iptables -A INPUT -p udp -j REJECT

EOF

chmod +x /etc/rc.d/rc.local
systemctl enable rc-local

# systemctl restart rc-local

# 配置kvm环境
yum -y install qemu-kvm libvirt libvirt-python libguestfs-tools virt-install virt-viewer virt-manager

systemctl enable libvirtd
systemctl start libvirtd

</code></pre>
<h3 id="infra1-node-day1"><a class="header" href="#infra1-node-day1">infra1 node day1</a></h3>
<pre><code class="language-bash">systemctl disable firewalld.service
systemctl stop firewalld.service

# secure for anti-scan
cat &lt;&lt; EOF &gt;&gt; /etc/rc.local

ipset create my-allow-set hash:net
ipset add my-allow-set 127.0.0.1/32
ipset add my-allow-set 223.87.20.0/24
ipset add my-allow-set 117.177.241.0/24
ipset add my-allow-set 39.134.200.0/24
ipset add my-allow-set 39.134.201.0/24
ipset add my-allow-set 39.137.101.0/24
ipset add my-allow-set 192.168.7.0/24
ipset add my-allow-set 112.44.102.224/27
ipset add my-allow-set 47.93.86.113/32

iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
iptables -A INPUT -m set --match-set my-allow-set src -j ACCEPT
iptables -A INPUT -p tcp -j REJECT
iptables -A INPUT -p udp -j REJECT

EOF

chmod +x /etc/rc.d/rc.local
systemctl enable rc-local

# systemctl restart rc-local

# 配置kvm环境
yum -y install qemu-kvm libvirt libvirt-python libguestfs-tools virt-install virt-viewer virt-manager

systemctl enable libvirtd
systemctl start libvirtd

</code></pre>
<h3 id="worker-0-day2-oper"><a class="header" href="#worker-0-day2-oper">worker-0 day2 oper</a></h3>
<pre><code class="language-bash">
podman login registry.redhat.ren:4443 -u zteadm

# localhost/ottcache-img:6.01.05.01T03
skopeo copy docker-archive:ZXCDN-OTT-IAS-IMGV6.01.05.01_TEST.tar docker://registry.redhat.ren:4443/zteadm/ottcache-img:6.01.05.01T03

# localhost/slbl7-img:6.01.05.01T03
skopeo copy docker-archive:ZXCDN-OTT-SLBL7-IMGV6.01.05.01_TEST.tar docker://registry.redhat.ren:4443/zteadm/slbl7-img:6.01.05.01T03

# localhost/webcache-img:v6.01.04.03
skopeo copy docker-archive:ZXCDN-CACHE-WEBCACHE-IMGV6.01.04.03.tar docker://registry.redhat.ren:4443/zteadm/webcache-img:v6.01.04.03

# localhost/pg-img:v1.01.01.01
skopeo copy docker-archive:ZXCDN-PG-IMGV1.01.01.01.tar docker://registry.redhat.ren:4443/zteadm/pg-img:v1.01.01.01

# localhost/slb-img:v6.01.04.03
skopeo copy docker-archive:ZXCDN-CACHE-SLB-IMGV6.01.04.03.tar docker://registry.redhat.ren:4443/zteadm/slb-img:v6.01.04.03

# io speed test
dd if=/dev/zero of=/data/testfile bs=1G count=10
# 10+0 records in
# 10+0 records out
# 10737418240 bytes (11 GB) copied, 6.85688 s, 1.6 GB/s

dd if=/dev/zero of=/data/testfile bs=1G count=10 oflag=direct
# 10+0 records in
# 10+0 records out
# 10737418240 bytes (11 GB) copied, 3.98098 s, 2.7 GB/s

dd if=/dev/zero of=/data/testfile bs=5M count=9999
# 9999+0 records in
# 9999+0 records out
# 52423557120 bytes (52 GB) copied, 27.8529 s, 1.9 GB/s

dd if=/dev/zero of=/data/testfile bs=5M count=9999 oflag=direct
# 9999+0 records in
# 9999+0 records out
# 52423557120 bytes (52 GB) copied, 16.1121 s, 3.3 GB/s

dd if=/dev/zero of=/data/testfile bs=5M count=9999 oflag=dsync
# 9999+0 records in
# 9999+0 records out
# 52423557120 bytes (52 GB) copied, 51.2713 s, 1.0 GB/s

dd if=/data/testfile of=/dev/null bs=1M count=9999 oflag=dsync
# 9999+0 records in
# 9999+0 records out
# 10484711424 bytes (10 GB) copied, 1.9141 s, 5.5 GB/s

dd if=/data/testfile of=/dev/null bs=5M count=9999 oflag=dsync
# 9999+0 records in
# 9999+0 records out
# 52423557120 bytes (52 GB) copied, 9.3676 s, 5.6 GB/s

# secure for anti-scan
cat &lt;&lt; EOF &gt; /etc/rc.local
#!/bin/bash
# THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES
#
# It is highly advisable to create own systemd services or udev rules
# to run scripts during boot instead of using this file.
#
# In contrast to previous versions due to parallel execution during boot
# this script will NOT be run after all other services.
#
# Please note that you must run 'chmod +x /etc/rc.d/rc.local' to ensure
# that this script will be executed during boot.

touch /var/lock/subsys/local

ipset create my-allow-set hash:net
ipset add my-allow-set 127.0.0.1/32
ipset add my-allow-set 223.87.20.0/24
ipset add my-allow-set 117.177.241.0/24
ipset add my-allow-set 39.134.200.0/24
ipset add my-allow-set 39.134.201.0/24
ipset add my-allow-set 39.137.101.0/24
ipset add my-allow-set 192.168.7.0/24
ipset add my-allow-set 112.44.102.224/27
ipset add my-allow-set 47.93.86.113/32
ipset add my-allow-set 221.226.0.75/32
ipset add my-allow-set 210.21.236.182/32
ipset add my-allow-set 61.132.54.2/32
ipset add my-allow-set 39.134.198.0/24

ipset add my-allow-set 218.205.236.16/28

ipset add my-allow-set 39.134.204.0/24

iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
iptables -A INPUT -m set --match-set my-allow-set src -j ACCEPT
iptables -A INPUT -p tcp -j REJECT
iptables -A INPUT -p udp -j REJECT

EOF

chmod +x /etc/rc.d/rc.local
systemctl enable rc-local

# systemctl restart rc-local

ipset add my-allow-set 221.226.0.75/32
ipset add my-allow-set 210.21.236.182/32
ipset add my-allow-set 61.132.54.2/32

# 配置kvm环境
yum -y install qemu-kvm libvirt libvirt-python libguestfs-tools virt-install virt-viewer virt-manager

systemctl enable libvirtd
systemctl start libvirtd

</code></pre>
<h3 id="worker-1-day2-oper"><a class="header" href="#worker-1-day2-oper">worker-1 day2 oper</a></h3>
<pre><code class="language-bash">cat &lt;&lt; EOF &gt; /etc/rc.local
#!/bin/bash
# THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES
#
# It is highly advisable to create own systemd services or udev rules
# to run scripts during boot instead of using this file.
#
# In contrast to previous versions due to parallel execution during boot
# this script will NOT be run after all other services.
#
# Please note that you must run 'chmod +x /etc/rc.d/rc.local' to ensure
# that this script will be executed during boot.

touch /var/lock/subsys/local

ipset create my-allow-set hash:net
ipset add my-allow-set 127.0.0.1/32
ipset add my-allow-set 223.87.20.0/24
ipset add my-allow-set 117.177.241.0/24
ipset add my-allow-set 39.134.200.0/24
ipset add my-allow-set 39.134.201.0/24
ipset add my-allow-set 39.137.101.0/24
ipset add my-allow-set 192.168.7.0/24
ipset add my-allow-set 112.44.102.224/27
ipset add my-allow-set 47.93.86.113/32
ipset add my-allow-set 221.226.0.75/32
ipset add my-allow-set 210.21.236.182/32
ipset add my-allow-set 61.132.54.2/32
ipset add my-allow-set 39.134.198.0/24

ipset add my-allow-set 218.205.236.16/28

ipset add my-allow-set 39.134.204.0/24

iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
iptables -A INPUT -m set --match-set my-allow-set src -j ACCEPT
iptables -A INPUT -p tcp -j REJECT
iptables -A INPUT -p udp -j REJECT

EOF

chmod +x /etc/rc.d/rc.local
systemctl enable rc-local

# systemctl restart rc-local

# 配置kvm环境
yum -y install qemu-kvm libvirt libvirt-python libguestfs-tools virt-install virt-viewer virt-manager

systemctl enable libvirtd
systemctl start libvirtd

</code></pre>
<h3 id="worker-2-day2-oper"><a class="header" href="#worker-2-day2-oper">worker-2 day2 oper</a></h3>
<pre><code class="language-bash">cat &lt;&lt; EOF &gt; /etc/rc.local
#!/bin/bash
# THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES
#
# It is highly advisable to create own systemd services or udev rules
# to run scripts during boot instead of using this file.
#
# In contrast to previous versions due to parallel execution during boot
# this script will NOT be run after all other services.
#
# Please note that you must run 'chmod +x /etc/rc.d/rc.local' to ensure
# that this script will be executed during boot.

touch /var/lock/subsys/local

ipset create my-allow-set hash:net
ipset add my-allow-set 127.0.0.1/32
ipset add my-allow-set 223.87.20.0/24
ipset add my-allow-set 117.177.241.0/24
ipset add my-allow-set 39.134.200.0/24
ipset add my-allow-set 39.134.201.0/24
ipset add my-allow-set 39.137.101.0/24
ipset add my-allow-set 192.168.7.0/24
ipset add my-allow-set 112.44.102.224/27
ipset add my-allow-set 47.93.86.113/32
ipset add my-allow-set 221.226.0.75/32
ipset add my-allow-set 210.21.236.182/32
ipset add my-allow-set 61.132.54.2/32
ipset add my-allow-set 39.134.198.0/24

ipset add my-allow-set 218.205.236.16/28

ipset add my-allow-set 39.134.204.0/24

iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
iptables -A INPUT -m set --match-set my-allow-set src -j ACCEPT
iptables -A INPUT -p tcp -j REJECT
iptables -A INPUT -p udp -j REJECT

EOF

chmod +x /etc/rc.d/rc.local
systemctl enable rc-local

# systemctl restart rc-local

# 配置kvm环境
yum -y install qemu-kvm libvirt libvirt-python libguestfs-tools virt-install virt-viewer virt-manager

systemctl enable libvirtd
systemctl start libvirtd
systemctl status libvirtd

systemctl stop libvirtd
systemctl disable libvirtd
# Installed:
#   libguestfs-tools.noarch 1:1.40.2-5.el7_7.3          libvirt.x86_64 0:4.5.0-23.el7_7.5          libvirt-python.x86_64 0:4.5.0-1.el7
#   qemu-kvm.x86_64 10:1.5.3-167.el7_7.4                virt-install.noarch 0:1.5.0-7.el7          virt-manager.noarch 0:1.5.0-7.el7
#   virt-viewer.x86_64 0:5.0-15.el7

# Dependency Installed:
#   adwaita-cursor-theme.noarch 0:3.28.0-1.el7                            adwaita-icon-theme.noarch 0:3.28.0-1.el7
#   at-spi2-atk.x86_64 0:2.26.2-1.el7                                     at-spi2-core.x86_64 0:2.28.0-1.el7
#   atk.x86_64 0:2.28.1-1.el7                                             augeas-libs.x86_64 0:1.4.0-9.el7
#   autogen-libopts.x86_64 0:5.18-5.el7                                   cairo.x86_64 0:1.15.12-4.el7
#   cairo-gobject.x86_64 0:1.15.12-4.el7                                  cdparanoia-libs.x86_64 0:10.2-17.el7
#   celt051.x86_64 0:0.5.1.3-8.el7                                        colord-libs.x86_64 0:1.3.4-1.el7
#   cyrus-sasl.x86_64 0:2.1.26-23.el7                                     dbus-x11.x86_64 1:1.10.24-13.el7_6
#   dconf.x86_64 0:0.28.0-4.el7                                           dejavu-fonts-common.noarch 0:2.33-6.el7
#   dejavu-sans-fonts.noarch 0:2.33-6.el7                                 flac-libs.x86_64 0:1.3.0-5.el7_1
#   fontconfig.x86_64 0:2.13.0-4.3.el7                                    fontpackages-filesystem.noarch 0:1.44-8.el7
#   fribidi.x86_64 0:1.0.2-1.el7_7.1                                      fuse.x86_64 0:2.9.2-11.el7
#   fuse-libs.x86_64 0:2.9.2-11.el7                                       gdk-pixbuf2.x86_64 0:2.36.12-3.el7
#   genisoimage.x86_64 0:1.1.11-25.el7                                    glib-networking.x86_64 0:2.56.1-1.el7
#   glusterfs-api.x86_64 0:3.12.2-47.2.el7                                glusterfs-cli.x86_64 0:3.12.2-47.2.el7
#   gnome-icon-theme.noarch 0:3.12.0-1.el7                                gnutls.x86_64 0:3.3.29-9.el7_6
#   gnutls-dane.x86_64 0:3.3.29-9.el7_6                                   gnutls-utils.x86_64 0:3.3.29-9.el7_6
#   gperftools-libs.x86_64 0:2.6.1-1.el7                                  graphite2.x86_64 0:1.3.10-1.el7_3
#   gsettings-desktop-schemas.x86_64 0:3.28.0-2.el7                       gsm.x86_64 0:1.0.13-11.el7
#   gstreamer1.x86_64 0:1.10.4-2.el7                                      gstreamer1-plugins-base.x86_64 0:1.10.4-2.el7
#   gtk-update-icon-cache.x86_64 0:3.22.30-3.el7                          gtk-vnc2.x86_64 0:0.7.0-3.el7
#   gtk3.x86_64 0:3.22.30-3.el7                                           gvnc.x86_64 0:0.7.0-3.el7
#   harfbuzz.x86_64 0:1.7.5-2.el7                                         hexedit.x86_64 0:1.2.13-5.el7
#   hicolor-icon-theme.noarch 0:0.12-7.el7                                hivex.x86_64 0:1.3.10-6.9.el7
#   ipxe-roms-qemu.noarch 0:20180825-2.git133f4c.el7                      iso-codes.noarch 0:3.46-2.el7
#   jasper-libs.x86_64 0:1.900.1-33.el7                                   jbigkit-libs.x86_64 0:2.0-11.el7
#   json-glib.x86_64 0:1.4.2-2.el7                                        lcms2.x86_64 0:2.6-3.el7
#   libICE.x86_64 0:1.0.9-9.el7                                           libSM.x86_64 0:1.2.2-2.el7
#   libX11.x86_64 0:1.6.7-2.el7                                           libX11-common.noarch 0:1.6.7-2.el7
#   libXau.x86_64 0:1.0.8-2.1.el7                                         libXcomposite.x86_64 0:0.4.4-4.1.el7
#   libXcursor.x86_64 0:1.1.15-1.el7                                      libXdamage.x86_64 0:1.1.4-4.1.el7
#   libXext.x86_64 0:1.3.3-3.el7                                          libXfixes.x86_64 0:5.0.3-1.el7
#   libXft.x86_64 0:2.3.2-2.el7                                           libXi.x86_64 0:1.7.9-1.el7
#   libXinerama.x86_64 0:1.1.3-2.1.el7                                    libXmu.x86_64 0:1.1.2-2.el7
#   libXrandr.x86_64 0:1.5.1-2.el7                                        libXrender.x86_64 0:0.9.10-1.el7
#   libXt.x86_64 0:1.1.5-3.el7                                            libXtst.x86_64 0:1.2.3-1.el7
#   libXv.x86_64 0:1.0.11-1.el7                                           libXxf86misc.x86_64 0:1.0.3-7.1.el7
#   libXxf86vm.x86_64 0:1.1.4-1.el7                                       libarchive.x86_64 0:3.1.2-14.el7_7
#   libasyncns.x86_64 0:0.8-7.el7                                         libcacard.x86_64 40:2.5.2-2.el7
#   libconfig.x86_64 0:1.4.9-5.el7                                        libepoxy.x86_64 0:1.5.2-1.el7
#   libglvnd.x86_64 1:1.0.1-0.8.git5baa1e5.el7                            libglvnd-egl.x86_64 1:1.0.1-0.8.git5baa1e5.el7
#   libglvnd-glx.x86_64 1:1.0.1-0.8.git5baa1e5.el7                        libgovirt.x86_64 0:0.3.4-3.el7
#   libguestfs.x86_64 1:1.40.2-5.el7_7.3                                  libguestfs-tools-c.x86_64 1:1.40.2-5.el7_7.3
#   libgusb.x86_64 0:0.2.9-1.el7                                          libibverbs.x86_64 0:22.1-3.el7
#   libiscsi.x86_64 0:1.9.0-7.el7                                         libjpeg-turbo.x86_64 0:1.2.90-8.el7
#   libmodman.x86_64 0:2.0.1-8.el7                                        libogg.x86_64 2:1.3.0-7.el7
#   libosinfo.x86_64 0:1.1.0-3.el7                                        libproxy.x86_64 0:0.4.11-11.el7
#   librdmacm.x86_64 0:22.1-3.el7                                         libsndfile.x86_64 0:1.0.25-10.el7
#   libsoup.x86_64 0:2.62.2-2.el7                                         libthai.x86_64 0:0.1.14-9.el7
#   libtheora.x86_64 1:1.1.1-8.el7                                        libtiff.x86_64 0:4.0.3-32.el7
#   libusal.x86_64 0:1.1.11-25.el7                                        libusbx.x86_64 0:1.0.21-1.el7
#   libvirt-bash-completion.x86_64 0:4.5.0-23.el7_7.5                     libvirt-client.x86_64 0:4.5.0-23.el7_7.5
#   libvirt-daemon.x86_64 0:4.5.0-23.el7_7.5                              libvirt-daemon-config-network.x86_64 0:4.5.0-23.el7_7.5
#   libvirt-daemon-config-nwfilter.x86_64 0:4.5.0-23.el7_7.5              libvirt-daemon-driver-interface.x86_64 0:4.5.0-23.el7_7.5
#   libvirt-daemon-driver-lxc.x86_64 0:4.5.0-23.el7_7.5                   libvirt-daemon-driver-network.x86_64 0:4.5.0-23.el7_7.5
#   libvirt-daemon-driver-nodedev.x86_64 0:4.5.0-23.el7_7.5               libvirt-daemon-driver-nwfilter.x86_64 0:4.5.0-23.el7_7.5
#   libvirt-daemon-driver-qemu.x86_64 0:4.5.0-23.el7_7.5                  libvirt-daemon-driver-secret.x86_64 0:4.5.0-23.el7_7.5
#   libvirt-daemon-driver-storage.x86_64 0:4.5.0-23.el7_7.5               libvirt-daemon-driver-storage-core.x86_64 0:4.5.0-23.el7_7.5
#   libvirt-daemon-driver-storage-disk.x86_64 0:4.5.0-23.el7_7.5          libvirt-daemon-driver-storage-gluster.x86_64 0:4.5.0-23.el7_7.5
#   libvirt-daemon-driver-storage-iscsi.x86_64 0:4.5.0-23.el7_7.5         libvirt-daemon-driver-storage-logical.x86_64 0:4.5.0-23.el7_7.5
#   libvirt-daemon-driver-storage-mpath.x86_64 0:4.5.0-23.el7_7.5         libvirt-daemon-driver-storage-rbd.x86_64 0:4.5.0-23.el7_7.5
#   libvirt-daemon-driver-storage-scsi.x86_64 0:4.5.0-23.el7_7.5          libvirt-daemon-kvm.x86_64 0:4.5.0-23.el7_7.5
#   libvirt-glib.x86_64 0:1.0.0-1.el7                                     libvirt-libs.x86_64 0:4.5.0-23.el7_7.5
#   libvisual.x86_64 0:0.4.0-16.el7                                       libvorbis.x86_64 1:1.3.3-8.el7.1
#   libwayland-client.x86_64 0:1.15.0-1.el7                               libwayland-cursor.x86_64 0:1.15.0-1.el7
#   libwayland-egl.x86_64 0:1.15.0-1.el7                                  libwayland-server.x86_64 0:1.15.0-1.el7
#   libxcb.x86_64 0:1.13-1.el7                                            libxkbcommon.x86_64 0:0.7.1-3.el7
#   libxshmfence.x86_64 0:1.2-1.el7                                       lsof.x86_64 0:4.87-6.el7
#   lzop.x86_64 0:1.03-10.el7                                             mesa-libEGL.x86_64 0:18.3.4-6.el7_7
#   mesa-libGL.x86_64 0:18.3.4-6.el7_7                                    mesa-libgbm.x86_64 0:18.3.4-6.el7_7
#   mesa-libglapi.x86_64 0:18.3.4-6.el7_7                                 mtools.x86_64 0:4.0.18-5.el7
#   netcf-libs.x86_64 0:0.2.8-4.el7                                       nettle.x86_64 0:2.7.1-8.el7
#   numad.x86_64 0:0.5-18.20150602git.el7                                 opus.x86_64 0:1.0.2-6.el7
#   orc.x86_64 0:0.4.26-1.el7                                             osinfo-db.noarch 0:20190319-2.el7
#   osinfo-db-tools.x86_64 0:1.1.0-1.el7                                  pango.x86_64 0:1.42.4-4.el7_7
#   pcre2.x86_64 0:10.23-2.el7                                            perl-Sys-Guestfs.x86_64 1:1.40.2-5.el7_7.3
#   perl-Sys-Virt.x86_64 0:4.5.0-2.el7                                    perl-hivex.x86_64 0:1.3.10-6.9.el7
#   perl-libintl.x86_64 0:1.20-12.el7                                     pixman.x86_64 0:0.34.0-1.el7
#   pulseaudio-libs.x86_64 0:10.0-5.el7                                   pulseaudio-libs-glib2.x86_64 0:10.0-5.el7
#   pycairo.x86_64 0:1.8.10-8.el7                                         python-gobject.x86_64 0:3.22.0-1.el7_4.1
#   qemu-img.x86_64 10:1.5.3-167.el7_7.4                                  qemu-kvm-common.x86_64 10:1.5.3-167.el7_7.4
#   radvd.x86_64 0:2.17-3.el7                                             rdma-core.x86_64 0:22.1-3.el7
#   rest.x86_64 0:0.8.1-2.el7                                             scrub.x86_64 0:2.5.2-7.el7
#   seabios-bin.noarch 0:1.11.0-2.el7                                     seavgabios-bin.noarch 0:1.11.0-2.el7
#   sgabios-bin.noarch 1:0.20110622svn-4.el7                              spice-glib.x86_64 0:0.35-4.el7
#   spice-gtk3.x86_64 0:0.35-4.el7                                        spice-server.x86_64 0:0.14.0-7.el7
#   squashfs-tools.x86_64 0:4.3-0.21.gitaae0aff4.el7                      supermin5.x86_64 0:5.1.19-1.el7
#   syslinux.x86_64 0:4.05-15.el7                                         syslinux-extlinux.x86_64 0:4.05-15.el7
#   trousers.x86_64 0:0.3.14-2.el7                                        unbound-libs.x86_64 0:1.6.6-1.el7
#   usbredir.x86_64 0:0.7.1-3.el7                                         virt-manager-common.noarch 0:1.5.0-7.el7
#   vte-profile.x86_64 0:0.52.2-2.el7                                     vte291.x86_64 0:0.52.2-2.el7
#   xkeyboard-config.noarch 0:2.24-1.el7                                  xml-common.noarch 0:0.6.3-39.el7
#   xorg-x11-server-utils.x86_64 0:7.7-20.el7                             xorg-x11-xauth.x86_64 1:1.0.9-1.el7
#   xorg-x11-xinit.x86_64 0:1.3.4-2.el7                                   yajl.x86_64 0:2.0.4-4.el7

</code></pre>
<h2 id="tips-2"><a class="header" href="#tips-2">tips</a></h2>
<ol>
<li>config local storage operator</li>
<li>config monitor storage</li>
<li>benchmark the storage using real senario</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="osx-系统如何录制系统声音"><a class="header" href="#osx-系统如何录制系统声音">OSX 系统如何录制系统声音</a></h1>
<p>mac系统上，如何录制系统的声音一直是个问题。特别是现在开在线会议，有的时候想在mac上录下来，在mac系统上，默认是不可以的。windows系统上不存在这个问题，不知道为什么mac上面反而特别麻烦。</p>
<p>解决这个问题的方法，就是<a href="https://github.com/kyleneideck/BackgroundMusic">BackgroundMusic</a></p>
<p>首先，下载和安装 BackgroundMusic ， 这个按照官网的步骤完成就好。</p>
<p>然后，启动audio midi程序，配置一个aggregate device</p>
<p><img src="osx/imgs/2021-01-22-19-32-15.png" alt="" /></p>
<p>确认一下系统的input, output设置</p>
<p><img src="osx/imgs/2021-01-22-19-33-17.png" alt="" /></p>
<p><img src="osx/imgs/2021-01-22-19-33-35.png" alt="" /></p>
<p>最后，我们开启录屏软件，输入设备选择aggregate device就可以了。</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
    </body>
</html>
